{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xJJYcgvDZQm"
      },
      "source": [
        "# Neural Network model for fluid dynamics (Part 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkiFIdrTDZRG"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paolodeangelis/Sistemi_a_combustione/blob/main/2.2-NN_Reynolds_P2.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro TODO"
      ],
      "metadata": {
        "id": "E1fgkgmEDp52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to `Edit` > `Notebook Settings`\n",
        "- select `T4 GPU` from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the GPU:"
      ],
      "metadata": {
        "id": "Ez_1bo8gGCz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "roYngX96GfoT",
        "outputId": "c599cdd0-d38f-4d52-9eab-1fc0b9cb3c30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.14.0\n",
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Libraries\n",
        "\n",
        "We begin by installing the necessary libraries to support our data manipulation, visualization, and deep learning modeling. (Note: `Tensorflow` and `Keras` are already installed on Colab)"
      ],
      "metadata": {
        "id": "5HODMa97DvP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install numpy pandas scipy matplotlib"
      ],
      "metadata": {
        "id": "L9lFAQhVEClk",
        "outputId": "5929f8a9-e472-4ace-daca-db4f548de5ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we import the necessary libraries"
      ],
      "metadata": {
        "id": "PCLXim7CrLiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # Operating system-related functions\n",
        "import pathlib  # Path manipulation and filesystem-related operations\n",
        "\n",
        "import matplotlib.pyplot as plt  # Data visualization library\n",
        "import numpy as np  # Numerical computing library\n",
        "import pandas as pd  # Data manipulation and analysis library\n",
        "import tensorflow as tf  # Deep learning framework for neural networks\n",
        "from tensorflow.keras import Sequential, layers, losses, optimizers"
      ],
      "metadata": {
        "id": "hvGzVOkNrMqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaYFSqhnvi1Y"
      },
      "source": [
        "## Data download and downsamplig\n",
        "\n",
        "In this section, we will download the dataset and  reduce its size to speed up the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igeIcVy1AIU0"
      },
      "source": [
        "### Download dataset files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9SsgVKDvZT_"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/data/lab2/velprof-Re.csv\n",
        "!wget https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/data/lab2/velprof-data.csv\n",
        "!wget https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/data/lab2/velprof-space.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUxOmbg2zSjx"
      },
      "source": [
        "### Read Reynolds (the labels for our model)\n",
        "\n",
        "We'll start by reading the Reynolds data, which serves as the labels for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq00UhcGwRiW"
      },
      "outputs": [],
      "source": [
        "data_Re = pd.read_csv(\"velprof-Re.csv\", index_col=False)\n",
        "data_Re.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wea1geIGzWwx"
      },
      "source": [
        "### Read the data file (the features for our model)\n",
        "\n",
        "Next, we'll read the data file containing the features for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhRwBsmCxuWG"
      },
      "outputs": [],
      "source": [
        "data_v = pd.read_csv(\"velprof-data.csv\", index_col=False)\n",
        "data_v.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxvKEMQ4AIU2"
      },
      "source": [
        "Let's also read another data file that contains information about space discretization, which, in our analogy, represents the probe's position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMg5TsrxNexm"
      },
      "outputs": [],
      "source": [
        "data_r = pd.read_csv(\"velprof-space.csv\", index_col=False)\n",
        "data_r.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0RERTjvAIU3"
      },
      "source": [
        "We can also merge the two datasets, which include both labels and features (it can be useful later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frCOBJ6C2fnn"
      },
      "outputs": [],
      "source": [
        "data_all = pd.concat([data_v, data_Re], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jL2z4K07mzZ"
      },
      "source": [
        "### Downsampling the database\n",
        "\n",
        "The dataset contains too many data points (100,000), so we will reduce it to 40,000 by randomly selecting from the entire database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84bOvbDDVn8z"
      },
      "outputs": [],
      "source": [
        "# Get the total number of data points in data_v\n",
        "Nall = data_v.shape[0]\n",
        "\n",
        "# Define the desired number of smaller data points to select\n",
        "Nsmall = 40000\n",
        "\n",
        "# Initialize a random number generator with a specific seed for reproducibility\n",
        "rand_gen = np.random.default_rng(seed=1234)\n",
        "\n",
        "# Generate a random sample of indices without replacement from the range [0, Nall)\n",
        "# This will be used to select a subset of data_v and data_Re\n",
        "indx = rand_gen.choice(np.arange(Nall), size=Nsmall, replace=False)\n",
        "\n",
        "# Create smaller subsets of data_v and data_Re based on the randomly selected indices\n",
        "data_v_small = data_v.iloc[indx, :]\n",
        "data_Re_small = data_Re.iloc[indx, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r2sqhPeBNJA"
      },
      "source": [
        "let's store it as `.csv` (comma-separated values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3yWskJZBP5u"
      },
      "outputs": [],
      "source": [
        "data_v_small.to_csv(\"small-data.csv\", index=False)\n",
        "data_Re_small.to_csv(\"small-Re.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW8Xxqkj_gqw"
      },
      "source": [
        "## Model 2: Re number *regression*\n",
        "\n",
        "-#TODO\n",
        "```\n",
        "In this section, we will employ our Neural-Network model for binary classification with the primary goal of distinguishing between turbulent and non-turbulent conditions. It's essential to clarify that, within the context of our database, \"non-turbulent\" encompasses both laminar and transitional conditions. Please note that for transitional conditions, the velocity profile results are obtained through interpolation, which may exhibit empirical inaccuracy.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCp3ul3GFDUI"
      },
      "source": [
        "### Setup database\n",
        "\n",
        "We load the features and the labels of our first model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsuC9LV7FUgh"
      },
      "outputs": [],
      "source": [
        "features = pd.read_csv(\"small-data.csv\", index_col=False).iloc[\n",
        "    :, 4:\n",
        "]  # Note: we drop the first 4 columns to study only the velocity profile\n",
        "labels = pd.read_csv(\"small-Re.csv\", index_col=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akhVBQVCAIU9"
      },
      "source": [
        "Display the first few rows of the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tutRAhKkGH6e"
      },
      "outputs": [],
      "source": [
        "features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzmrYgOKAIU9"
      },
      "source": [
        "Display the first few rows of the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_aJe56njcUu"
      },
      "outputs": [],
      "source": [
        "labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot3bq9CWjaHX"
      },
      "source": [
        "Next, we will convert the labels from numerical (`float`) to boolean (`bool`) using the following criteria:\n",
        "\n",
        "* `True` when the flow is turbulent ($Re \\ge 10^4$).\n",
        "* `False` when the flow is non-turbulent, which includes both laminar and transitional regimes ($Re < 10^4$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBWgcn8xB0cO"
      },
      "outputs": [],
      "source": [
        "labels_Re = labels.pop(\"Re(-)\")\n",
        "labels[\"Turbolent\"] = labels_Re >= 1e4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP8pCchmAIU-"
      },
      "source": [
        "Display the first few rows of the updated labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf4WNC7OkcJu"
      },
      "outputs": [],
      "source": [
        "labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuEFFf5G_ODX"
      },
      "source": [
        "To create a training and test dataset, we perform an 80/20 split.\n",
        "\n",
        "The purpose of this split is to reserve a portion of the data for testing the model's performance while ensuring that the two sets have a statistically similar distribution of features and labels. Since the database is already shuffled, we can conveniently take the first 1000 data points as the test set, given the total dataset size of 5000 items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhgLDXeS-ilW"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and test sets\n",
        "# Training set (80% of the data)\n",
        "labels_train = labels.iloc[1000:, :]\n",
        "features_train = features.iloc[1000:, :]\n",
        "labels_Re_train = labels_Re.iloc[1000:]\n",
        "\n",
        "# Test set (20% of the data)\n",
        "labels_test = labels.iloc[:1000, :]\n",
        "features_test = features.iloc[:1000, :]\n",
        "labels_Re_test = labels_Re.iloc[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1o08OhLlWjg"
      },
      "source": [
        "### Storing\n",
        "\n",
        "We will store the datasets and labels in a structured folder for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMrHqRUT-2Yh"
      },
      "outputs": [],
      "source": [
        "pathlib.Path(\"model_2\").mkdir(parents=True, exist_ok=True)  # Make a folder\n",
        "# Make train and test subfolders\n",
        "pathlib.Path(os.path.join(\"model_2\", \"train\")).mkdir(exist_ok=True)\n",
        "pathlib.Path(os.path.join(\"model_2\", \"test\")).mkdir(exist_ok=True)\n",
        "# Storing\n",
        "labels_train.to_csv(os.path.join(\"model_2\", \"train\", \"labels.csv\"), index=False)\n",
        "labels_test.to_csv(os.path.join(\"model_2\", \"test\", \"labels.csv\"), index=False)\n",
        "features_train.to_csv(os.path.join(\"model_2\", \"train\", \"features.csv\"), index=False)\n",
        "features_test.to_csv(os.path.join(\"model_2\", \"test\", \"features.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbxK0AFDAIVO"
      },
      "source": [
        "### First Try\n",
        "\n",
        "#### Building the Model with Keras and TensorFlow\n",
        "\n",
        "Now we define a function to create our neural network model using Keras and TensorFlow. This involves designing the layers, specifying their connections, and defining their behavior, including activation functions, regularization, dropout, and more.\n",
        "\n",
        "In a neural network, a *layer* serves as a fundamental building block responsible for processing data and extracting features. Layers are combined to form the architecture of the neural network. Each layer consists of one or more \"neurons\" (also known as \"nodes\" or \"units\").\n",
        "\n",
        "For our first model, we need to include the following types of layers:\n",
        "\n",
        "* **Input Layer**: This is the initial layer that receives raw input data. Its primary role is to pass the data to subsequent layers. The input layer typically has one neuron for each feature present in the input data.\n",
        "\n",
        "* **Hidden Layers**: These intermediate layers sit between the input and output layers. Hidden layers perform complex transformations on the data, enabling the network to learn and extract features from the input. Each neuron in a hidden layer receives input from multiple neurons in the previous layer.\n",
        "\n",
        "* **Output Layer**: The final layer in the neural network is responsible for producing the model's predictions. The number of neurons in the output layer depends on the nature of the problem being addressed. For binary classification (True/False or 0/1), a single neuron is common in the output layer. In multiclass classification or regression tasks, the number of output neurons can vary.\n",
        "\n",
        "In addition to layers, we also introduce the concept of an *activation function*. An activation function is a critical element within each neuron of a neural network. It dictates how the output of a neuron is calculated based on its input. The activation function introduces non-linearity into the model, allowing it to learn complex patterns and relationships within the data.\n",
        "\n",
        "Common activation functions include:\n",
        "\n",
        "* **ReLU (Rectified Linear Unit)**: ReLU is one of the most widely used activation functions. It returns the input value if it's positive and zero if it's negative. Mathematically, it can be represented as $f(x) = \\max(0, x)$. ReLU is effective in training deep networks and addressing the vanishing gradient problem.\n",
        "\n",
        "* **Sigmoid**: The sigmoid activation function is commonly used in the output layer of binary classification models. It squashes the output into a range between 0 and 1, which can be interpreted as a probability. Mathematically, it's expressed as $f(x) = \\dfrac{1}{1 + e^{-x}}$.\n",
        "\n",
        "* **Tanh (Hyperbolic Tangent)**: Tanh is similar to the sigmoid function but maps values to the range between -1 and 1. It is often used in hidden layers. Mathematically, it's defined as $f(x) = \\dfrac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$.\n",
        "\n",
        "The choice of the activation function depends on the specific problem and the architecture of the network. Each activation function has its own strengths and weaknesses, and selecting the right one is a crucial part of the network design process.\n",
        "\n",
        "In our case, we use the `relu` (Rectified Linear Unit) activation function in the first hidden layer and a smoother function like `sigmoid` for the output layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RToY7LfOluwF"
      },
      "outputs": [],
      "source": [
        "def build_model_2(n_cols: int) -> tf.keras.models.Sequential:\n",
        "    \"\"\"\n",
        "    Build a the first possible architecture for our neural network model.\n",
        "\n",
        "    Args:\n",
        "        n_cols (int): Number of input features.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.models.Sequential: A Keras Sequential model.\n",
        "    \"\"\"\n",
        "    model = Sequential(\n",
        "        [\n",
        "            # Input layer with the specified input shape\n",
        "            layers.InputLayer(input_shape=(n_cols,), name=\"input_layer\"),\n",
        "            # Add the first hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(64, activation=\"relu\", name=\"hidden_layer_1\"),\n",
        "            # Add the output layer with a single perceptron (we expect a True/False answer) and sigmoid activation\n",
        "            layers.Dense(1, activation=\"sigmoid\", name=\"output_layer\"),\n",
        "        ]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mwLYp_WAkK1"
      },
      "source": [
        "Let's call the function to build our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Foqg8JhAksQ"
      },
      "outputs": [],
      "source": [
        "model = build_model_1(features.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl6f1wJQBX14"
      },
      "source": [
        "#### Model Summary and Structure Visualization\n",
        "\n",
        "Let's examine the summary of our compiled model and visualize its architecture.\n",
        "\n",
        "Given that the input data has 50 features and the first hidden layer consists of 64 neurons (each with weights $w$ and biases $b$), we have the following:\n",
        "\n",
        "- Number of parameters in the first hidden layer ($\\mathbf{w}_{h1}$): $50 \\times 64 = 3200$\n",
        "- Number of parameters in the first hidden layer biases ($\\mathbf{b}_{h1}$): $1 \\times 64 = 64$\n",
        "\n",
        "In addition, considering the output layer:\n",
        "\n",
        "- Number of parameters in the output layer weights ($\\mathbf{w}_{o}$): $64 \\times 1 = 64$\n",
        "- Number of parameters in the output layer biases ($\\mathbf{b}_{o}$): $64 \\times 1 = 1$\n",
        "\n",
        "This results in a total of 3329 degrees of freedom (dofs) within our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OswAkDOgCdOs"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FraaL19gBh0h"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model=model, rankdir=\"LR\", dpi=72, show_shapes=True, show_layer_activations=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zMitJulAIVQ"
      },
      "source": [
        "#### Model Compilation\n",
        "\n",
        "Now we will compile our neural network model. Model compilation involves defining key components, such as the loss function, optimizer, learning rate, and metrics.\n",
        "\n",
        "##### Loss Function\n",
        "\n",
        "For our binary classification task, we use the Binary Cross-Entropy loss function. The *Binary Cross-Entropy* loss is calculated as:\n",
        "\n",
        "$\\text{Binary Cross-Entropy Loss} = -\\frac{1}{N}\\sum_{i=1}^{N}\\left(y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)\\right) $\n",
        "\n",
        "where $N$ is the number of samples, $y_i$ represents the true labels, and $p_i$ is the predicted probability of the positive class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc5LQtQCAIVQ"
      },
      "outputs": [],
      "source": [
        "loss = losses.BinaryCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYEWcIv0AIVQ"
      },
      "source": [
        "##### Optimizer\n",
        "\n",
        "We use the *Adam* optimizer, a popular choice for training neural networks. The [Adam optimization algorithm](https://doi.org/10.48550/arXiv.1412.6980) is a neural network-specific adaptation of the [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY3DPOlKAIVQ"
      },
      "outputs": [],
      "source": [
        "optimizer = optimizers.Adam(learning_rate=1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM5JKUmNAIVQ"
      },
      "source": [
        "##### Metrics\n",
        "\n",
        "Metrics are functions needed to measure the behavior of our model. There are many to choose from depending on the task of the model. For our case:\n",
        "\n",
        "- **Accuracy**: This metric measures the overall classification accuracy of the model. It is calculated as the ratio of correct predictions to the total number of samples.\n",
        "\n",
        "- **Binary Accuracy**: It's a specific metric for binary classification tasks. We use a threshold of 0.5 to determine binary predictions. Binary accuracy is computed as:\n",
        "\n",
        "$ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XIcDX88AIVR"
      },
      "outputs": [],
      "source": [
        "metrics = [\n",
        "    tf.keras.metrics.Accuracy(),\n",
        "    tf.keras.metrics.BinaryAccuracy(threshold=0.5),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RGVjl--AIVR"
      },
      "source": [
        "\n",
        "##### Compilation\n",
        "\n",
        "Finally, we compile the model by specifying the optimizer, loss function, and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHXgUnqbAIVR"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer,\n",
        "    loss,\n",
        "    metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBArdA58HErL"
      },
      "source": [
        "#### Training\n",
        "In training, we define two key parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULdIIY_fHKBx"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhj7X3CiAIVS"
      },
      "source": [
        "* **Batch Size**: It specifies the number of training examples used in each iteration. A smaller batch size updates the model more frequently, while a larger one may improve training efficiency but requiring more volatile memory (RAM).\n",
        "\n",
        "* **Epochs**: Each epoch represents one pass through the entire training dataset. It controls how many times the model iterates over the data, influencing convergence and potential overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uF9yyCmAIVS"
      },
      "source": [
        "Let's (finally) start the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Uz4jwPtHGB7"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    np.array(features_train),  # before to feed the data we convert it into an array\n",
        "    np.array(labels_train).astype(\"float\"),\n",
        "    batch_size,\n",
        "    epochs,\n",
        "    validation_data=(\n",
        "        np.array(features_test),\n",
        "        np.array(labels_test).astype(\"float\"),\n",
        "    ),  # test-set\n",
        "    verbose=1,  # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-loZjQNrOjLN"
      },
      "source": [
        "#### Plot Training Progress\n",
        "\n",
        "Now, we can plot the training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVUl8PLKMdac"
      },
      "outputs": [],
      "source": [
        "# Plot traing loop\n",
        "train_binary_accuracy = np.array(history.history[\"binary_accuracy\"])\n",
        "test_binary_accuracy = np.array(history.history[\"val_binary_accuracy\"])\n",
        "train_loss = np.array(history.history[\"loss\"])\n",
        "test_loss = np.array(history.history[\"val_loss\"])\n",
        "\n",
        "\n",
        "epochs_i = np.arange(1, train_loss.shape[0] + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ_iSIsjQh38"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 3.3))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax_twin = ax.twinx()\n",
        "    a1 = ax.plot(\n",
        "        epochs_i,\n",
        "        train_binary_accuracy * 100,\n",
        "        color=\"k\",\n",
        "        ls=\"-\",\n",
        "        label=\"Binary accuracy (train)\",\n",
        "    )\n",
        "    a2 = ax.plot(\n",
        "        epochs_i,\n",
        "        test_binary_accuracy * 100,\n",
        "        color=\"k\",\n",
        "        ls=\"--\",\n",
        "        label=\"Binary accuracy (test)\",\n",
        "    )\n",
        "    l1 = ax_twin.plot(epochs_i, train_loss, color=\"r\", ls=\"-\", label=\"Loss (train)\")\n",
        "    l2 = ax_twin.plot(epochs_i, test_loss, color=\"r\", ls=\"--\", label=\"Loss (test)\")\n",
        "    ax.set_xlabel(\"Epochs [-]\")\n",
        "    ax.set_ylabel(\"Binary accuracy [%]\")\n",
        "    ax_twin.set_ylabel(\"Loss [-]\")\n",
        "    ax_twin.legend(\n",
        "        a1 + a2 + l1 + l2,\n",
        "        [\n",
        "            \"Binary accuracy (train)\",\n",
        "            \"Binary accuracy (test)\",\n",
        "            \"Loss (train)\",\n",
        "            \"Loss (test)\",\n",
        "        ],\n",
        "        loc=\"center right\",\n",
        "    )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPJ4nXZtAIVT"
      },
      "source": [
        "#### Model result analysis\n",
        "\n",
        "Let's compare the model's predictions with the Reynolds number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z01ZChgQpV8"
      },
      "outputs": [],
      "source": [
        "# Predictions are generated using the model over the test set,\n",
        "# which the model has never seen during training.\n",
        "predictions = model.predict(np.array(features_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaNAI-wLQ9Um"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(7, 3.3))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    axins = ax.inset_axes([0.3, 0.3, 0.65, 0.54])\n",
        "    ax.scatter(labels_Re_test, predictions, s=7, alpha=0.4)\n",
        "    axins.scatter(labels_Re_test, predictions, s=7, alpha=0.4)\n",
        "    axins.set_xlim([0, 12000])\n",
        "    ax.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
        "    ax.axvline(2e3, lw=1, color=\"k\", ls=\":\")\n",
        "    ax.axvline(1e4, lw=1, color=\"k\", ls=\":\")\n",
        "    axins.axvline(2e3, lw=1, color=\"k\", ls=\":\")\n",
        "    axins.axvline(1e4, lw=1, color=\"k\", ls=\":\")\n",
        "    ax.set_xlabel(\"Re [-]\")\n",
        "    ax.set_ylabel(\"Turbulence probability [-]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyECwdpVtE2j"
      },
      "source": [
        "In the following code cell, we define functions for deeper analysis, which we can use also later in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e--qgHt3tJcD"
      },
      "outputs": [],
      "source": [
        "def compute_confusion_matrix(labels, predicted):\n",
        "    \"\"\"\n",
        "    Compute the confusion matrix and return its elements along with the indices of TP, TN, FP, and FN.\n",
        "\n",
        "    Args:\n",
        "        labels (array-like): The actual binary classification labels (0 or 1).\n",
        "        predicted (array-like): The predicted binary classification labels (0 or 1).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The 2x2 confusion matrix.\n",
        "        list: Indices of True Positives (TP)\n",
        "        list: Indices of True Negatives (TN)\n",
        "        list: Indices of False Positives (FP)\n",
        "        list: Indices of False Negatives (FN)\n",
        "    \"\"\"\n",
        "    if len(labels) != len(predicted):\n",
        "        raise ValueError(\"Input arrays must have the same length.\")\n",
        "\n",
        "    confusion_matrix = np.zeros((2, 2), dtype=int)\n",
        "    TP_indices, TN_indices, FP_indices, FN_indices = [], [], [], []\n",
        "\n",
        "    for i, (l, p) in enumerate(zip(labels, predicted)):\n",
        "        confusion_matrix[l][p] += 1\n",
        "        if l == 1 and p == 1:\n",
        "            TP_indices.append(i)\n",
        "        elif l == 0 and p == 0:\n",
        "            TN_indices.append(i)\n",
        "        elif l == 0 and p == 1:\n",
        "            FP_indices.append(i)\n",
        "        elif l == 1 and p == 0:\n",
        "            FN_indices.append(i)\n",
        "\n",
        "    return confusion_matrix, TP_indices, TN_indices, FP_indices, FN_indices\n",
        "\n",
        "\n",
        "def plot_conf_matrix(ax, confusion_matrix, class_names, cmap, title=\"Confusion Matrix\"):\n",
        "    \"\"\"\n",
        "    Plot the confusion matrix with percentages.\n",
        "\n",
        "    Args:\n",
        "        ax: Matplotlib axes to plot the matrix.\n",
        "        confusion_matrix (numpy.ndarray): The confusion matrix.\n",
        "        class_names: Class names for labeling.\n",
        "        cmap: Colormap for the plot.\n",
        "        title (str): Title for the plot.\n",
        "    \"\"\"\n",
        "    conf_matrix_perc = (confusion_matrix.T / confusion_matrix.sum(axis=1)).T * 100.0\n",
        "    cm = ax.imshow(\n",
        "        conf_matrix_perc, interpolation=\"nearest\", cmap=cmap, vmax=100.0, vmin=0.0\n",
        "    )\n",
        "    ax.set_title(title)\n",
        "    plt.colorbar(cm)\n",
        "\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    ax.set_xticks(tick_marks, class_names)\n",
        "    ax.set_yticks(tick_marks, class_names)\n",
        "    ax.set_yticklabels(class_names, rotation=90, ha=\"right\", va=\"center\")\n",
        "    for i in range(len(class_names)):\n",
        "        for j in range(len(class_names)):\n",
        "            ax.text(\n",
        "                j,\n",
        "                i,\n",
        "                f\"{conf_matrix_perc[i, j]:3.2f} %\",\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"k\",\n",
        "            )\n",
        "\n",
        "    ax.set_ylabel(\"True labels\")\n",
        "    ax.set_xlabel(\"Predicted labels\")\n",
        "\n",
        "\n",
        "def plot_velocity_conf_matrix(axs, cm_indices, velocity, class_names, title):\n",
        "    \"\"\"\n",
        "    Plot velocity profiles for TP, TN, FP, and FN.\n",
        "\n",
        "    Args:\n",
        "        axs: Matplotlib axes for subplots.\n",
        "        cm_indices: Confusion matrix indices.\n",
        "        velocity: Array of velocity profiles.\n",
        "        class_names: Class names for labeling.\n",
        "        title (str): Title for the plot.\n",
        "    \"\"\"\n",
        "    velocity = np.asarray(velocity)\n",
        "    r = np.linspace(0, 1, velocity.shape[1])\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            for k in cm_indices[i][j]:\n",
        "                axs[i][j].plot(r, velocity[k, :])\n",
        "            axs[i][j].set_yticks([np.mean(axs[i][j].get_ylim())], [class_names[i]])\n",
        "            axs[i][j].set_xticks([np.mean(axs[i][j].get_xlim())], [class_names[j]])\n",
        "            axs[i][j].set_yticklabels(\n",
        "                [class_names[i]], rotation=90, ha=\"right\", va=\"center\"\n",
        "            )\n",
        "    ax.set_ylabel(\"True labels\")\n",
        "    axs[0][0].set_ylabel(\"True labels\")\n",
        "    axs[1][1].set_xlabel(\"Predicted labels\")\n",
        "    axs[0][1].set_xticklabels([])\n",
        "    axs[0][1].set_yticklabels([])\n",
        "    axs[1][1].set_yticklabels([])\n",
        "    axs[0][0].yaxis.set_label_coords(-0.22, 0.0, transform=axs[0][0].transAxes)\n",
        "    axs[1][1].xaxis.set_label_coords(0.0, -0.22, transform=axs[1][1].transAxes)\n",
        "    axs[0][0].set_title(title, loc=\"right\", fontdict={\"ha\": \"center\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0T4BsAGAIVU"
      },
      "source": [
        "Now, let's compute the confusion matrix and plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPYY9jNYtSu2"
      },
      "outputs": [],
      "source": [
        "conf_matrix, i_TP, i_TN, i_FP, i_FN = compute_confusion_matrix(\n",
        "    np.squeeze(np.array(labels_test).astype(int)),\n",
        "    np.squeeze(predictions > 0.5).astype(int),\n",
        ")\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3.6))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    grid = fig.add_gridspec(2, 8, wspace=0.0, hspace=0.0)\n",
        "    ax_cm = fig.add_subplot(grid[:, :2])\n",
        "    plot_conf_matrix(ax_cm, conf_matrix, [\"Non-turbulent\", \"Turbulent\"], \"summer\")\n",
        "    ax_cm_p = []\n",
        "    for i in range(2):\n",
        "        ax_ = []\n",
        "        for j in range(2):\n",
        "            ax_.append(fig.add_subplot(grid[i, j + 3]))\n",
        "        ax_cm_p.append(ax_)\n",
        "\n",
        "    ax_cm_pn = []\n",
        "    for i in range(2):\n",
        "        ax_ = []\n",
        "        for j in range(2):\n",
        "            ax_.append(fig.add_subplot(grid[i, j + 6]))\n",
        "        ax_cm_pn.append(ax_)\n",
        "\n",
        "    cm_i = [[i_TN, i_FP], [i_FN, i_TP]]\n",
        "    plot_velocity_conf_matrix(\n",
        "        ax_cm_p,\n",
        "        cm_i,\n",
        "        features_test,\n",
        "        [\"Non-turbulent\", \"Turbulent\"],\n",
        "        \"Veloecity profiles\",\n",
        "    )\n",
        "    plot_velocity_conf_matrix(\n",
        "        ax_cm_pn,\n",
        "        cm_i,\n",
        "        (features_test.T / features_test.max(axis=1).values).T,\n",
        "        [\"Non-turbulent\", \"Turbulent\"],\n",
        "        \"Veloecity profiles normalized\",\n",
        "    )\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6Ucd0vhAwO8"
      },
      "source": [
        "### Second Try: Increasing Model Depth\n",
        "\n",
        "In this section, we aim to enhance the model's complexity by adding an additional hidden layer.\n",
        "We will evaluate if this increased complexity leads to significant performance improvements and whether we remain far from overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1R1-wby4rLWx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lHgkO2YDZRO"
      },
      "source": [
        "<img src=\"https://github.com/paolodeangelis/Sistemi_a_combustione/raw/main/assets/img/warning-work-in-progress.jpg\" width=\"600\"/>"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}