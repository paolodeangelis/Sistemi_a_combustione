{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xJJYcgvDZQm"
      },
      "source": [
        "# Neural Network model for fluid dynamics (Part 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkiFIdrTDZRG"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paolodeangelis/Sistemi_a_combustione/blob/main/2.2-NN_Reynolds_P2.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1fgkgmEDp52"
      },
      "source": [
        "## Hypothetical Experiment\n",
        "\n",
        "Let's imagine an experimental setup as depicted in the sketch below. In this hypothetical scenario, we are examining the flow of incompressible water within a duct, which exhibits various flow regimes. To gather data, we employ a fictional Pitot tube grid to obtain velocity profiles.\n",
        "\n",
        "\n",
        "<img style=\"display: block; margin: auto;\" alt=\"Experimental Setup\" src=\"https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/assets/img/NN_problem.png\">\n",
        "\n",
        "The central question we pose in this hypothetical experiment is: Can we effectively utilize a neural network (NN) to analyze and interpret these hypothetical velocity profiles and estimate the Reynolds number $\\mathrm{Re}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez_1bo8gGCz5"
      },
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to `Edit` > `Notebook Settings`\n",
        "- Select `T4 GPU` from the Hardware Accelerator drop-down\n",
        "- If the GPU nodes are busy, use the CPU one.\n",
        "\n",
        "Next, we'll check that we can connect to the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roYngX96GfoT",
        "outputId": "031aa689-bc7e-48b5-fff7-ec8926de0fdf"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "device_name = tf.test.gpu_device_name()  # GPU detection\n",
        "if device_name == \"/device:GPU:0\":\n",
        "    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
        "    print(f\"Running on GPU at: {device_name}\")\n",
        "else:\n",
        "    warnings.warn(\"GPU device not found\")\n",
        "    try:\n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
        "            tpu=\"\"\n",
        "        )  # TPU detection\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        strategy = tf.distribute.TPUStrategy(resolver)\n",
        "        print(\n",
        "            \"Running on TPU at:\",\n",
        "            \"\\n\\t\".join([f\"{i}\" for i in tf.config.list_logical_devices(\"TPU\")]),\n",
        "        )\n",
        "    except ValueError:\n",
        "        warnings.warn(\"TPU device not found\")\n",
        "        warnings.warn(\"Default parallization strategy will be used\")\n",
        "        strategy = tf.distribute.get_strategy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HODMa97DvP2"
      },
      "source": [
        "## Installing Libraries\n",
        "\n",
        "We begin by installing the necessary libraries to support our data manipulation, visualization, and deep learning modeling. (Note: `Tensorflow` and `Keras` are already installed on Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9lFAQhVEClk",
        "outputId": "4a0b6a83-3cb4-457b-ef21-938dce037173"
      },
      "outputs": [],
      "source": [
        "%pip install numpy pandas scipy matplotlib  scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCLXim7CrLiU"
      },
      "source": [
        "And now we import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvGzVOkNrMqV"
      },
      "outputs": [],
      "source": [
        "import os  # Operating system-related functions\n",
        "import pathlib  # Path manipulation and filesystem-related operations\n",
        "\n",
        "import matplotlib.pyplot as plt  # Data visualization library\n",
        "import numpy as np  # Numerical computing library\n",
        "import pandas as pd  # Data manipulation and analysis library\n",
        "import tensorflow as tf  # Deep learning framework for neural networks\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras import Sequential, layers, losses, optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaYFSqhnvi1Y"
      },
      "source": [
        "## Data download and downsamplig\n",
        "\n",
        "In this section, we will download the dataset and  reduce its size to speed up the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igeIcVy1AIU0"
      },
      "source": [
        "### Download dataset files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9SsgVKDvZT_",
        "outputId": "2d026dbb-1c14-45be-9a90-93298462464e"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/data/lab2/velprof-Re.csv\n",
        "!wget https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/data/lab2/velprof-data.csv\n",
        "!wget https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/data/lab2/velprof-space.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUxOmbg2zSjx"
      },
      "source": [
        "### Read Reynolds (the labels for our model)\n",
        "\n",
        "We'll start by reading the Reynolds data, which serves as the labels for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uq00UhcGwRiW",
        "outputId": "a141b198-0a43-40fc-954c-86e1074232b5"
      },
      "outputs": [],
      "source": [
        "data_Re = pd.read_csv(\"velprof-Re.csv\", index_col=False)\n",
        "data_Re.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wea1geIGzWwx"
      },
      "source": [
        "### Read the data file (the features for our model)\n",
        "\n",
        "Next, we'll read the data file containing the features for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "rhRwBsmCxuWG",
        "outputId": "3a3c2b76-1216-4282-c8c8-402652624986"
      },
      "outputs": [],
      "source": [
        "data_v = pd.read_csv(\"velprof-data.csv\", index_col=False)\n",
        "data_v.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxvKEMQ4AIU2"
      },
      "source": [
        "Let's also read another data file that contains information about space discretization, which, in our analogy, represents the probe's position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "GMg5TsrxNexm",
        "outputId": "0197463b-4d20-4b5b-f556-6c1c1216c9f2"
      },
      "outputs": [],
      "source": [
        "data_r = pd.read_csv(\"velprof-space.csv\", index_col=False)\n",
        "data_r.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0RERTjvAIU3"
      },
      "source": [
        "We can also merge the two datasets, which include both labels and features (it can be useful later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frCOBJ6C2fnn"
      },
      "outputs": [],
      "source": [
        "data_all = pd.concat([data_v, data_Re], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jL2z4K07mzZ"
      },
      "source": [
        "### Downsampling the database\n",
        "\n",
        "The dataset contains too many data points (100,000), so we will reduce it to 40,000 by randomly selecting from the entire database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84bOvbDDVn8z"
      },
      "outputs": [],
      "source": [
        "# Get the total number of data points in data_v\n",
        "Nall = data_v.shape[0]\n",
        "\n",
        "# Define the desired number of smaller data points to select\n",
        "Nsmall = 40000\n",
        "\n",
        "# Initialize a random number generator with a specific seed for reproducibility\n",
        "rand_gen = np.random.default_rng(seed=1234)\n",
        "\n",
        "# Generate a random sample of indices without replacement from the range [0, Nall)\n",
        "# This will be used to select a subset of data_v and data_Re\n",
        "indx = rand_gen.choice(np.arange(Nall), size=Nsmall, replace=False)\n",
        "\n",
        "# Create smaller subsets of data_v and data_Re based on the randomly selected indices\n",
        "data_v_small = data_v.iloc[indx, :]\n",
        "data_Re_small = data_Re.iloc[indx, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r2sqhPeBNJA"
      },
      "source": [
        "let's store it as `.csv` (comma-separated values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3yWskJZBP5u"
      },
      "outputs": [],
      "source": [
        "data_v_small.to_csv(\"small-data.csv\", index=False)\n",
        "data_Re_small.to_csv(\"small-Re.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW8Xxqkj_gqw"
      },
      "source": [
        "## Model 2: Re number *regression*\n",
        "\n",
        "\n",
        "In this section, we will employ our Neural-Network model for *regression* with the primary goal of predicting the reynolds number from the velocity profile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCp3ul3GFDUI"
      },
      "source": [
        "### Setup database\n",
        "\n",
        "We load the features and the labels of our first model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsuC9LV7FUgh"
      },
      "outputs": [],
      "source": [
        "# Note: this time we include the intial coloms contaiong phisical property of the fluid, and channel sizes (mu(Pas)\trho(kg/m3)\tL(m)\tR(m))\n",
        "features = pd.read_csv(\"small-data.csv\", index_col=False)\n",
        "labels = pd.read_csv(\"small-Re.csv\", index_col=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akhVBQVCAIU9"
      },
      "source": [
        "Display the first few rows of the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "tutRAhKkGH6e",
        "outputId": "08af0773-fa51-42bc-ac1a-9d9474e97784"
      },
      "outputs": [],
      "source": [
        "features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzmrYgOKAIU9"
      },
      "source": [
        "Display the first few rows of the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "O_aJe56njcUu",
        "outputId": "b1d77c08-ab2b-4bb3-f2e6-5bbc340484c3"
      },
      "outputs": [],
      "source": [
        "labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuEFFf5G_ODX"
      },
      "source": [
        "To create a training and test dataset, we perform an 80/20 split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhgLDXeS-ilW"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and test sets\n",
        "# Training set (80% of the data)\n",
        "labels_train = labels.iloc[int(Nsmall * 0.2) :, :]\n",
        "features_train = features.iloc[int(Nsmall * 0.2) :, :]\n",
        "\n",
        "# Test set (20% of the data)\n",
        "labels_test = labels.iloc[: int(Nsmall * 0.2), :]\n",
        "features_test = features.iloc[: int(Nsmall * 0.2), :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1o08OhLlWjg"
      },
      "source": [
        "### Storing\n",
        "\n",
        "We will store the datasets and labels in a structured folder for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMrHqRUT-2Yh"
      },
      "outputs": [],
      "source": [
        "pathlib.Path(\"model_2\").mkdir(parents=True, exist_ok=True)  # Make a folder\n",
        "# Make train and test subfolders\n",
        "pathlib.Path(os.path.join(\"model_2\", \"train\")).mkdir(exist_ok=True)\n",
        "pathlib.Path(os.path.join(\"model_2\", \"test\")).mkdir(exist_ok=True)\n",
        "# Note: This time, we include the initial columns containing physical properties of the fluid and channel sizes (mu (Pas), rho (kg/mÂ³), L (m), R (m)).\n",
        "labels_train.to_csv(os.path.join(\"model_2\", \"train\", \"labels.csv\"), index=False)\n",
        "labels_test.to_csv(os.path.join(\"model_2\", \"test\", \"labels.csv\"), index=False)\n",
        "features_train.to_csv(os.path.join(\"model_2\", \"train\", \"features.csv\"), index=False)\n",
        "features_test.to_csv(os.path.join(\"model_2\", \"test\", \"features.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUiu38pJA5Dx"
      },
      "source": [
        "### First Try"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UthZ97ZeA5Dx"
      },
      "source": [
        "The sketch below represents the proposed neural network architecture.\n",
        "\n",
        "<img style=\"display: block; margin: auto;\" alt=\"NN sketch\" src=\"https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/assets/img/NN_model2_try1.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnk1CBXgA5Dx"
      },
      "source": [
        "#### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbxK0AFDAIVO"
      },
      "source": [
        "#### Building the Model with Keras and TensorFlow\n",
        "\n",
        "\n",
        "Now we define a function to create our neural network model using Keras and TensorFlow. This involves designing the layers, specifying their connections, and defining their behavior, including activation functions, regularization, dropout, and more.\n",
        "\n",
        "In a neural network, a *layer* serves as a fundamental building block responsible for processing data and extracting features. Layers are combined to form the architecture of the neural network. Each layer consists of one or more \"neurons\" (also known as \"nodes\" or \"units\").\n",
        "\n",
        "For our first model, we need to include the following types of layers:\n",
        "\n",
        "* **Input Layer**: This is the initial layer that receives raw input data. Its primary role is to pass the data to subsequent layers. The input layer typically has one neuron for each feature present in the input data.\n",
        "\n",
        "* **Hidden Layers**: These intermediate layers sit between the input and output layers. Hidden layers perform complex transformations on the data, enabling the network to learn and extract features from the input. Each neuron in a hidden layer receives input from multiple neurons in the previous layer.\n",
        "\n",
        "* **Output Layer**: The final layer in the neural network is responsible for producing the model's predictions. The number of neurons in the output layer depends on the nature of the problem being addressed. For the Reynolds number regression ($\\mathrm{Re} \\in \\left[0, +\\infty\\right)$), a single neuron is common in the output layer. Here is crucial the choice of the activation function\n",
        "\n",
        "In addition to layers, we also introduce the concept of an *activation function*. An activation function is a critical element within each neuron of a neural network. It dictates how the output of a neuron is calculated based on its input. The activation function introduces non-linearity into the model, allowing it to learn complex patterns and relationships within the data.\n",
        "\n",
        "In the following example we are going to use\n",
        "\n",
        "* **ReLU (Rectified Linear Unit)**: ReLU is one of the most widely used activation functions. It returns the input value if it's positive and zero if it's negative. Mathematically, it can be represented as $f(x) = \\max(0, x)$. ReLU is effective in training deep networks and addressing the vanishing gradient problem.\n",
        "\n",
        "* **Linear**: Basicaly the output neurons gather features from the previous layer and return the result through a matrix operation $\\mathbf{W}\\mathbf{x} + b$ with $\\mathbf{W} \\in \\mathbb{R}^{1\\times N}$, $\\mathbf{x} \\in \\mathbb{R}^{N\\times 1}$. Mathematically, it's expressed as $f(x) = x$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoyRhsa8HUCR"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrOQg_CoMcw8"
      },
      "outputs": [],
      "source": [
        "features_train_try1 = pd.read_csv(\n",
        "    os.path.join(\"model_2\", \"train\", \"features.csv\"), index_col=False\n",
        ").iloc[\n",
        "    :, 4:\n",
        "]  # Note: we drop the first 4 columns to study only the velocity profile\n",
        "features_test_try1 = pd.read_csv(\n",
        "    os.path.join(\"model_2\", \"test\", \"features.csv\"), index_col=False\n",
        ").iloc[\n",
        "    :, 4:\n",
        "]  # Note: we drop the first 4 columns to study only the velocity profile\n",
        "labels_train_try1 = pd.read_csv(\n",
        "    os.path.join(\"model_2\", \"train\", \"labels.csv\"), index_col=False\n",
        ")\n",
        "labels_test_try1 = pd.read_csv(\n",
        "    os.path.join(\"model_2\", \"test\", \"labels.csv\"), index_col=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RToY7LfOluwF"
      },
      "outputs": [],
      "source": [
        "def build_model_2(n_cols: int, out_activation: str) -> tf.keras.models.Sequential:\n",
        "    \"\"\"\n",
        "    Build a the first possible architecture for our neural network model.\n",
        "\n",
        "    Args:\n",
        "        n_cols (int): Number of input features.\n",
        "        out_activation (str): Output activation function.\n",
        "    Returns:\n",
        "        tf.keras.models.Sequential: A Keras Sequential model.\n",
        "    \"\"\"\n",
        "    model = Sequential(\n",
        "        [\n",
        "            # Input layer with the specified input shape\n",
        "            layers.InputLayer(input_shape=(n_cols,), name=\"input_layer\"),\n",
        "            # Add the first hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(64, activation=\"relu\", name=\"hidden_layer_1\"),\n",
        "            # Add the second hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(64, activation=\"relu\", name=\"hidden_layer_2\"),\n",
        "            # Add the third hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(32, activation=\"relu\", name=\"hidden_layer_3\"),\n",
        "            # Add the forth hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(32, activation=\"relu\", name=\"hidden_layer_4\"),\n",
        "            # Add the output layer with a single perceptron (we expect a True/False answer) and sigmoid activation\n",
        "            layers.Dense(1, activation=out_activation, name=\"output_layer\"),\n",
        "        ]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mwLYp_WAkK1"
      },
      "source": [
        "Let's call the function to build our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Foqg8JhAksQ"
      },
      "outputs": [],
      "source": [
        "model = build_model_2(features_train_try1.shape[1], \"linear\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl6f1wJQBX14"
      },
      "source": [
        "#### Model Summary and Structure Visualization\n",
        "\n",
        "Let's examine the summary of our compiled model and visualize its architecture.\n",
        "\n",
        "Given that the input data has 50 features and the first hidden layer consists of 64 neurons (each with weights $w$ and biases $b$), we have the following:\n",
        "\n",
        "- Number of parameters in the first hidden layer ($\\mathbf{w}_{h1}$): $50 \\times 64 = 3200$\n",
        "- Number of parameters in the first hidden layer biases ($\\mathbf{b}_{h1}$): $1 \\times 64 = 64$\n",
        "\n",
        "The second hidden layer is densely connected with the first one, therefore we have 64 \"inputs feature\" from the first hidden layer:\n",
        "\n",
        "- Number of parameters in the first hidden layer ($\\mathbf{w}_{h1}$): $64 \\times 64 = 4096$\n",
        "- Number of parameters in the first hidden layer biases ($\\mathbf{b}_{h1}$): $1 \\times 64 = 64$\n",
        "\n",
        "The third hidden layer is densely connected with the second one, therefore we have 32 \"inputs feature\" from the first hidden layer:\n",
        "\n",
        "- Number of parameters in the first hidden layer ($\\mathbf{w}_{h1}$): $64 \\times 32 = 2048$\n",
        "- Number of parameters in the first hidden layer biases ($\\mathbf{b}_{h1}$): $1 \\times 32 = 32$\n",
        "\n",
        "The forth hidden layer is densely connected with the third one, therefore we have 32 \"inputs feature\" from the first hidden layer:\n",
        "\n",
        "- Number of parameters in the first hidden layer ($\\mathbf{w}_{h1}$): $32 \\times 32 = 1024$\n",
        "- Number of parameters in the first hidden layer biases ($\\mathbf{b}_{h1}$): $1 \\times 32 = 32$\n",
        "\n",
        "In addition, considering the output layer:\n",
        "\n",
        "- Number of parameters in the output layer weights ($\\mathbf{w}_{o}$): $64 \\times 1 = 64$\n",
        "- Number of parameters in the output layer biases ($\\mathbf{b}_{o}$): $64 \\times 1 = 1$\n",
        "\n",
        "This results in a total of $(3200 + 64) + (4096 +64) + (2048 + 32) + (1024 +32) + (64 +1) = 10625 $ degrees of freedom (dofs) within our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OswAkDOgCdOs",
        "outputId": "e6a04e82-c224-448e-f787-403fbb18370a"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "FraaL19gBh0h",
        "outputId": "ccc05c4d-bc5b-4411-fc6d-5517fd0a68f1"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model=model, rankdir=\"LR\", dpi=72, show_shapes=True, show_layer_activations=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zMitJulAIVQ"
      },
      "source": [
        "#### Model Compilation\n",
        "\n",
        "Now we will compile our neural network model. Model compilation involves defining key components, such as the loss function, optimizer, learning rate, and metrics.\n",
        "\n",
        "##### Loss Function\n",
        "\n",
        "For our regression task, we use the Mean Absolute Percentage Error (MAPE) loss function. The *Mean Absolute Percentage Error loss is calculated as:\n",
        "\n",
        "The Mean Absolute Percentage Error (MAPE) loss function can be represented in LaTeX as follows:\n",
        "\n",
        "$$ MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100\\% $$\n",
        "\n",
        "where $ n $ is the number of data points, $ y_i $ represents the actual (observed) values, $ \\hat{y}_i $ represents the predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc5LQtQCAIVQ"
      },
      "outputs": [],
      "source": [
        "loss = losses.MeanAbsolutePercentageError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYEWcIv0AIVQ"
      },
      "source": [
        "##### Optimizer\n",
        "\n",
        "We use the *Adam* optimizer, a popular choice for training neural networks. The [Adam optimization algorithm](https://doi.org/10.48550/arXiv.1412.6980) is a neural network-specific adaptation of the [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY3DPOlKAIVQ"
      },
      "outputs": [],
      "source": [
        "optimizer = optimizers.Adam(learning_rate=1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM5JKUmNAIVQ"
      },
      "source": [
        "##### Metrics\n",
        "\n",
        "Metrics are functions needed to measure the behavior of our model. There are many to choose from depending on the task of the model. For our case:\n",
        "\n",
        "- **Root Mean Squared Error (RMSE)**: RMSE is a metric used to evaluate regression models. It measures the square root of the average of the squared differences between predicted and actual values. It quantifies the model's prediction error.\n",
        "\n",
        "  $$ RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
        "\n",
        "  Where $n$ is the number of data points, $y_i$ represents the actual (observed) values and $\\hat{y}_i$ represents the predicted values.\n",
        "\n",
        "- **Mean Squared Error (MSE)**: MSE is another metric used to evaluate regression models. It is computed as the average of the squared differences between predicted and actual values.\n",
        "\n",
        "  $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
        "\n",
        "  where $n$ is the number of data points, $y_i$ represents the actual (observed) values, and $\\hat{y}_i$ represents the predicted values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XIcDX88AIVR"
      },
      "outputs": [],
      "source": [
        "metrics = [\n",
        "    tf.keras.metrics.RootMeanSquaredError(),\n",
        "    tf.keras.metrics.MeanSquaredError(),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RGVjl--AIVR"
      },
      "source": [
        "\n",
        "##### Compilation\n",
        "\n",
        "Finally, we compile the model by specifying the optimizer, loss function, and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHXgUnqbAIVR"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer,\n",
        "    loss,\n",
        "    metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBArdA58HErL"
      },
      "source": [
        "#### Training\n",
        "In training, we define two key parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULdIIY_fHKBx"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "epochs = 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhj7X3CiAIVS"
      },
      "source": [
        "* **Batch Size**: It specifies the number of training examples used in each iteration. A smaller batch size updates the model more frequently, while a larger one may improve training efficiency but requiring more volatile memory (RAM).\n",
        "\n",
        "* **Epochs**: Each epoch represents one pass through the entire training dataset. It controls how many times the model iterates over the data, influencing convergence and potential overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uF9yyCmAIVS"
      },
      "source": [
        "Let's (finally) start the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uz4jwPtHGB7",
        "outputId": "a40b9fe4-3bb9-4cc8-d9ed-4cbe195c0a36"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    # These initial lines of code are repeated because they need to be defined\n",
        "    # inside the parallelization context to efficiently utilize the GPU/TPU.\n",
        "\n",
        "    # Step 1: Building the Model\n",
        "    model = build_model_2(features_train_try1.shape[1], \"linear\")\n",
        "\n",
        "    # Step 2: Compiling the Model\n",
        "    loss = losses.MeanAbsolutePercentageError()\n",
        "    optimizer = optimizers.Adam(\n",
        "        learning_rate=1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-08\n",
        "    )\n",
        "    metrics = [\n",
        "        tf.keras.metrics.RootMeanSquaredError(),\n",
        "        tf.keras.metrics.MeanSquaredError(),\n",
        "    ]\n",
        "    model.compile(\n",
        "        optimizer,\n",
        "        loss,\n",
        "        metrics,\n",
        "    )\n",
        "\n",
        "    # Step 3: Training the Model\n",
        "    history = model.fit(\n",
        "        np.array(\n",
        "            features_train_try1\n",
        "        ),  # Convert the data into an array before feeding it\n",
        "        np.array(labels_train_try1).astype(\"float\"),\n",
        "        batch_size,\n",
        "        epochs,\n",
        "        validation_data=(\n",
        "            np.array(features_test_try1),\n",
        "            np.array(labels_test_try1).astype(\"float\"),\n",
        "        ),  # Validation set\n",
        "        verbose=1,  # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-loZjQNrOjLN"
      },
      "source": [
        "#### Plot Training Progress\n",
        "\n",
        "Now, we can plot the training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fmr964txXvVU"
      },
      "outputs": [],
      "source": [
        "# @title Ausiliar plot function\n",
        "\n",
        "\n",
        "def plot_training(\n",
        "    ax,\n",
        "    ax_twin,\n",
        "    history,\n",
        "    metric=\"root_mean_squared_error\",\n",
        "    metric_label=\"RMSE\",\n",
        "    halflife=25,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot training history with specified metric.\n",
        "\n",
        "    Args:\n",
        "        ax (Matplotlib Axis): The main plot axis.\n",
        "        ax_twin (Matplotlib Axis): The twinned plot axis.\n",
        "        history (Pandas DataFrame): Training history data.\n",
        "        metric (str): The name of the metric to plot.\n",
        "        metric_label (str): Label for the metric on the plot.\n",
        "        halflife (int): Exponential moving average halflife for smoothing.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    ax.plot(history.index, history[metric], color=\"k\", ls=\"-\", alpha=0.25)\n",
        "    a1 = ax.plot(\n",
        "        history.index,\n",
        "        history[metric].ewm(halflife=halflife).mean(),\n",
        "        color=\"k\",\n",
        "        ls=\"-\",\n",
        "        label=metric_label + \" (train)\",\n",
        "    )\n",
        "    ax.plot(history.index, history[\"val_\" + metric], color=\"k\", ls=\"--\", alpha=0.25)\n",
        "    a2 = ax.plot(\n",
        "        history.index,\n",
        "        history[\"val_\" + metric].ewm(halflife=halflife).mean(),\n",
        "        color=\"k\",\n",
        "        ls=\"--\",\n",
        "        label=metric_label + \" (test)\",\n",
        "    )\n",
        "    ax_twin.plot(history.index, history[\"loss\"], color=\"r\", ls=\"-\", alpha=0.25)\n",
        "    l1 = ax_twin.plot(\n",
        "        history.index,\n",
        "        history[\"loss\"].ewm(halflife=halflife).mean(),\n",
        "        color=\"r\",\n",
        "        ls=\"-\",\n",
        "        label=\"Loss (train)\",\n",
        "    )\n",
        "    ax_twin.plot(history.index, history[\"val_loss\"], color=\"r\", ls=\"--\", alpha=0.25)\n",
        "    l2 = ax_twin.plot(\n",
        "        history.index,\n",
        "        history[\"val_loss\"].ewm(halflife=halflife).mean(),\n",
        "        color=\"r\",\n",
        "        ls=\"--\",\n",
        "        label=\"Loss (test)\",\n",
        "    )\n",
        "    ax.set_xlabel(\"Epochs [-]\")\n",
        "    ax.set_ylabel(metric_label + \" [-]\")\n",
        "    ax_twin.set_ylabel(metric_label + \"Loss [-]\")\n",
        "    ax_twin.legend(\n",
        "        a1 + a2 + l1 + l2,\n",
        "        [\n",
        "            metric_label + \" (train)\",\n",
        "            metric_label + \" (test)\",\n",
        "            \"Loss (train)\",\n",
        "            \"Loss (test)\",\n",
        "        ],\n",
        "        loc=\"upper right\",\n",
        "    )\n",
        "    metric_data = np.concatenate(\n",
        "        [\n",
        "            history[metric].ewm(halflife=halflife).mean().values,\n",
        "            history[\"val_\" + metric].ewm(halflife=halflife).mean().values,\n",
        "        ]\n",
        "    )\n",
        "    lb, ub = [np.percentile(metric_data, 0.5), np.percentile(metric_data, 99.5)]\n",
        "    delta = ub - lb\n",
        "    ax.set_ylim([lb - 0.05 * delta, ub + 0.05 * delta])\n",
        "    loss_data = np.concatenate(\n",
        "        [\n",
        "            history[\"loss\"].ewm(halflife=halflife).mean().values,\n",
        "            history[\"val_loss\"].ewm(halflife=halflife).mean().values,\n",
        "        ]\n",
        "    )\n",
        "    lb, ub = [np.percentile(loss_data, 0.5), np.percentile(loss_data, 99.5)]\n",
        "    delta = ub - lb\n",
        "    ax_twin.set_ylim([lb - 0.05 * delta, ub + 0.05 * delta])\n",
        "\n",
        "\n",
        "def get_rmsre(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Get the Root Mean Square Relative Error.\n",
        "\n",
        "    Args:\n",
        "        y_true (array-like): True values.\n",
        "        y_pred (array-like): Predicted values.\n",
        "\n",
        "    Returns:\n",
        "        RMSE (float): The Root Mean Square Relative Error.\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.mean(np.square((y_pred - y_true) / y_true)))\n",
        "\n",
        "\n",
        "def get_mare(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Get the Max Absolute Relative Error.\n",
        "\n",
        "    Args:\n",
        "        y_true (array-like): True values.\n",
        "        y_pred (array-like): Predicted values.\n",
        "\n",
        "    Returns:\n",
        "        MARE (float): The Max Absolute Relative Error.\n",
        "    \"\"\"\n",
        "    return np.max(np.abs(y_pred - y_true) / y_true)\n",
        "\n",
        "\n",
        "def plot_regression(ax, axins, train_true, train_pred, test_true, test_pred):\n",
        "    \"\"\"\n",
        "    Plot regression results and error distribution.\n",
        "\n",
        "    Args:\n",
        "        ax (Matplotlib Axis): The main plot axis.\n",
        "        axins (Matplotlib Axis): The twinned plot axis.\n",
        "        train_true (array-like): True values for the training set.\n",
        "        train_pred (array-like): Predicted values for the training set.\n",
        "        test_true (array-like): True values for the test set.\n",
        "        test_pred (array-like): Predicted values for the test set.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # compute error\n",
        "    error_test_perc = np.abs(test_pred - test_true) / test_true * 100\n",
        "    error_train_perc = np.abs(train_pred - train_true) / train_true * 100\n",
        "    # compute error distibution\n",
        "    p_test, bins_test = np.histogram(np.log(error_test_perc), bins=200, density=True)\n",
        "    x_bins_test = (bins_test[1:] + bins_test[:-1]) / 2\n",
        "    p_train, bins_train = np.histogram(np.log(error_train_perc), bins=200, density=True)\n",
        "    x_bins_train = (bins_train[1:] + bins_train[:-1]) / 2\n",
        "    # compute some more metrhics\n",
        "    r2_test = r2_score(test_true, test_pred)\n",
        "    r2_train = r2_score(train_true, train_pred)\n",
        "    RMSRE_test = get_rmsre(test_true, test_pred) * 100.0\n",
        "    RMSRE_train = get_rmsre(train_true, train_pred) * 100.0\n",
        "    MARE_test = get_mare(test_true, test_pred) * 100.0\n",
        "    MARE_train = get_mare(train_true, train_pred) * 100.0\n",
        "    # train\n",
        "    ax.scatter(train_true, train_pred, s=4, alpha=0.2, color=\"#126fbf\", label=\"Train\")\n",
        "    axins.fill_between(\n",
        "        np.exp(x_bins_train),\n",
        "        p_train * 0.0,\n",
        "        p_train,\n",
        "        alpha=0.4,\n",
        "        color=\"#126fbf\",\n",
        "        label=\"Train\",\n",
        "    )\n",
        "    axins.plot(np.exp(x_bins_train), p_train, color=\"#126fbf\", lw=1.2)\n",
        "    # test\n",
        "    ax.scatter(test_true, test_pred, s=4, alpha=0.2, color=\"#f3702b\", label=\"Test\")\n",
        "    axins.fill_between(\n",
        "        np.exp(x_bins_test),\n",
        "        p_test * 0.0,\n",
        "        p_test,\n",
        "        alpha=0.4,\n",
        "        color=\"#f3702b\",\n",
        "        label=\"Test\",\n",
        "    )\n",
        "    axins.plot(np.exp(x_bins_test), p_test, color=\"#f3702b\", lw=1.2)\n",
        "    # additional metrics\n",
        "    ax.annotate(\n",
        "        f\"Train:\\n\\t $R^2 = {r2_train:1.3f}\"\n",
        "        + f\"$\\n\\t $RMSRE = {RMSRE_train:1.3f}$ % \"\n",
        "        + f\"\\n\\t $MARE = {MARE_train:1.3f}$ % \\n\"\n",
        "        + f\"Test:\\n\\t $R^2 = {r2_test:1.3f}$\"\n",
        "        + f\"\\n\\t $RMSRE = {RMSRE_test:1.3f}$ %\"\n",
        "        + f\"\\n\\t $MARE = {MARE_test:1.3f}$ %\",\n",
        "        xy=(1.05, 0.5),\n",
        "        xycoords=ax.transAxes,\n",
        "        va=\"center\",\n",
        "    )\n",
        "    ax.set_xlim([50e-2, 5e6])\n",
        "    ax.set_ylim([50e-2, 5e6])\n",
        "    ax.set_aspect(\"equal\", adjustable=\"datalim\")\n",
        "    ax.axline([0, 0], [1, 1], color=\"k\", ls=\"--\", lw=0.75)\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    ax.set_xlabel(r\"True $\\mathrm{Re}$ [-]\")\n",
        "    ax.set_ylabel(r\"Prediction $\\mathrm{Re}$ [-]\")\n",
        "    axins.legend(loc=\"upper left\")\n",
        "    axins.set_xlabel(\"Relative error [%]\")\n",
        "    axins.set_ylabel(\"Probability density [-]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVUl8PLKMdac"
      },
      "outputs": [],
      "source": [
        "# Plot traing\n",
        "training_history_try1 = pd.DataFrame(history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "oJ_iSIsjQh38",
        "outputId": "76458a61-d70f-42ee-afce-749114f811ac"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 3.3))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax_twin = ax.twinx()\n",
        "    plot_training(\n",
        "        ax,\n",
        "        ax_twin,\n",
        "        training_history_try1,\n",
        "        metric=\"root_mean_squared_error\",\n",
        "        metric_label=\"RMSE\",\n",
        "        halflife=20,\n",
        "    )\n",
        "    ax.set_yscale(\"log\")\n",
        "    ax_twin.set_yscale(\"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPJ4nXZtAIVT"
      },
      "source": [
        "#### Model result analysis\n",
        "\n",
        "Let's compare the model's predictions with the Reynolds number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z01ZChgQpV8",
        "outputId": "4367007a-eaab-42cd-c389-da0096e6c095"
      },
      "outputs": [],
      "source": [
        "# Predictions are generated using the model over the test set,\n",
        "# which the model has never seen during training.\n",
        "real_test_Re = np.array(labels_test_try1)\n",
        "prediction_test_Re = model.predict(np.array(features_test_try1))\n",
        "real_train_Re = np.array(labels_train_try1)\n",
        "prediction_train_Re = model.predict(np.array(features_train_try1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "FaNAI-wLQ9Um",
        "outputId": "047c0241-eef5-4ef2-d5eb-d56119eb84f4"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    axins = ax.inset_axes([0.18, 0.65, 0.32, 0.32])\n",
        "    plot_regression(\n",
        "        ax, axins, real_train_Re, prediction_train_Re, real_test_Re, prediction_test_Re\n",
        "    )\n",
        "    ax.set_xscale(\"log\")\n",
        "    ax.set_yscale(\"log\")\n",
        "    axins.set_xscale(\"log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bgFKKuaKPl5"
      },
      "source": [
        "### Exercise 1: Activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caWn5PJLK8c8"
      },
      "source": [
        "Let's explore some of the activation functions provided by`Tensorflow`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "id": "t3CvAItxLrzG",
        "outputId": "54c78b7d-c525-47b5-f9eb-979a44384a8c"
      },
      "outputs": [],
      "source": [
        "# List of activation function names\n",
        "activation_functions = [\n",
        "    \"relu\",\n",
        "    \"elu\",\n",
        "    \"selu\",\n",
        "    \"gelu\",\n",
        "    \"swish\",\n",
        "    \"mish\",\n",
        "    \"linear\",\n",
        "    \"exponential\",\n",
        "    \"softplus\",\n",
        "    \"tanh\",\n",
        "    \"softsign\",\n",
        "    \"sigmoid\",\n",
        "]\n",
        "\n",
        "# Create TensorFlow constant input\n",
        "x = tf.constant(np.linspace(-5, 5, 100), dtype=tf.float32)\n",
        "\n",
        "# Create a grid of subplots\n",
        "fig = plt.figure(figsize=(14, 10))\n",
        "gs = fig.add_gridspec(3, 4, hspace=0.4, wspace=0.3)\n",
        "axs = gs.subplots()\n",
        "\n",
        "# Plot activation functions\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    for i, activation_name in enumerate(activation_functions):\n",
        "        row, col = i // 4, i % 4\n",
        "        activation_func = getattr(tf.keras.activations, activation_name)\n",
        "        y = activation_func(x)\n",
        "\n",
        "        axs[row, col].plot(x, y, label=activation_name.capitalize())\n",
        "        axs[row, col].set_xlabel(\"$x$ (-)\")\n",
        "        axs[row, col].set_ylabel(\"$f(x)$ (-)\")\n",
        "        axs[row, col].set_title(activation_name.capitalize())\n",
        "        axs[row, col].grid()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-5_855fA5EL"
      },
      "source": [
        "For our regression model, determining the optimal activation function is essential. Let's perform a sensitivity analysis together by retraining the model with various activation functions available in TensorFlow. This exploration will help us identify the activation function that yields the best performance for our regression task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4Utp_XFA5EL"
      },
      "outputs": [],
      "source": [
        "ACTIVATION = \"linear\"  # <--- Place here the one with the best metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51p6mvGoDGpq"
      },
      "source": [
        "### Second Try: Increase inputs information\n",
        "\n",
        "In this second experiment, we aim to explore the impact of increasing the dimensionality of the problem by incorporating additional information as inputs to the model, not limited to just the velocity profile.\n",
        "\n",
        "Within our dataset, we possess valuable information about the physical properties of the fluid ($\\mu \\,\\text{(Pas)}$, $\\rho \\,\\mathrm{(kg/m^3)}$) and the geometric characteristics of the experimental setup ($L \\,\\text{(m)}$, $R \\,\\text{(m3)}$). These data are readily available for the model and can enhance its performance.\n",
        "\n",
        "Furthermore, we can compute the partial derivative of the velocity profile $\\dfrac{\\partial u_y}{\\partial x}$, which we will later discover is closely linked to the velocity stress tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhWH4j94A5EM"
      },
      "source": [
        "#### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhKG0b0cA5EM"
      },
      "source": [
        "Since the input shape changes drastically, as a rule of thumb, we increase the width of the model.\n",
        "\n",
        "The sketch below represents the proposed neural network architecture.\n",
        "\n",
        "<img style=\"display: block; margin: auto;\" alt=\"NN sketch\" src=\"https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/assets/img/NN_model2_try2.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPsGX8-vDGpv"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ecr9674EDGpw"
      },
      "outputs": [],
      "source": [
        "# Note: we are going to use all the data incluse the phisical properties (mu(Pas),\trho(kg/m3))\n",
        "# and the gemetrical data (L(m),\tR(m))\n",
        "features_train_try2 = pd.read_csv(\n",
        "    os.path.join(\"model_2\", \"train\", \"features.csv\"), index_col=False\n",
        ")\n",
        "features_test_try2 = pd.read_csv(\n",
        "    os.path.join(\"model_2\", \"test\", \"features.csv\"), index_col=False\n",
        ")\n",
        "labels_train_try2 = pd.read_csv(\n",
        "    os.path.join(\"model_2\", \"train\", \"labels.csv\"), index_col=False\n",
        ")\n",
        "labels_test_try2 = pd.read_csv(\n",
        "    os.path.join(\"model_2\", \"test\", \"labels.csv\"), index_col=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb5KrXj-veq-"
      },
      "source": [
        "Let's increase futher more the input information computing the derivative\n",
        "\n",
        "$$ \\left[\\nabla \\mathbf{u} \\right]_{ij} = \\partial_i u_j =  \\dfrac{\\partial u_y}{\\partial x} \\approx \\dfrac{u_y^n+1 - u_y^n}{\\Delta x}$$\n",
        "\n",
        "Note that for the Navier-Stoke for compressible fluids ($\\rho = \\rho_0 = \\mathrm{const}(t)$):\n",
        "\n",
        "$$ \\rho_0\\dfrac{\\partial \\mathbf{u}}{\\partial t} + \\rho_0(\\mathbf{u} \\cdot \\nabla) \\mathbf{u} = - \\nabla p + \\nabla\\cdot\\mathbf{\\tau}   + \\rho_0\\mathbf{g}  $$\n",
        "\n",
        "where the stress tensor $\\mathbf{\\tau}$\n",
        "$$ \\mathbf{\\tau} =  \\mu \\left(\\nabla\\mathbf{u} + \\left(\\nabla\\mathbf{u}\\right)^\\text{T} \\right) $$\n",
        "\n",
        "So what we are including as information is proportional to the $ij=ji$ component of the sress tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_5UtPcl2s7-"
      },
      "outputs": [],
      "source": [
        "# Extract the velocity data and radii from the training and test datasets\n",
        "velocity_train = features_train_try2.iloc[:, 4:].values\n",
        "R_train = features_train_try2[\"R(m)\"].values\n",
        "velocity_test = features_test_try2.iloc[:, 4:].values\n",
        "R_test = features_test_try2[\"R(m)\"].values\n",
        "\n",
        "# Determine the number of columns (velocity components) in the data\n",
        "N = velocity_train.shape[1]\n",
        "\n",
        "# Calculate the velocity gradient (dudx) for the training dataset\n",
        "dudx_train = pd.DataFrame(\n",
        "    data=(velocity_train[:, :-1] - velocity_train[:, 1:])\n",
        "    / (R_train[..., np.newaxis].repeat(N - 1, axis=1) / N),\n",
        "    columns=[f\"dudx[{i:d}]\" for i in range(N - 1)],\n",
        ")\n",
        "\n",
        "# Calculate the velocity gradient (dudx) for the test dataset\n",
        "dudx_test = pd.DataFrame(\n",
        "    data=(velocity_test[:, :-1] - velocity_test[:, 1:])\n",
        "    / (R_test[..., np.newaxis].repeat(N - 1, axis=1) / N),\n",
        "    columns=[f\"dudx[{i:d}]\" for i in range(N - 1)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNBAfgq4Cwea"
      },
      "source": [
        "Let's glue the new data to the input features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc7xcneNCu7D"
      },
      "outputs": [],
      "source": [
        "features_train_try2_full = pd.concat([features_train_try2, dudx_train], axis=1)\n",
        "features_test_try2_full = pd.concat([features_test_try2, dudx_test], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "uNuY7kJI_HOH",
        "outputId": "0ceccdff-e25a-485f-a387-baf905599f74"
      },
      "outputs": [],
      "source": [
        "features_train_try2_full.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrbdH56yvX8f"
      },
      "source": [
        "#### Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr5nohznA5EP"
      },
      "outputs": [],
      "source": [
        "def build_model_2_wide(n_cols: int, out_activation: str) -> tf.keras.models.Sequential:\n",
        "    \"\"\"\n",
        "    Build a the first possible architecture for our neural network model.\n",
        "\n",
        "    Args:\n",
        "        n_cols (int): Number of input features.\n",
        "        out_activation (str): Output activation function.\n",
        "    Returns:\n",
        "        tf.keras.models.Sequential: A Keras Sequential model.\n",
        "    \"\"\"\n",
        "    model = Sequential(\n",
        "        [\n",
        "            # Input layer with the specified input shape\n",
        "            layers.InputLayer(input_shape=(n_cols,), name=\"input_layer\"),\n",
        "            # Add the first hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(128, activation=\"relu\", name=\"hidden_layer_1\"),\n",
        "            # Add the second hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(128, activation=\"relu\", name=\"hidden_layer_2\"),\n",
        "            # Add the third hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(64, activation=\"relu\", name=\"hidden_layer_3\"),\n",
        "            # Add the fourth hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(64, activation=\"relu\", name=\"hidden_layer_4\"),\n",
        "            # Add the output layer with a single perceptron (we expect a True/False answer) and sigmoid activation\n",
        "            layers.Dense(1, activation=out_activation, name=\"output_layer\"),\n",
        "        ]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hqs1fJSDGpx"
      },
      "source": [
        "Let's call the function to build our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT32tgwzDGpy"
      },
      "outputs": [],
      "source": [
        "model_v2 = build_model_2_wide(features_train_try2_full.shape[1], ACTIVATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETkrPdqdDGpy"
      },
      "source": [
        "#### Model Summary and Structure Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiCvs-h4DGpz",
        "outputId": "4a515fef-ca2a-4815-ae81-67f2cde5a139"
      },
      "outputs": [],
      "source": [
        "model_v2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "rkgcOVdODGp0",
        "outputId": "9208e775-52e5-45c5-ff4c-cc5b1189c338"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model=model_v2, rankdir=\"LR\", dpi=72, show_shapes=True, show_layer_activations=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5jlwHq9DGp1"
      },
      "source": [
        "#### Model Compilation And Training\n",
        "\n",
        "Now we will compile our neural network model. Model compilation involves defining key components, such as the loss function, optimizer, learning rate, and metrics. And then we train the new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELYRHn_rDGp3"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "epochs = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6GZ2577FtjP",
        "outputId": "774ea371-f1ca-444c-d47a-2822c514fc24"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    # These initla lines of code are repeated because it must be defined\n",
        "    # inside the parallelization context for efficient GPU/TPU utilization.\n",
        "\n",
        "    # Step 1: Building the Model\n",
        "    model_v2 = build_model_2_wide(features_train_try2_full.shape[1], ACTIVATION)\n",
        "\n",
        "    # Step 2: Compiling the Model\n",
        "    loss = losses.MeanAbsolutePercentageError()\n",
        "    optimizer = optimizers.Adam(\n",
        "        learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08\n",
        "    )\n",
        "    metrics = [\n",
        "        tf.keras.metrics.RootMeanSquaredError(),\n",
        "        tf.keras.metrics.MeanSquaredError(),\n",
        "    ]\n",
        "    model_v2.compile(\n",
        "        optimizer,\n",
        "        loss,\n",
        "        metrics,\n",
        "    )\n",
        "\n",
        "    # Step 3: Training the Model\n",
        "    history = model_v2.fit(\n",
        "        np.array(\n",
        "            features_train_try2_full\n",
        "        ),  # Convert the data into an array before feeding it\n",
        "        np.array(labels_train_try2).astype(\"float\"),\n",
        "        batch_size,\n",
        "        epochs,\n",
        "        validation_data=(\n",
        "            np.array(features_test_try2_full),\n",
        "            np.array(labels_test_try2).astype(\"float\"),\n",
        "        ),  # Validation set\n",
        "        verbose=1,  # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRGge0ulDGp5"
      },
      "source": [
        "#### Plot Training Progress\n",
        "\n",
        "Now, we can plot the training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgX_oWFUDGp5"
      },
      "outputs": [],
      "source": [
        "# Plot traing loop\n",
        "training_history_try2 = pd.DataFrame(history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "smC-knFvKt4G",
        "outputId": "9cc088df-158b-4a29-8a5d-da08270c2711"
      },
      "outputs": [],
      "source": [
        "training_history_try2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "8A-jjWGTLhaS",
        "outputId": "2bd8270c-e732-439f-9e00-c3f934c63c1e"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 3.3))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax_twin = ax.twinx()\n",
        "    plot_training(\n",
        "        ax,\n",
        "        ax_twin,\n",
        "        training_history_try2,\n",
        "        metric=\"root_mean_squared_error\",\n",
        "        metric_label=\"RMSE\",\n",
        "        halflife=50,\n",
        "    )\n",
        "    ax.set_yscale(\"log\")\n",
        "    ax_twin.set_yscale(\"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLm4E3KeLhaT"
      },
      "source": [
        "#### Model result analysis\n",
        "\n",
        "Let's compare the model's predictions with the Reynolds number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O0SVTkvLhaU",
        "outputId": "d0e3b780-ee28-4a7d-a3ad-38a15c7abe07"
      },
      "outputs": [],
      "source": [
        "# Predictions are generated using the model over the test set,\n",
        "# which the model has never seen during training.\n",
        "real_test_Re = np.array(labels_test_try2)\n",
        "prediction_test_Re = model_v2.predict(np.array(features_test_try2_full))\n",
        "real_train_Re = np.array(labels_train_try2)\n",
        "prediction_train_Re = model_v2.predict(np.array(features_train_try2_full))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "V9ewQwUBLhaU",
        "outputId": "fa97ed43-1bd4-4ddf-9eb4-57c9f570d8e0"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    axins = ax.inset_axes([0.18, 0.65, 0.32, 0.32])\n",
        "    plot_regression(\n",
        "        ax, axins, real_train_Re, prediction_train_Re, real_test_Re, prediction_test_Re\n",
        "    )\n",
        "    ax.set_xscale(\"log\")\n",
        "    ax.set_yscale(\"log\")\n",
        "    axins.set_xscale(\"log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIyaJRljkVEN"
      },
      "source": [
        "### Third Try: Normalization on the fly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xpeqHknA5Ee"
      },
      "source": [
        "#### Architecture\n",
        "\n",
        "Since the input shape changes drastically, as a rule of thumb, we increase the width of the model.\n",
        "\n",
        "The sketch below represents the proposed neural network architecture.\n",
        "\n",
        "<img style=\"display: block; margin: auto;\" alt=\"NN sketch\" src=\"https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/assets/img/NN_model2_try3.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IodHrbM7kVEP"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiZ7AJMxkVEP"
      },
      "outputs": [],
      "source": [
        "features_train_try3 = pd.read_csv(\n",
        "    os.path.join(\"model_2\", \"train\", \"features.csv\"), index_col=False\n",
        ").iloc[\n",
        "    :, 4:\n",
        "]  # Note: we drop the first 4 columns to study only the velocity profile\n",
        "features_test_try3 = pd.read_csv(\n",
        "    os.path.join(\"model_2\", \"test\", \"features.csv\"), index_col=False\n",
        ").iloc[\n",
        "    :, 4:\n",
        "]  # Note: we drop the first 4 columns to study only the velocity profile\n",
        "labels_train_try3 = pd.read_csv(\n",
        "    os.path.join(\"model_2\", \"train\", \"labels.csv\"), index_col=False\n",
        ")\n",
        "labels_test_try3 = pd.read_csv(\n",
        "    os.path.join(\"model_2\", \"test\", \"labels.csv\"), index_col=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQVFDcmXkVES"
      },
      "source": [
        "#### Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo_DdWSnkVEV"
      },
      "source": [
        "##### Make custom *normalization* layer\n",
        "\n",
        "It is important to normalize input data for neural network (NN) training for the following reasons:\n",
        "\n",
        "* **Stability and Convergence**: Normalization helps in stabilizing the training process. When input features have significantly different scales, it can lead to slow convergence or the network getting stuck in local minima during training. By normalizing the data, the optimization algorithm can work more effectively and converge faster.\n",
        "\n",
        "* **Gradient Descent**: During gradient descent, the learning rate plays a crucial role. If input features are not normalized, the learning rate might need to be adjusted manually to prevent divergence or slow convergence. Normalized data reduces the sensitivity to the learning rate, making it easier to find a good learning rate automatically.\n",
        "\n",
        "* **Improved Generalization**: Normalization can lead to better generalization of the model.\n",
        "\n",
        "* **Model Interpretability**: Normalized data can make it easier to interpret the importance of features in the model. Without normalization, the magnitude of feature weights in the neural network may not accurately reflect their importance.\n",
        "\n",
        "Now, let's write a custom layer `VelocityNormalization`` that perform the normalization of the velocity profile, and at the same time, add the maximum velocity to the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWWqjt5-7om1"
      },
      "outputs": [],
      "source": [
        "class VelocityNormalization(layers.Layer):\n",
        "    def __init__(self, axis=1, name=\"velocity_normalization\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.axis = axis\n",
        "\n",
        "    def call(self, batch):\n",
        "        batch_max = tf.reduce_max(batch, axis=self.axis, keepdims=True)\n",
        "        normalized_batch = batch / batch_max\n",
        "        normalized_with_scale = tf.concat([normalized_batch, batch_max], axis=self.axis)\n",
        "        return normalized_with_scale\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[self.axis] += 1  # One additional element for the scale vector\n",
        "        return tuple(output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOT6hGMU-Lc4"
      },
      "outputs": [],
      "source": [
        "# Convert the NumPy array to a TensorFlow constant\n",
        "input_data_tf = tf.constant(features_train_try3.iloc[:4, :].values, dtype=tf.float32)\n",
        "\n",
        "# Create a VelocityNormalization layer\n",
        "batch_max_norm_layer = VelocityNormalization()\n",
        "\n",
        "# Normalize the input data using the VelocityNormalization layer\n",
        "normalized_data = batch_max_norm_layer(input_data_tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "6SwX8Q2k-ZND",
        "outputId": "b045e7ac-bb33-4ed5-85d1-4ea87b9c5233"
      },
      "outputs": [],
      "source": [
        "# Create a plot of random data from the datasets\n",
        "fig = plt.figure(figsize=(8, 3))\n",
        "# Set the style of the plot to \"seaborn-v0_8-paper\"\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax1 = fig.add_subplot(121)\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    # Plot the data and labels it with Re value\n",
        "    for i in range(input_data_tf.shape[0]):\n",
        "        ax1.plot(input_data_tf[i].numpy())\n",
        "        ax2.plot(normalized_data[i].numpy()[:-1])\n",
        "\n",
        "    # Adding labels and titles to the plot\n",
        "    ax1.set_xlabel(\"i (-)\")\n",
        "    ax1.set_ylabel(\"$u$ (m/s)\")\n",
        "    ax2.set_xlabel(\"i (-)\")\n",
        "    ax2.set_ylabel(\"$u/u_{max}$ (-)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJDe5eErEvMm"
      },
      "outputs": [],
      "source": [
        "def build_model_2_norm(n_cols: int, out_activation: str) -> tf.keras.models.Sequential:\n",
        "    \"\"\"\n",
        "    Build a the second possible architecture for our neural network model.\n",
        "\n",
        "    Args:\n",
        "        n_cols (int): Number of input features.\n",
        "        out_activation (str): Output activation function.\n",
        "    Returns:\n",
        "        tf.keras.models.Sequential: A Keras Sequential model.\n",
        "    \"\"\"\n",
        "    model = Sequential(\n",
        "        [\n",
        "            # Input layer with the specified input shape\n",
        "            layers.InputLayer(input_shape=(n_cols,), name=\"input_layer\"),\n",
        "            # Normalization layer\n",
        "            VelocityNormalization(name=\"normalization\"),\n",
        "            # Add the first hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(64, activation=\"relu\", name=\"hidden_layer_1\"),\n",
        "            # Add the second hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(64, activation=\"relu\", name=\"hidden_layer_2\"),\n",
        "            # Add the third hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(32, activation=\"relu\", name=\"hidden_layer_3\"),\n",
        "            # Add the forth hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(32, activation=\"relu\", name=\"hidden_layer_4\"),\n",
        "            # Add the output layer with a single perceptron (we expect a True/False answer) and sigmoid activation\n",
        "            layers.Dense(1, activation=out_activation, name=\"output_layer\"),\n",
        "        ]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yU1_hxEUkVEV"
      },
      "outputs": [],
      "source": [
        "model_v3 = build_model_2_norm(features_train_try3.shape[1], ACTIVATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwHeq66hkVEV"
      },
      "source": [
        "#### Model Summary and Structure Visualization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJNokn1VkVEW",
        "outputId": "9196a624-4b5e-4ae5-cf58-c95f963a4917"
      },
      "outputs": [],
      "source": [
        "model_v3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "PMpMmSMUkVEX",
        "outputId": "65278b59-0509-4c88-9661-21faa5a61019"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model=model_v3, rankdir=\"LR\", dpi=72, show_shapes=True, show_layer_activations=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A62XuPpYkVEX"
      },
      "source": [
        "#### Model Compilation And Training\n",
        "\n",
        "Now we will compile our neural network model. Model compilation involves defining key components, such as the loss function, optimizer, learning rate, and metrics. And then we train the new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55uUk5x4kVEY"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "epochs = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osX48yXTkVEY",
        "outputId": "2caed249-fe64-42c3-8035-f86bf3ab98f2"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    # These initla lines of code are repeated because it must be defined\n",
        "    # inside the parallelization context for efficient GPU/TPU utilization.\n",
        "\n",
        "    # Step 1: Building the Model\n",
        "    model_v3 = build_model_2_norm(features_train_try3.shape[1], ACTIVATION)\n",
        "\n",
        "    # Step 2: Compiling the Model\n",
        "    loss = losses.MeanAbsolutePercentageError()\n",
        "    optimizer = optimizers.Adam(\n",
        "        learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08\n",
        "    )\n",
        "    metrics = [\n",
        "        tf.keras.metrics.RootMeanSquaredError(),\n",
        "        tf.keras.metrics.MeanSquaredError(),\n",
        "    ]\n",
        "    model_v3.compile(\n",
        "        optimizer,\n",
        "        loss,\n",
        "        metrics,\n",
        "    )\n",
        "\n",
        "    # Step 3: Training the Model\n",
        "    history = model_v3.fit(\n",
        "        np.array(\n",
        "            features_train_try3\n",
        "        ),  # Convert the data into an array before feeding it\n",
        "        np.array(labels_train_try3).astype(\"float\"),\n",
        "        batch_size,\n",
        "        epochs,\n",
        "        validation_data=(\n",
        "            np.array(features_test_try3),\n",
        "            np.array(labels_test_try3).astype(\"float\"),\n",
        "        ),  # Validation set\n",
        "        verbose=1,  # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSnJa6HgkVEZ"
      },
      "source": [
        "#### Plot Training Progress\n",
        "\n",
        "Now, we can plot the training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_44Xzob8kVEZ"
      },
      "outputs": [],
      "source": [
        "# Plot traing loop\n",
        "training_history_try3 = pd.DataFrame(history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UgedmnpYkVEa",
        "outputId": "3a4de892-2a2e-4ba8-dd11-1446beef82f3"
      },
      "outputs": [],
      "source": [
        "training_history_try3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "fgCBrLSUkVEa",
        "outputId": "4635920c-66ca-4352-ab7d-2ed5584f5ae3"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 3.3))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax_twin = ax.twinx()\n",
        "    plot_training(\n",
        "        ax,\n",
        "        ax_twin,\n",
        "        training_history_try3,\n",
        "        metric=\"root_mean_squared_error\",\n",
        "        metric_label=\"RMSE\",\n",
        "        halflife=10,\n",
        "    )\n",
        "    # ax.set_yscale(\"log\")\n",
        "    ax_twin.set_yscale(\"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRaaCy6DkVEb"
      },
      "source": [
        "#### Model result analysis\n",
        "\n",
        "Let's compare the model's predictions with the Reynolds number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfcwlH1bkVEb",
        "outputId": "e4d447b0-5d99-4c7b-dc8d-30a45a19a95b"
      },
      "outputs": [],
      "source": [
        "# Predictions are generated using the model over the test set,\n",
        "# which the model has never seen during training.\n",
        "real_test_Re = np.array(labels_test_try3)\n",
        "prediction_test_Re = model_v3.predict(np.array(features_test_try3))\n",
        "real_train_Re = np.array(labels_train_try3)\n",
        "prediction_train_Re = model_v3.predict(np.array(features_train_try3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "Hj6ZhkaZkVEb",
        "outputId": "8fecc767-34ed-4c2f-9987-eb41683d387f"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    axins = ax.inset_axes([0.18, 0.65, 0.32, 0.32])\n",
        "    plot_regression(\n",
        "        ax, axins, real_train_Re, prediction_train_Re, real_test_Re, prediction_test_Re\n",
        "    )\n",
        "    ax.set_xscale(\"log\")\n",
        "    ax.set_yscale(\"log\")\n",
        "    axins.set_xscale(\"log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUhdbN6lA5Es"
      },
      "source": [
        "### Exercise 2: Can NN predict Velocity profile\n",
        "\n",
        "Knowing the physical properties of the fluid ($\\mu \\,\\text{(Pas)}$, $\\rho \\,\\mathrm{(kg/m^3)}$) and the geometric characteristics of the experimental setup ($L \\,\\text{(m)}$, $R \\,\\text{(m3)}$) and the Raynolds number ($\\mathrm{Re} \\,\\mathrm{(-)}$), can we build a model that predict the velocity profile?\n",
        "\n",
        "steps:\n",
        "- Design a architecture\n",
        "- Chose a loss function and metrics\n",
        "- Compile the model\n",
        "- Training\n",
        "- Visualize the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bxf3YIYBkVEc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
