{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymEstwuvvZT6"
      },
      "source": [
        "# Neural Network model for fluid dynamics (Part 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh79OT9Cf3Jh"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paolodeangelis/Sistemi_a_combustione/blob/main/2.1-NN_Reynolds_P1.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoK6XeQ9AIUv"
      },
      "source": [
        "In this Jupyter we are going to build our first neural networ for studing ingegneristic problems.\n",
        "\n",
        "A basic explanation of neural networks (NN) is that they are computational models inspired by the human brain. They comprise various interconnected components, which include:\n",
        "\n",
        "* **Neurons** (Evolution of the Frank Rosenblatt's Perceptron [[1]](#1).)\n",
        "    - Neurons are the basic processing units in a neural network.\n",
        "    - They receive input data, apply mathematical operations, and produce an output.\n",
        "    - Neurons have evolved from the original concept of perceptrons, which had binary outputs. Modern neurons use continuous activation functions to model complex patterns in data.\n",
        "\n",
        "* **Activation Function**\n",
        "    - Activation functions introduce non-linearity into neural networks.\n",
        "    - They determine the output of a neuron based on its weighted input.\n",
        "    - Common activation functions include the sigmoid, ReLU (Rectified Linear Unit), and tanh, allowing neurons to capture complex relationships in data.\n",
        "\n",
        "* **Hidden Layer**\n",
        "    - The hidden layer is an intermediary layer between the input and output layers.\n",
        "    - It is crucial for the network's ability to learn and represent complex patterns in the data.\n",
        "    - The number of neurons and the structure of the hidden layers greatly affect a neural network's performance.\n",
        "\n",
        "![NN sketch](https://github.com/paolodeangelis/Sistemi_a_combustione/raw/main/assets/img/NN_intro.png)\n",
        "\n",
        "In summary, neural networks use interconnected neurons that have evolved from the simple perceptron concept. They leverage activation functions to introduce non-linearity and hidden layers to model intricate data patterns. This structure allows neural networks to excel in various machine learning tasks, from image recognition to natural language processing.\n",
        "\n",
        "<a id=\"1\">[1]</a> : [Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6), 386.](http://homepages.math.uic.edu/~lreyzin/papers/rosenblatt58.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem\n",
        "\n",
        "We"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F98l6SKFAIUw"
      },
      "source": [
        "As a general guideline for an ML/NN (Machine Learning/Neural Network) project, we follow the A. GÃ©ron checklist:\n",
        "\n",
        "1. Frame the problem and look at the big picture. (In our case: Studying the flow in a channel)\n",
        "2. Get the data. (As in: Acquiring velocity profiles from experiments)\n",
        "3. Explore the data to gain insights. (See Section 2 - *Data Download and Inspection*)\n",
        "4. Prepare the data to expose underlying patterns to Machine Learning algorithms. (Refer to Section 3 - *Downsampling the Database*)\n",
        "5. Explore different models and shortlist the best ones. (See Section 4 - *Model 1: Binary Classification*)\n",
        "6. Fine-tune your models and combine them for an optimized solution. (Exercise)\n",
        "7. Present your solution. (Skipped, as it's not relevant for this lab class)\n",
        "8. Launch, monitor, and maintain your system. (Skipped, as it's not relevant for this lab class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iD3JY4cwt_0"
      },
      "source": [
        "## Installing Libraries\n",
        "\n",
        "We begin by installing the necessary libraries to support our data manipulation, visualization, and deep learning modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mSxW9TCw3jk",
        "outputId": "083a2aa6-b405-4828-94b5-c42cc37893f5"
      },
      "outputs": [],
      "source": [
        "%pip install numpy pandas scipy matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGsKDRlMAIUz"
      },
      "source": [
        "And now we import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpatprDJxBlY"
      },
      "outputs": [],
      "source": [
        "import os  # Operating system-related functions\n",
        "import pathlib  # Path manipulation and filesystem-related operations\n",
        "\n",
        "import matplotlib.pyplot as plt  # Data visualization library\n",
        "import numpy as np  # Numerical computing library\n",
        "import pandas as pd  # Data manipulation and analysis library\n",
        "import tensorflow as tf  # Deep learning framework for neural networks\n",
        "from tensorflow.keras import Sequential, layers, losses, optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaYFSqhnvi1Y"
      },
      "source": [
        "## Data download and inspection\n",
        "\n",
        "In this section, we will download the dataset and inspect its components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igeIcVy1AIU0"
      },
      "source": [
        "### Download dataset files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9SsgVKDvZT_",
        "outputId": "72ac1016-5ee1-48e3-9c61-8626a21adcdd"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/data/lab2/velprof-Re.csv\n",
        "!wget https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/data/lab2/velprof-data.csv\n",
        "!wget https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/data/lab2/velprof-space.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUxOmbg2zSjx"
      },
      "source": [
        "### Read Reynolds (the labels for our model)\n",
        "\n",
        "We'll start by reading the Reynolds data, which serves as the labels for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uq00UhcGwRiW",
        "outputId": "cbd3d025-33b1-4a99-c069-25b03cdf9fb9"
      },
      "outputs": [],
      "source": [
        "data_Re = pd.read_csv(\"velprof-Re.csv\", index_col=False)\n",
        "data_Re.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wea1geIGzWwx"
      },
      "source": [
        "### Read the data file (the features for our model)\n",
        "\n",
        "Next, we'll read the data file containing the features for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "rhRwBsmCxuWG",
        "outputId": "c5e85edf-b7a8-417e-f4cc-6a137a1680d8"
      },
      "outputs": [],
      "source": [
        "data_v = pd.read_csv(\"velprof-data.csv\", index_col=False)\n",
        "data_v.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxvKEMQ4AIU2"
      },
      "source": [
        "Let's also read another data file that contains information about space discretization, which, in our analogy, represents the probe's position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "GMg5TsrxNexm",
        "outputId": "a491a33d-d25b-4b44-df94-5da3383e0e7f"
      },
      "outputs": [],
      "source": [
        "data_r = pd.read_csv(\"velprof-space.csv\", index_col=False)\n",
        "data_r.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0RERTjvAIU3"
      },
      "source": [
        "We can also merge the two datasets, which include both labels and features (it can be useful later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frCOBJ6C2fnn"
      },
      "outputs": [],
      "source": [
        "data_all = pd.concat([data_v, data_Re], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCbieJGL5SOl"
      },
      "source": [
        "### Data Inspection and Visualization\n",
        "\n",
        "In this section, we will perform the following tasks:\n",
        "1. Plot a random velocity profile to visually examine the data.\n",
        "2. Compute and create a correlation matrix plot to analyze the relationships between variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "xNrjoZEL5Swe",
        "outputId": "13b2d827-59b9-42b7-b8ae-b2d6564e1aa2"
      },
      "outputs": [],
      "source": [
        "# Create a plot of random data from the datasets\n",
        "fig = plt.figure(figsize=(5, 3))\n",
        "indx = np.random.randint(data_v.shape[0] - 1)\n",
        "\n",
        "# Set the style of the plot to \"seaborn-v0_8-paper\"\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    # Plot the data and labels it with Re value\n",
        "    ax.plot(\n",
        "        data_r.iloc[indx, :],\n",
        "        data_v.iloc[indx, 4:],\n",
        "        label=f\"Re = {data_Re.iloc[indx].values[0]:1.2f}\",\n",
        "    )\n",
        "\n",
        "    # Add a legend, labels, and titles to the plot\n",
        "    ax.legend(loc=\"lower left\")\n",
        "    ax.set_xlabel(\"r (m)\")\n",
        "    ax.set_ylabel(\"u (m/s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_ltRySaYhQI"
      },
      "source": [
        "Let's study the data and its correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "Iuh0b5LFPlin",
        "outputId": "bbd844bc-cb7a-467e-827a-32a0316c73d9"
      },
      "outputs": [],
      "source": [
        "# Correlation Matrix Plot\n",
        "\n",
        "# Define the number of bins and create a figure\n",
        "Nbins = 50\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "\n",
        "# Define the labels for the correlation matrix\n",
        "labels = [\n",
        "    \"Re(-)\",\n",
        "    \"mu(Pas)\",\n",
        "    \"rho(kg/m3)\",\n",
        "    \"L(m)\",\n",
        "    \"R(m)\",\n",
        "    \"vel[0](m/s)\",\n",
        "    \"vel[1](m/s)\",\n",
        "]\n",
        "N = len(labels)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = data_all.corr()\n",
        "\n",
        "# Apply the \"seaborn-v0_8-paper\" style\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    grid = fig.add_gridspec(N, N, wspace=0.03, hspace=0.03)\n",
        "    ax = []\n",
        "    cmap = plt.get_cmap(\"Blues\")\n",
        "\n",
        "    for i in range(N):\n",
        "        for j in range(N):\n",
        "            ax.append(fig.add_subplot(grid[i, j]))\n",
        "\n",
        "            # Plot scatter for lower triangle, correlation value for upper triangle, and histograms for diagonal\n",
        "            if j < i:\n",
        "                ax[-1].scatter(data_all[labels[j]], data_all[labels[i]], s=1, alpha=0.1)\n",
        "            elif j > i:\n",
        "                corr = corr_matrix.loc[labels[j], labels[i]]\n",
        "                ax[-1].text(0.5, 0.5, f\"{corr:1.3f}\", ha=\"center\", va=\"center\")\n",
        "                ax[-1].set_facecolor(cmap(np.abs(corr)))\n",
        "                ax[-1].set_xticks([])\n",
        "                ax[-1].set_yticks([])\n",
        "            else:\n",
        "                ax[-1].hist(data_all[labels[i]], bins=Nbins, alpha=0.5)\n",
        "\n",
        "            # Set labels for x and y axes\n",
        "            if j == 0 or i == N - 1:\n",
        "                if j == 0 and i != N - 1:\n",
        "                    ax[-1].set_ylabel(labels[i])\n",
        "                    ax[-1].set_xticklabels([])\n",
        "                elif i == N - 1 and j != 0:\n",
        "                    ax[-1].set_xlabel(labels[j])\n",
        "                    ax[-1].set_yticklabels([])\n",
        "                else:\n",
        "                    ax[-1].set_ylabel(labels[i])\n",
        "                    ax[-1].set_xlabel(labels[j])\n",
        "\n",
        "            # Remove tick labels\n",
        "            ax[-1].set_xticklabels([])\n",
        "            ax[-1].set_yticklabels([])\n",
        "\n",
        "    # Set the title for the correlation matrix\n",
        "    fig.suptitle(\"(Pearson) Correlation matrix\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jL2z4K07mzZ"
      },
      "source": [
        "## Downsampling the database\n",
        "\n",
        "The dataset contains too many data points (100,000), so we will reduce it to 5,000 by randomly selecting from the entire database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84bOvbDDVn8z"
      },
      "outputs": [],
      "source": [
        "# Get the total number of data points in data_v\n",
        "Nall = data_v.shape[0]\n",
        "\n",
        "# Define the desired number of smaller data points to select\n",
        "Nsmall = 5000\n",
        "\n",
        "# Initialize a random number generator with a specific seed for reproducibility\n",
        "rand_gen = np.random.default_rng(seed=1234)\n",
        "\n",
        "# Generate a random sample of indices without replacement from the range [0, Nall)\n",
        "# This will be used to select a subset of data_v and data_Re\n",
        "indx = rand_gen.choice(np.arange(Nall), size=Nsmall, replace=False)\n",
        "\n",
        "# Create smaller subsets of data_v and data_Re based on the randomly selected indices\n",
        "data_v_small = data_v.iloc[indx, :]\n",
        "data_Re_small = data_Re.iloc[indx, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r2sqhPeBNJA"
      },
      "source": [
        "let's store it as `.csv` (comma-separated values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3yWskJZBP5u"
      },
      "outputs": [],
      "source": [
        "data_v_small.to_csv(\"small-data.csv\", index=False)\n",
        "data_Re_small.to_csv(\"small-Re.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW8Xxqkj_gqw"
      },
      "source": [
        "## Model 1: Binary Classification\n",
        "\n",
        "In this section, we will employ our Neural-Network model for binary classification with the primary goal of distinguishing between turbulent and non-turbulent conditions. It's essential to clarify that, within the context of our database, \"non-turbulent\" encompasses both laminar and transitional conditions. Please note that for transitional conditions, the velocity profile results are obtained through interpolation, which may exhibit empirical inaccuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCp3ul3GFDUI"
      },
      "source": [
        "### Setup database\n",
        "\n",
        "We load the features and the labels of our first model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsuC9LV7FUgh"
      },
      "outputs": [],
      "source": [
        "features = pd.read_csv(\"small-data.csv\", index_col=False).iloc[\n",
        "    :, 4:\n",
        "]  # Note: we drop the first 4 columns to study only the velocity profile\n",
        "labels = pd.read_csv(\"small-Re.csv\", index_col=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akhVBQVCAIU9"
      },
      "source": [
        "Display the first few rows of the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "tutRAhKkGH6e",
        "outputId": "4d23edf8-fb27-4875-80af-784526675d3b"
      },
      "outputs": [],
      "source": [
        "features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzmrYgOKAIU9"
      },
      "source": [
        "Display the first few rows of the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "O_aJe56njcUu",
        "outputId": "bcca048b-e652-4db6-c59a-79e81e6c2a64"
      },
      "outputs": [],
      "source": [
        "labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot3bq9CWjaHX"
      },
      "source": [
        "Next, we will convert the labels from numerical (`float`) to boolean (`bool`) using the following criteria:\n",
        "\n",
        "* `True` when the flow is turbulent ($Re \\ge 10^4$).\n",
        "* `False` when the flow is non-turbulent, which includes both laminar and transitional regimes ($Re < 10^4$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBWgcn8xB0cO"
      },
      "outputs": [],
      "source": [
        "labels_Re = labels.pop(\"Re(-)\")\n",
        "labels[\"Turbolent\"] = labels_Re >= 1e4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP8pCchmAIU-"
      },
      "source": [
        "Display the first few rows of the updated labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gf4WNC7OkcJu",
        "outputId": "84f6bbff-a1af-4354-e20d-bb5f20bc6525"
      },
      "outputs": [],
      "source": [
        "labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuEFFf5G_ODX"
      },
      "source": [
        "To create a training and test dataset, we perform an 80/20 split.\n",
        "\n",
        "The purpose of this split is to reserve a portion of the data for testing the model's performance while ensuring that the two sets have a statistically similar distribution of features and labels. Since the database is already shuffled, we can conveniently take the first 1000 data points as the test set, given the total dataset size of 5000 items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhgLDXeS-ilW"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and test sets\n",
        "# Training set (80% of the data)\n",
        "labels_train = labels.iloc[1000:, :]\n",
        "features_train = features.iloc[1000:, :]\n",
        "labels_Re_train = labels_Re.iloc[1000:]\n",
        "\n",
        "# Test set (20% of the data)\n",
        "labels_test = labels.iloc[:1000, :]\n",
        "features_test = features.iloc[:1000, :]\n",
        "labels_Re_test = labels_Re.iloc[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1o08OhLlWjg"
      },
      "source": [
        "### Storing\n",
        "\n",
        "We will store the datasets and labels in a structured folder for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMrHqRUT-2Yh"
      },
      "outputs": [],
      "source": [
        "pathlib.Path(\"model_1\").mkdir(parents=True, exist_ok=True)  # Make a folder\n",
        "# Make train and test subfolders\n",
        "pathlib.Path(os.path.join(\"model_1\", \"train\")).mkdir(exist_ok=True)\n",
        "pathlib.Path(os.path.join(\"model_1\", \"test\")).mkdir(exist_ok=True)\n",
        "# Storing\n",
        "labels_train.to_csv(os.path.join(\"model_1\", \"train\", \"labels.csv\"), index=False)\n",
        "labels_test.to_csv(os.path.join(\"model_1\", \"test\", \"labels.csv\"), index=False)\n",
        "features_train.to_csv(os.path.join(\"model_1\", \"train\", \"features.csv\"), index=False)\n",
        "features_test.to_csv(os.path.join(\"model_1\", \"test\", \"features.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbxK0AFDAIVO"
      },
      "source": [
        "### First Try\n",
        "\n",
        "#### Building the Model with Keras and TensorFlow\n",
        "\n",
        "Now we define a function to create our neural network model using Keras and TensorFlow. This involves designing the layers, specifying their connections, and defining their behavior, including activation functions, regularization, dropout, and more.\n",
        "\n",
        "In a neural network, a *layer* serves as a fundamental building block responsible for processing data and extracting features. Layers are combined to form the architecture of the neural network. Each layer consists of one or more \"neurons\" (also known as \"nodes\" or \"units\").\n",
        "\n",
        "For our first model, we need to include the following types of layers:\n",
        "\n",
        "* **Input Layer**: This is the initial layer that receives raw input data. Its primary role is to pass the data to subsequent layers. The input layer typically has one neuron for each feature present in the input data.\n",
        "\n",
        "* **Hidden Layers**: These intermediate layers sit between the input and output layers. Hidden layers perform complex transformations on the data, enabling the network to learn and extract features from the input. Each neuron in a hidden layer receives input from multiple neurons in the previous layer.\n",
        "\n",
        "* **Output Layer**: The final layer in the neural network is responsible for producing the model's predictions. The number of neurons in the output layer depends on the nature of the problem being addressed. For binary classification (True/False or 0/1), a single neuron is common in the output layer. In multiclass classification or regression tasks, the number of output neurons can vary.\n",
        "\n",
        "In addition to layers, we also introduce the concept of an *activation function*. An activation function is a critical element within each neuron of a neural network. It dictates how the output of a neuron is calculated based on its input. The activation function introduces non-linearity into the model, allowing it to learn complex patterns and relationships within the data.\n",
        "\n",
        "Common activation functions include:\n",
        "\n",
        "* **ReLU (Rectified Linear Unit)**: ReLU is one of the most widely used activation functions. It returns the input value if it's positive and zero if it's negative. Mathematically, it can be represented as $f(x) = \\max(0, x)$. ReLU is effective in training deep networks and addressing the vanishing gradient problem.\n",
        "\n",
        "* **Sigmoid**: The sigmoid activation function is commonly used in the output layer of binary classification models. It squashes the output into a range between 0 and 1, which can be interpreted as a probability. Mathematically, it's expressed as $f(x) = \\dfrac{1}{1 + e^{-x}}$.\n",
        "\n",
        "* **Tanh (Hyperbolic Tangent)**: Tanh is similar to the sigmoid function but maps values to the range between -1 and 1. It is often used in hidden layers. Mathematically, it's defined as $f(x) = \\dfrac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$.\n",
        "\n",
        "The choice of the activation function depends on the specific problem and the architecture of the network. Each activation function has its own strengths and weaknesses, and selecting the right one is a crucial part of the network design process.\n",
        "\n",
        "In our case, we use the `relu` (Rectified Linear Unit) activation function in the first hidden layer and a smoother function like `sigmoid` for the output layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RToY7LfOluwF"
      },
      "outputs": [],
      "source": [
        "def build_model_1(n_cols: int) -> tf.keras.models.Sequential:\n",
        "    \"\"\"\n",
        "    Build a the first possible architecture for our neural network model.\n",
        "\n",
        "    Args:\n",
        "        n_cols (int): Number of input features.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.models.Sequential: A Keras Sequential model.\n",
        "    \"\"\"\n",
        "    model = Sequential(\n",
        "        [\n",
        "            # Input layer with the specified input shape\n",
        "            layers.InputLayer(input_shape=(n_cols,), name=\"input_layer\"),\n",
        "            # Add the first hidden layer with 64 perceptron and ReLU activation\n",
        "            layers.Dense(64, activation=\"relu\", name=\"hidden_layer_1\"),\n",
        "            # Add the output layer with a single perceptron (we expect a True/False answer) and sigmoid activation\n",
        "            layers.Dense(1, activation=\"sigmoid\", name=\"output_layer\"),\n",
        "        ]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mwLYp_WAkK1"
      },
      "source": [
        "Let's call the function to build our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Foqg8JhAksQ"
      },
      "outputs": [],
      "source": [
        "model = build_model_1(features.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl6f1wJQBX14"
      },
      "source": [
        "#### Model Summary and Structure Visualization\n",
        "\n",
        "Let's examine the summary of our compiled model and visualize its architecture.\n",
        "\n",
        "Given that the input data has 50 features and the first hidden layer consists of 64 neurons (each with weights $w$ and biases $b$), we have the following:\n",
        "\n",
        "- Number of parameters in the first hidden layer ($\\mathbf{w}_{h1}$): $50 \\times 64 = 3200$\n",
        "- Number of parameters in the first hidden layer biases ($\\mathbf{b}_{h1}$): $1 \\times 64 = 64$\n",
        "\n",
        "In addition, considering the output layer:\n",
        "\n",
        "- Number of parameters in the output layer weights ($\\mathbf{w}_{o}$): $64 \\times 1 = 64$\n",
        "- Number of parameters in the output layer biases ($\\mathbf{b}_{o}$): $64 \\times 1 = 1$\n",
        "\n",
        "This results in a total of 3329 degrees of freedom (dofs) within our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OswAkDOgCdOs",
        "outputId": "82331fc9-5052-456b-ea34-8007e83491d4"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "FraaL19gBh0h",
        "outputId": "20eea676-c1c7-4259-e266-c8dc0b363dcd"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model=model, rankdir=\"LR\", dpi=72, show_shapes=True, show_layer_activations=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zMitJulAIVQ"
      },
      "source": [
        "#### Model Compilation\n",
        "\n",
        "Now we will compile our neural network model. Model compilation involves defining key components, such as the loss function, optimizer, learning rate, and metrics.\n",
        "\n",
        "##### Loss Function\n",
        "\n",
        "For our binary classification task, we use the Binary Cross-Entropy loss function. The *Binary Cross-Entropy* loss is calculated as:\n",
        "\n",
        "$\\text{Binary Cross-Entropy Loss} = -\\frac{1}{N}\\sum_{i=1}^{N}\\left(y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)\\right) $\n",
        "\n",
        "where $N$ is the number of samples, $y_i$ represents the true labels, and $p_i$ is the predicted probability of the positive class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc5LQtQCAIVQ"
      },
      "outputs": [],
      "source": [
        "loss = losses.BinaryCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYEWcIv0AIVQ"
      },
      "source": [
        "##### Optimizer\n",
        "\n",
        "We use the *Adam* optimizer, a popular choice for training neural networks. The [Adam optimization algorithm](https://doi.org/10.48550/arXiv.1412.6980) is a neural network-specific adaptation of the [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY3DPOlKAIVQ"
      },
      "outputs": [],
      "source": [
        "optimizer = optimizers.Adam(learning_rate=1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM5JKUmNAIVQ"
      },
      "source": [
        "##### Metrics\n",
        "\n",
        "Metrics are functions needed to measure the behavior of our model. There are many to choose from depending on the task of the model. For our case:\n",
        "\n",
        "- **Accuracy**: This metric measures the overall classification accuracy of the model. It is calculated as the ratio of correct predictions to the total number of samples.\n",
        "\n",
        "- **Binary Accuracy**: It's a specific metric for binary classification tasks. We use a threshold of 0.5 to determine binary predictions. Binary accuracy is computed as:\n",
        "\n",
        "$ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XIcDX88AIVR"
      },
      "outputs": [],
      "source": [
        "metrics = [\n",
        "    tf.keras.metrics.Accuracy(),\n",
        "    tf.keras.metrics.BinaryAccuracy(threshold=0.5),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RGVjl--AIVR"
      },
      "source": [
        "\n",
        "##### Compilation\n",
        "\n",
        "Finally, we compile the model by specifying the optimizer, loss function, and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHXgUnqbAIVR"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer,\n",
        "    loss,\n",
        "    metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBArdA58HErL"
      },
      "source": [
        "#### Training\n",
        "In training, we define two key parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULdIIY_fHKBx"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhj7X3CiAIVS"
      },
      "source": [
        "* **Batch Size**: It specifies the number of training examples used in each iteration. A smaller batch size updates the model more frequently, while a larger one may improve training efficiency but requiring more volatile memory (RAM).\n",
        "\n",
        "* **Epochs**: Each epoch represents one pass through the entire training dataset. It controls how many times the model iterates over the data, influencing convergence and potential overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uF9yyCmAIVS"
      },
      "source": [
        "Let's (finally) start the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uz4jwPtHGB7",
        "outputId": "63ee7c4e-ad31-4858-9157-bca92762c6c5"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    np.array(features_train),  # before to feed the data we convert it into an array\n",
        "    np.array(labels_train).astype(\"float\"),\n",
        "    batch_size,\n",
        "    epochs,\n",
        "    validation_data=(\n",
        "        np.array(features_test),\n",
        "        np.array(labels_test).astype(\"float\"),\n",
        "    ),  # test-set\n",
        "    verbose=1,  # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-loZjQNrOjLN"
      },
      "source": [
        "#### Plot Training Progress\n",
        "\n",
        "Now, we can plot the training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVUl8PLKMdac"
      },
      "outputs": [],
      "source": [
        "# Plot traing loop\n",
        "train_binary_accuracy = np.array(history.history[\"binary_accuracy\"])\n",
        "test_binary_accuracy = np.array(history.history[\"val_binary_accuracy\"])\n",
        "train_loss = np.array(history.history[\"loss\"])\n",
        "test_loss = np.array(history.history[\"val_loss\"])\n",
        "\n",
        "\n",
        "epochs_i = np.arange(1, train_loss.shape[0] + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "oJ_iSIsjQh38",
        "outputId": "2252c38c-d6cc-403d-e280-68a2bdd1f7f5"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 3.3))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax_twin = ax.twinx()\n",
        "    a1 = ax.plot(\n",
        "        epochs_i,\n",
        "        train_binary_accuracy * 100,\n",
        "        color=\"k\",\n",
        "        ls=\"-\",\n",
        "        label=\"Binary accuracy (train)\",\n",
        "    )\n",
        "    a2 = ax.plot(\n",
        "        epochs_i,\n",
        "        test_binary_accuracy * 100,\n",
        "        color=\"k\",\n",
        "        ls=\"--\",\n",
        "        label=\"Binary accuracy (test)\",\n",
        "    )\n",
        "    l1 = ax_twin.plot(epochs_i, train_loss, color=\"r\", ls=\"-\", label=\"Loss (train)\")\n",
        "    l2 = ax_twin.plot(epochs_i, test_loss, color=\"r\", ls=\"--\", label=\"Loss (test)\")\n",
        "    ax.set_xlabel(\"Epochs [-]\")\n",
        "    ax.set_ylabel(\"Binary accuracy [%]\")\n",
        "    ax_twin.set_ylabel(\"Loss [-]\")\n",
        "    ax_twin.legend(\n",
        "        a1 + a2 + l1 + l2,\n",
        "        [\n",
        "            \"Binary accuracy (train)\",\n",
        "            \"Binary accuracy (test)\",\n",
        "            \"Loss (train)\",\n",
        "            \"Loss (test)\",\n",
        "        ],\n",
        "        loc=\"center right\",\n",
        "    )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPJ4nXZtAIVT"
      },
      "source": [
        "#### Model result analysis\n",
        "\n",
        "Let's compare the model's predictions with the Reynolds number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z01ZChgQpV8",
        "outputId": "7d6420d7-b6bf-4059-9e1f-9285f09d797e"
      },
      "outputs": [],
      "source": [
        "# Predictions are generated using the model over the test set,\n",
        "# which the model has never seen during training.\n",
        "predictions = model.predict(np.array(features_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "FaNAI-wLQ9Um",
        "outputId": "bc68c5d9-cc29-4376-9cef-7304212d574e"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(7, 3.3))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    axins = ax.inset_axes([0.3, 0.3, 0.65, 0.54])\n",
        "    ax.scatter(labels_Re_test, predictions, s=7, alpha=0.4)\n",
        "    axins.scatter(labels_Re_test, predictions, s=7, alpha=0.4)\n",
        "    axins.set_xlim([0, 12000])\n",
        "    ax.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
        "    ax.axvline(2e3, lw=1, color=\"k\", ls=\":\")\n",
        "    ax.axvline(1e4, lw=1, color=\"k\", ls=\":\")\n",
        "    axins.axvline(2e3, lw=1, color=\"k\", ls=\":\")\n",
        "    axins.axvline(1e4, lw=1, color=\"k\", ls=\":\")\n",
        "    ax.set_xlabel(\"Re [-]\")\n",
        "    ax.set_ylabel(\"Turbulence probability [-]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyECwdpVtE2j"
      },
      "source": [
        "In the following code cell, we define functions for deeper analysis, which we can use also later in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e--qgHt3tJcD"
      },
      "outputs": [],
      "source": [
        "def compute_confusion_matrix(labels, predicted):\n",
        "    \"\"\"\n",
        "    Compute the confusion matrix and return its elements along with the indices of TP, TN, FP, and FN.\n",
        "\n",
        "    Args:\n",
        "        labels (array-like): The actual binary classification labels (0 or 1).\n",
        "        predicted (array-like): The predicted binary classification labels (0 or 1).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The 2x2 confusion matrix.\n",
        "        list: Indices of True Positives (TP)\n",
        "        list: Indices of True Negatives (TN)\n",
        "        list: Indices of False Positives (FP)\n",
        "        list: Indices of False Negatives (FN)\n",
        "    \"\"\"\n",
        "    if len(labels) != len(predicted):\n",
        "        raise ValueError(\"Input arrays must have the same length.\")\n",
        "\n",
        "    confusion_matrix = np.zeros((2, 2), dtype=int)\n",
        "    TP_indices, TN_indices, FP_indices, FN_indices = [], [], [], []\n",
        "\n",
        "    for i, (l, p) in enumerate(zip(labels, predicted)):\n",
        "        confusion_matrix[l][p] += 1\n",
        "        if l == 1 and p == 1:\n",
        "            TP_indices.append(i)\n",
        "        elif l == 0 and p == 0:\n",
        "            TN_indices.append(i)\n",
        "        elif l == 0 and p == 1:\n",
        "            FP_indices.append(i)\n",
        "        elif l == 1 and p == 0:\n",
        "            FN_indices.append(i)\n",
        "\n",
        "    return confusion_matrix, TP_indices, TN_indices, FP_indices, FN_indices\n",
        "\n",
        "\n",
        "def plot_conf_matrix(ax, confusion_matrix, class_names, cmap, title=\"Confusion Matrix\"):\n",
        "    \"\"\"\n",
        "    Plot the confusion matrix with percentages.\n",
        "\n",
        "    Args:\n",
        "        ax: Matplotlib axes to plot the matrix.\n",
        "        confusion_matrix (numpy.ndarray): The confusion matrix.\n",
        "        class_names: Class names for labeling.\n",
        "        cmap: Colormap for the plot.\n",
        "        title (str): Title for the plot.\n",
        "    \"\"\"\n",
        "    conf_matrix_perc = (confusion_matrix.T / confusion_matrix.sum(axis=1)).T * 100.0\n",
        "    cm = ax.imshow(\n",
        "        conf_matrix_perc, interpolation=\"nearest\", cmap=cmap, vmax=100.0, vmin=0.0\n",
        "    )\n",
        "    ax.set_title(title)\n",
        "    plt.colorbar(cm)\n",
        "\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    ax.set_xticks(tick_marks, class_names)\n",
        "    ax.set_yticks(tick_marks, class_names)\n",
        "    ax.set_yticklabels(class_names, rotation=90, ha=\"right\", va=\"center\")\n",
        "    for i in range(len(class_names)):\n",
        "        for j in range(len(class_names)):\n",
        "            ax.text(\n",
        "                j,\n",
        "                i,\n",
        "                f\"{conf_matrix_perc[i, j]:3.2f} %\",\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"k\",\n",
        "            )\n",
        "\n",
        "    ax.set_ylabel(\"True labels\")\n",
        "    ax.set_xlabel(\"Predicted labels\")\n",
        "\n",
        "\n",
        "def plot_velocity_conf_matrix(axs, cm_indices, velocity, class_names, title):\n",
        "    \"\"\"\n",
        "    Plot velocity profiles for TP, TN, FP, and FN.\n",
        "\n",
        "    Args:\n",
        "        axs: Matplotlib axes for subplots.\n",
        "        cm_indices: Confusion matrix indices.\n",
        "        velocity: Array of velocity profiles.\n",
        "        class_names: Class names for labeling.\n",
        "        title (str): Title for the plot.\n",
        "    \"\"\"\n",
        "    velocity = np.asarray(velocity)\n",
        "    r = np.linspace(0, 1, velocity.shape[1])\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            for k in cm_indices[i][j]:\n",
        "                axs[i][j].plot(r, velocity[k, :])\n",
        "            axs[i][j].set_yticks([np.mean(axs[i][j].get_ylim())], [class_names[i]])\n",
        "            axs[i][j].set_xticks([np.mean(axs[i][j].get_xlim())], [class_names[j]])\n",
        "            axs[i][j].set_yticklabels(\n",
        "                [class_names[i]], rotation=90, ha=\"right\", va=\"center\"\n",
        "            )\n",
        "    ax.set_ylabel(\"True labels\")\n",
        "    axs[0][0].set_ylabel(\"True labels\")\n",
        "    axs[1][1].set_xlabel(\"Predicted labels\")\n",
        "    axs[0][1].set_xticklabels([])\n",
        "    axs[0][1].set_yticklabels([])\n",
        "    axs[1][1].set_yticklabels([])\n",
        "    axs[0][0].yaxis.set_label_coords(-0.22, 0.0, transform=axs[0][0].transAxes)\n",
        "    axs[1][1].xaxis.set_label_coords(0.0, -0.22, transform=axs[1][1].transAxes)\n",
        "    axs[0][0].set_title(title, loc=\"right\", fontdict={\"ha\": \"center\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0T4BsAGAIVU"
      },
      "source": [
        "Now, let's compute the confusion matrix and plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "QPYY9jNYtSu2",
        "outputId": "85a6920b-829f-4164-dd75-5a175711c8ee"
      },
      "outputs": [],
      "source": [
        "conf_matrix, i_TP, i_TN, i_FP, i_FN = compute_confusion_matrix(\n",
        "    np.squeeze(np.array(labels_test).astype(int)),\n",
        "    np.squeeze(predictions > 0.5).astype(int),\n",
        ")\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3.6))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    grid = fig.add_gridspec(2, 8, wspace=0.0, hspace=0.0)\n",
        "    ax_cm = fig.add_subplot(grid[:, :2])\n",
        "    plot_conf_matrix(ax_cm, conf_matrix, [\"Non-turbulent\", \"Turbulent\"], \"summer\")\n",
        "    ax_cm_p = []\n",
        "    for i in range(2):\n",
        "        ax_ = []\n",
        "        for j in range(2):\n",
        "            ax_.append(fig.add_subplot(grid[i, j + 3]))\n",
        "        ax_cm_p.append(ax_)\n",
        "\n",
        "    ax_cm_pn = []\n",
        "    for i in range(2):\n",
        "        ax_ = []\n",
        "        for j in range(2):\n",
        "            ax_.append(fig.add_subplot(grid[i, j + 6]))\n",
        "        ax_cm_pn.append(ax_)\n",
        "\n",
        "    cm_i = [[i_TN, i_FP], [i_FN, i_TP]]\n",
        "    plot_velocity_conf_matrix(\n",
        "        ax_cm_p,\n",
        "        cm_i,\n",
        "        features_test,\n",
        "        [\"Non-turbulent\", \"Turbulent\"],\n",
        "        \"Veloecity profiles\",\n",
        "    )\n",
        "    plot_velocity_conf_matrix(\n",
        "        ax_cm_pn,\n",
        "        cm_i,\n",
        "        (features_test.T / features_test.max(axis=1).values).T,\n",
        "        [\"Non-turbulent\", \"Turbulent\"],\n",
        "        \"Veloecity profiles normalized\",\n",
        "    )\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6Ucd0vhAwO8"
      },
      "source": [
        "### Second Try: Increasing Model Depth\n",
        "\n",
        "In this section, we aim to enhance the model's complexity by adding an additional hidden layer.\n",
        "We will evaluate if this increased complexity leads to significant performance improvements and whether we remain far from overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpALJK4KBVnK"
      },
      "outputs": [],
      "source": [
        "def build_model_1_deeper(n_cols):\n",
        "    model = Sequential(\n",
        "        [\n",
        "            layers.InputLayer(input_shape=(n_cols,), name=\"in\"),\n",
        "            layers.Dense(64, activation=\"relu\", name=\"h1\"),\n",
        "            layers.Dense(64, activation=\"relu\", name=\"h2\"),\n",
        "            layers.Dense(1, activation=\"sigmoid\", name=\"out\"),\n",
        "        ]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kUupBdmBVnK"
      },
      "source": [
        "#### Build, compile and training\n",
        "\n",
        "As before let's call the function and build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RYS51i8BVnL"
      },
      "outputs": [],
      "source": [
        "model_v1 = build_model_1_deeper(features.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTbyZv6YBVnL"
      },
      "source": [
        "In this case, the degree of freedom of our model increases significantly as we add more neurons, resulting in a dramatic increase in the number of connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG1HAYFhBVnL",
        "outputId": "6b82c56f-36bd-41a9-d349-6681e0e5bf0c"
      },
      "outputs": [],
      "source": [
        "model_v1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "mFZLChbYBVnM",
        "outputId": "b1cea67e-fb04-4d64-8d76-702c3f0713ed"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model=model_v1, rankdir=\"LR\", dpi=72, show_shapes=True, show_layer_activations=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btea2sQjB16Z"
      },
      "source": [
        "We compile the model as before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f9MAeOzBVnM"
      },
      "outputs": [],
      "source": [
        "optimizer = optimizers.Adam(learning_rate=1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "loss = losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = [\n",
        "    tf.keras.metrics.Accuracy(),\n",
        "    tf.keras.metrics.BinaryAccuracy(threshold=0.5),\n",
        "]\n",
        "\n",
        "model_v1.compile(\n",
        "    optimizer,\n",
        "    loss,\n",
        "    metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrfKl3_yB4Q5"
      },
      "source": [
        "And then we train it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_I4thlFBVnN"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZogMonCUBVnN",
        "outputId": "9a87d59f-3881-4020-df91-1f43f1dd3041"
      },
      "outputs": [],
      "source": [
        "history_v1 = model_v1.fit(\n",
        "    np.array(features_train),  # before to feed the data we convert it into an array\n",
        "    np.array(labels_train).astype(\"float\"),\n",
        "    batch_size,\n",
        "    epochs,\n",
        "    validation_data=(\n",
        "        np.array(features_test),\n",
        "        np.array(labels_test).astype(\"float\"),\n",
        "    ),  # test-set\n",
        "    verbose=1,  # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql_yna9fBVnN"
      },
      "source": [
        "#### Analysis\n",
        "\n",
        "Training result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I5_yNl6BVnO"
      },
      "outputs": [],
      "source": [
        "# Plot traing loop\n",
        "train_binary_accuracy_v1 = np.array(history_v1.history[\"binary_accuracy\"])\n",
        "test_binary_accuracy_v1 = np.array(history_v1.history[\"val_binary_accuracy\"])\n",
        "train_loss_v1 = np.array(history_v1.history[\"loss\"])\n",
        "test_loss_v1 = np.array(history_v1.history[\"val_loss\"])\n",
        "\n",
        "\n",
        "epochs_i = np.arange(1, train_loss_v1.shape[0] + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "lYTd7NOdBVnO",
        "outputId": "0f4594ad-7a3c-44fe-d0cf-735fe28305b2"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 3.3))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax_twin = ax.twinx()\n",
        "    a1 = ax.plot(\n",
        "        epochs_i,\n",
        "        train_binary_accuracy_v1 * 100,\n",
        "        color=\"k\",\n",
        "        ls=\"-\",\n",
        "        label=\"Binary accuracy (train)\",\n",
        "    )\n",
        "    a2 = ax.plot(\n",
        "        epochs_i,\n",
        "        test_binary_accuracy_v1 * 100,\n",
        "        color=\"k\",\n",
        "        ls=\"--\",\n",
        "        label=\"Binary accuracy (test)\",\n",
        "    )\n",
        "    l1 = ax_twin.plot(epochs_i, train_loss_v1, color=\"r\", ls=\"-\", label=\"Loss (train)\")\n",
        "    l2 = ax_twin.plot(epochs_i, test_loss_v1, color=\"r\", ls=\"--\", label=\"Loss (test)\")\n",
        "    ax.set_xlabel(\"Epochs [-]\")\n",
        "    ax.set_ylabel(\"Binary accuracy [%]\")\n",
        "    ax_twin.set_ylabel(\"Loss [-]\")\n",
        "    ax_twin.legend(\n",
        "        a1 + a2 + l1 + l2,\n",
        "        [\n",
        "            \"Binary accuracy (train)\",\n",
        "            \"Binary accuracy (test)\",\n",
        "            \"Loss (train)\",\n",
        "            \"Loss (test)\",\n",
        "        ],\n",
        "        loc=\"center right\",\n",
        "    )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGIlWwOmBVnO"
      },
      "source": [
        "Prediction vs Reynolds number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4l08P12BVnP",
        "outputId": "9ecb79be-e11b-4bca-a2f8-9aad0bb0501d"
      },
      "outputs": [],
      "source": [
        "predictions_v1 = model_v1.predict(np.array(features_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Ab5tJpVUBVnP",
        "outputId": "6b75877c-dbf5-4c08-ffbf-4d284bfe6e39"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(7, 3.3))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    axins = ax.inset_axes([0.3, 0.3, 0.65, 0.54])\n",
        "    ax.scatter(labels_Re_test, predictions_v1, s=7, alpha=0.4)\n",
        "    axins.scatter(labels_Re_test, predictions_v1, s=7, alpha=0.4)\n",
        "    axins.set_xlim([0, 12000])\n",
        "    ax.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
        "    ax.axvline(2e3, lw=1, color=\"k\", ls=\":\")\n",
        "    ax.axvline(1e4, lw=1, color=\"k\", ls=\":\")\n",
        "    axins.axvline(2e3, lw=1, color=\"k\", ls=\":\")\n",
        "    axins.axvline(1e4, lw=1, color=\"k\", ls=\":\")\n",
        "    ax.set_xlabel(\"Re [-]\")\n",
        "    ax.set_ylabel(\"Turbulence probability [-]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "66MyZcfRELYu",
        "outputId": "2567c06c-27d5-4cae-91d3-55dffd428d77"
      },
      "outputs": [],
      "source": [
        "conf_matrix, i_TP, i_TN, i_FP, i_FN = compute_confusion_matrix(\n",
        "    np.squeeze(np.array(labels_test).astype(int)),\n",
        "    np.squeeze(predictions_v1 > 0.5).astype(int),\n",
        ")\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3.6))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    grid = fig.add_gridspec(2, 8, wspace=0.0, hspace=0.0)\n",
        "    ax_cm = fig.add_subplot(grid[:, :2])\n",
        "    plot_conf_matrix(ax_cm, conf_matrix, [\"Non-turbulent\", \"Turbulent\"], \"summer\")\n",
        "    ax_cm_p = []\n",
        "    for i in range(2):\n",
        "        ax_ = []\n",
        "        for j in range(2):\n",
        "            ax_.append(fig.add_subplot(grid[i, j + 3]))\n",
        "        ax_cm_p.append(ax_)\n",
        "\n",
        "    ax_cm_pn = []\n",
        "    for i in range(2):\n",
        "        ax_ = []\n",
        "        for j in range(2):\n",
        "            ax_.append(fig.add_subplot(grid[i, j + 6]))\n",
        "        ax_cm_pn.append(ax_)\n",
        "\n",
        "    cm_i = [[i_TN, i_FP], [i_FN, i_TP]]\n",
        "    plot_velocity_conf_matrix(\n",
        "        ax_cm_p,\n",
        "        cm_i,\n",
        "        features_test,\n",
        "        [\"Non-turbulent\", \"Turbulent\"],\n",
        "        \"Veloecity profiles\",\n",
        "    )\n",
        "    plot_velocity_conf_matrix(\n",
        "        ax_cm_pn,\n",
        "        cm_i,\n",
        "        (features_test.T / features_test.max(axis=1).values).T,\n",
        "        [\"Non-turbulent\", \"Turbulent\"],\n",
        "        \"Veloecity profiles normalized\",\n",
        "    )\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXYl3AnwxQ7-"
      },
      "source": [
        "### Third Try: Expanding the Database\n",
        "\n",
        "In this section, we seek to enhance the performance of the initial model (with only one hidden layer) by training it on a larger database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZGzLLrZxSQv"
      },
      "source": [
        "#### Medium size database\n",
        "\n",
        "Let's build a medium size database with 40'000 points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTATP3YFxSQv"
      },
      "outputs": [],
      "source": [
        "Nall = data_v.shape[0]\n",
        "Nsmall = 40000\n",
        "\n",
        "rand_gen = np.random.default_rng(seed=1234)\n",
        "indx = rand_gen.choice(np.arange(Nall), size=Nsmall, replace=False)\n",
        "\n",
        "data_v_medium = data_v.iloc[indx, :]\n",
        "data_Re_medium = data_Re.iloc[indx, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7OwM6J_xSQw"
      },
      "source": [
        "let's store it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJHwDgahxSQw"
      },
      "outputs": [],
      "source": [
        "data_v_medium.to_csv(\"medium-data.csv\", index=False)\n",
        "data_Re_medium.to_csv(\"medium-Re.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J47g3k7yDPz"
      },
      "source": [
        "as before we reload the data for the *features* and the *labels* of our model, and split it in training and test set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rdCQHJuyDP1"
      },
      "outputs": [],
      "source": [
        "features_v2 = pd.read_csv(\"medium-data.csv\", index_col=False).iloc[\n",
        "    :, 4:\n",
        "]  # note: we drop the first 4 colomns to study only the velocity profile\n",
        "labels_v2 = pd.read_csv(\"medium-Re.csv\", index_col=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHVmnYYgyDP3"
      },
      "outputs": [],
      "source": [
        "labels_Re_v2 = labels_v2.pop(\"Re(-)\")\n",
        "labels_v2[\"Turbolent\"] = labels_Re_v2 >= 1e4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dff7f8ISyDP4"
      },
      "outputs": [],
      "source": [
        "labels_train_v2 = labels_v2.iloc[8000:, :]\n",
        "labels_test_v2 = labels_v2.iloc[:8000, :]\n",
        "features_train_v2 = features_v2.iloc[8000:, :]\n",
        "features_test_v2 = features_v2.iloc[:8000, :]\n",
        "# the Re number will be useful later\n",
        "labels_Re_train_v2 = labels_Re_v2.iloc[8000:]\n",
        "labels_Re_test_v2 = labels_Re_v2.iloc[:8000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_-hIJtCyDP5"
      },
      "source": [
        "Storing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pJc_lT7yDP5"
      },
      "outputs": [],
      "source": [
        "pathlib.Path(\"model_1\").mkdir(parents=True, exist_ok=True)  # make a folder\n",
        "# make train and test subfolder\n",
        "pathlib.Path(os.path.join(\"model_1\", \"train\")).mkdir(exist_ok=True)\n",
        "pathlib.Path(os.path.join(\"model_1\", \"test\")).mkdir(exist_ok=True)\n",
        "# storin\n",
        "labels_train.to_csv(os.path.join(\"model_1\", \"train\", \"labels_v2.csv\"), index=False)\n",
        "labels_test.to_csv(os.path.join(\"model_1\", \"test\", \"labels_v2.csv\"), index=False)\n",
        "features_train.to_csv(os.path.join(\"model_1\", \"train\", \"features_v2.csv\"), index=False)\n",
        "features_test.to_csv(os.path.join(\"model_1\", \"test\", \"features_v2.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apOP-lRhyDP5"
      },
      "source": [
        "#### Build, compile and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuHbt__VyDP7"
      },
      "outputs": [],
      "source": [
        "model_v2 = build_model_1(features_v2.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svspvTOMyDP8",
        "outputId": "80bae05c-98bd-4803-b401-9fd51a8c528a"
      },
      "outputs": [],
      "source": [
        "model_v2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "ksEaiYqryDP8",
        "outputId": "6033fcbe-7d88-4bcc-b721-c12e38476aa1"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model=model_v2, rankdir=\"LR\", dpi=72, show_shapes=True, show_layer_activations=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7bjv0SOyDP9"
      },
      "outputs": [],
      "source": [
        "optimizer = optimizers.Adam(learning_rate=1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "loss = losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = [\n",
        "    tf.keras.metrics.Accuracy(),\n",
        "    tf.keras.metrics.BinaryAccuracy(threshold=0.5),\n",
        "]\n",
        "\n",
        "model_v2.compile(\n",
        "    optimizer,\n",
        "    loss,\n",
        "    metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma14dwV8yDP9"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vmhWQwWyDP9"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrN02POnyDP-",
        "outputId": "274f903a-a0d9-4b3a-98b1-b247e31a409e"
      },
      "outputs": [],
      "source": [
        "history_v2 = model_v2.fit(\n",
        "    np.array(features_train_v2),  # before to feed the data we convert it into an array\n",
        "    np.array(labels_train_v2).astype(\"float\"),\n",
        "    batch_size,\n",
        "    epochs,\n",
        "    validation_data=(\n",
        "        np.array(features_test_v2),\n",
        "        np.array(labels_test_v2).astype(\"float\"),\n",
        "    ),  # test-set\n",
        "    verbose=1,  # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4b8JJBkyDP-"
      },
      "source": [
        "#### Analysis\n",
        "\n",
        "Now we perfome the usual analisys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0gR-RrbyDP-"
      },
      "outputs": [],
      "source": [
        "# Plot traing loop\n",
        "train_binary_accuracy_v2 = np.array(history_v2.history[\"binary_accuracy\"])\n",
        "test_binary_accuracy_v2 = np.array(history_v2.history[\"val_binary_accuracy\"])\n",
        "train_loss_v2 = np.array(history_v2.history[\"loss\"])\n",
        "test_loss_v2 = np.array(history_v2.history[\"val_loss\"])\n",
        "\n",
        "\n",
        "epochs_i = np.arange(1, train_loss_v2.shape[0] + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "JuvMevPpyDP_",
        "outputId": "58e483a7-7980-423a-d408-568240f4a451"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 3.3))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax_twin = ax.twinx()\n",
        "    a1 = ax.plot(\n",
        "        epochs_i,\n",
        "        train_binary_accuracy_v2 * 100,\n",
        "        color=\"k\",\n",
        "        ls=\"-\",\n",
        "        label=\"Binary accuracy (train)\",\n",
        "    )\n",
        "    a2 = ax.plot(\n",
        "        epochs_i,\n",
        "        test_binary_accuracy_v2 * 100,\n",
        "        color=\"k\",\n",
        "        ls=\"--\",\n",
        "        label=\"Binary accuracy (test)\",\n",
        "    )\n",
        "    l1 = ax_twin.plot(epochs_i, train_loss_v2, color=\"r\", ls=\"-\", label=\"Loss (train)\")\n",
        "    l2 = ax_twin.plot(epochs_i, test_loss_v2, color=\"r\", ls=\"--\", label=\"Loss (test)\")\n",
        "    ax.set_xlabel(\"Epochs [-]\")\n",
        "    ax.set_ylabel(\"Binary accuracy [%]\")\n",
        "    ax_twin.set_ylabel(\"Loss [-]\")\n",
        "    ax_twin.legend(\n",
        "        a1 + a2 + l1 + l2,\n",
        "        [\n",
        "            \"Binary accuracy (train)\",\n",
        "            \"Binary accuracy (test)\",\n",
        "            \"Loss (train)\",\n",
        "            \"Loss (test)\",\n",
        "        ],\n",
        "        loc=\"center right\",\n",
        "    )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qInfEGvvyDP_",
        "outputId": "31cf9100-c085-41c1-d4ac-e348a2930991"
      },
      "outputs": [],
      "source": [
        "predictions_v2 = model.predict(np.array(features_test_v2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "4yP62wTmyDQA",
        "outputId": "eb20a21e-1611-4a3d-875b-a92243c04aa8"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(7, 3.3))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    axins = ax.inset_axes([0.3, 0.3, 0.65, 0.54])\n",
        "    ax.scatter(labels_Re_test_v2, predictions_v2, s=7, alpha=0.4)\n",
        "    axins.scatter(labels_Re_test_v2, predictions_v2, s=7, alpha=0.4)\n",
        "    axins.set_xlim([0, 12000])\n",
        "    ax.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
        "    ax.axvline(2e3, lw=1, color=\"k\", ls=\":\")\n",
        "    ax.axvline(1e4, lw=1, color=\"k\", ls=\":\")\n",
        "    axins.axvline(2e3, lw=1, color=\"k\", ls=\":\")\n",
        "    axins.axvline(1e4, lw=1, color=\"k\", ls=\":\")\n",
        "    ax.set_xlabel(\"Re [-]\")\n",
        "    ax.set_ylabel(\"Turbulence probability [-]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89gYRIkuyDQA"
      },
      "source": [
        "Futher analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "B6cqorIIyDQB",
        "outputId": "b0c89265-50be-4bc3-855b-22da59c9ce80"
      },
      "outputs": [],
      "source": [
        "conf_matrix, i_TP, i_TN, i_FP, i_FN = compute_confusion_matrix(\n",
        "    np.squeeze(np.array(labels_test_v2).astype(int)),\n",
        "    np.squeeze(predictions_v2 > 0.5).astype(int),\n",
        ")\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3.6))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    grid = fig.add_gridspec(2, 8, wspace=0.0, hspace=0.0)\n",
        "    ax_cm = fig.add_subplot(grid[:, :2])\n",
        "    plot_conf_matrix(\n",
        "        ax_cm,\n",
        "        conf_matrix,\n",
        "        [\"Non-turbulent\", \"Turbulent\"],\n",
        "        \"summer\",\n",
        "        title=\"CM Model v0\",\n",
        "    )\n",
        "    ax_cm_p = []\n",
        "    for i in range(2):\n",
        "        ax_ = []\n",
        "        for j in range(2):\n",
        "            ax_.append(fig.add_subplot(grid[i, j + 3]))\n",
        "        ax_cm_p.append(ax_)\n",
        "\n",
        "    ax_cm_pn = []\n",
        "    for i in range(2):\n",
        "        ax_ = []\n",
        "        for j in range(2):\n",
        "            ax_.append(fig.add_subplot(grid[i, j + 6]))\n",
        "        ax_cm_pn.append(ax_)\n",
        "\n",
        "    cm_i = [[i_TN, i_FP], [i_FN, i_TP]]\n",
        "    plot_velocity_conf_matrix(\n",
        "        ax_cm_p,\n",
        "        cm_i,\n",
        "        features_test_v2,\n",
        "        [\"Non-turbulent\", \"Turbulent\"],\n",
        "        \"Veloecity profiles\",\n",
        "    )\n",
        "    plot_velocity_conf_matrix(\n",
        "        ax_cm_pn,\n",
        "        cm_i,\n",
        "        (features_test_v2.T / features_test_v2.max(axis=1).values).T,\n",
        "        [\"Non-turbulent\", \"Turbulent\"],\n",
        "        \"Veloecity profiles normalized\",\n",
        "    )\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC0HH4gO1Eg9"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Let's compare the 3 setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "k3FOQ2jW1HkI",
        "outputId": "33be0e2e-9a61-4dd1-c7ab-9120df34cfae"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 3.3))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax_twin = ax.twinx()\n",
        "    ax_twin.plot(np.nan, np.nan, color=\"k\", ls=\":\", label=\"Binary accuracy (train)\")\n",
        "    ax_twin.plot(np.nan, np.nan, color=\"k\", ls=\"-\", label=\"Binary accuracy (test)\")\n",
        "    ax_twin.plot(np.nan, np.nan, color=\"r\", ls=\":\", label=\"Loss (train)\")\n",
        "    ax_twin.plot(np.nan, np.nan, color=\"r\", ls=\"-\", label=\"Loss (test)\")\n",
        "    ax.plot(\n",
        "        epochs_i,\n",
        "        train_binary_accuracy * 100,\n",
        "        color=\"k\",\n",
        "        ls=\":\",\n",
        "        marker=\"o\",\n",
        "        markevery=5,\n",
        "        mfc=\"none\",\n",
        "        mew=1,\n",
        "    )\n",
        "    ax.plot(\n",
        "        epochs_i,\n",
        "        train_binary_accuracy_v1 * 100,\n",
        "        color=\"k\",\n",
        "        ls=\":\",\n",
        "        marker=\"s\",\n",
        "        markevery=5,\n",
        "        mfc=\"none\",\n",
        "        mew=1,\n",
        "    )\n",
        "    ax.plot(\n",
        "        epochs_i,\n",
        "        train_binary_accuracy_v2 * 100,\n",
        "        color=\"k\",\n",
        "        ls=\":\",\n",
        "        marker=\"^\",\n",
        "        markevery=5,\n",
        "        mfc=\"none\",\n",
        "        mew=1,\n",
        "    )\n",
        "    ax.plot(\n",
        "        epochs_i,\n",
        "        test_binary_accuracy * 100,\n",
        "        color=\"k\",\n",
        "        ls=\"-\",\n",
        "        marker=\"o\",\n",
        "        markevery=5,\n",
        "        mfc=\"none\",\n",
        "        mew=1,\n",
        "        label=\"Model v0\",\n",
        "    )\n",
        "    ax.plot(\n",
        "        epochs_i,\n",
        "        test_binary_accuracy_v1 * 100,\n",
        "        color=\"k\",\n",
        "        ls=\"-\",\n",
        "        marker=\"s\",\n",
        "        markevery=5,\n",
        "        mfc=\"none\",\n",
        "        mew=1,\n",
        "        label=\"Model v1\",\n",
        "    )\n",
        "    ax.plot(\n",
        "        epochs_i,\n",
        "        test_binary_accuracy_v2 * 100,\n",
        "        color=\"k\",\n",
        "        ls=\"-\",\n",
        "        marker=\"^\",\n",
        "        markevery=5,\n",
        "        mfc=\"none\",\n",
        "        mew=1,\n",
        "        label=\"Model v2\",\n",
        "    )\n",
        "\n",
        "    ax_twin.plot(\n",
        "        epochs_i,\n",
        "        train_loss,\n",
        "        color=\"r\",\n",
        "        ls=\":\",\n",
        "        marker=\"o\",\n",
        "        markevery=10,\n",
        "        mfc=\"none\",\n",
        "        mew=1,\n",
        "    )\n",
        "    ax_twin.plot(\n",
        "        epochs_i,\n",
        "        train_loss_v1,\n",
        "        color=\"r\",\n",
        "        ls=\":\",\n",
        "        marker=\"s\",\n",
        "        markevery=10,\n",
        "        mfc=\"none\",\n",
        "        mew=1,\n",
        "    )\n",
        "    ax_twin.plot(\n",
        "        epochs_i,\n",
        "        train_loss_v2,\n",
        "        color=\"r\",\n",
        "        ls=\":\",\n",
        "        marker=\"^\",\n",
        "        markevery=10,\n",
        "        mfc=\"none\",\n",
        "        mew=1,\n",
        "    )\n",
        "    ax_twin.plot(\n",
        "        epochs_i,\n",
        "        test_loss,\n",
        "        color=\"r\",\n",
        "        ls=\"-\",\n",
        "        marker=\"o\",\n",
        "        markevery=10,\n",
        "        mfc=\"none\",\n",
        "        mew=1,\n",
        "    )\n",
        "    ax_twin.plot(\n",
        "        epochs_i,\n",
        "        test_loss_v1,\n",
        "        color=\"r\",\n",
        "        ls=\"-\",\n",
        "        marker=\"s\",\n",
        "        markevery=10,\n",
        "        mfc=\"none\",\n",
        "        mew=1,\n",
        "    )\n",
        "    ax_twin.plot(\n",
        "        epochs_i,\n",
        "        test_loss_v2,\n",
        "        color=\"r\",\n",
        "        ls=\"-\",\n",
        "        marker=\"^\",\n",
        "        markevery=10,\n",
        "        mfc=\"none\",\n",
        "        mew=1,\n",
        "    )\n",
        "    ax.set_xlabel(\"Epochs [-]\")\n",
        "    ax.set_ylabel(\"Binary accuracy [%]\")\n",
        "    ax.set_ylim([95, 100])\n",
        "    ax_twin.set_ylabel(\"Loss [-]\")\n",
        "    ax_twin.legend(loc=\"upper left\", bbox_to_anchor=(1.2, 1.0))\n",
        "    ax.legend(loc=\"lower left\", bbox_to_anchor=(1.2, 0.0))\n",
        "    # ax_twin.legend(a1+a2+l1+l2, ['Binary accuracy (train)', 'Binary accuracy (test)', 'Loss (train)', 'Loss (test)'], loc='center right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "Vs8TLJRt5Llg",
        "outputId": "3ca3defc-dfab-42ad-e775-f15cce3b7415"
      },
      "outputs": [],
      "source": [
        "conf_matrix, _, _, _, _ = compute_confusion_matrix(\n",
        "    np.squeeze(np.array(labels_test).astype(int)),\n",
        "    np.squeeze(predictions > 0.5).astype(int),\n",
        ")\n",
        "conf_matrix_v1, _, _, _, _ = compute_confusion_matrix(\n",
        "    np.squeeze(np.array(labels_test).astype(int)),\n",
        "    np.squeeze(predictions_v1 > 0.5).astype(int),\n",
        ")\n",
        "conf_matrix_v2, _, _, _, _ = compute_confusion_matrix(\n",
        "    np.squeeze(np.array(labels_test_v2).astype(int)),\n",
        "    np.squeeze(predictions_v2 > 0.5).astype(int),\n",
        ")\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3.6))\n",
        "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
        "    grid = fig.add_gridspec(2, 8, wspace=0.0, hspace=0.0)\n",
        "    ax_cm_0 = fig.add_subplot(grid[:, :2])\n",
        "    ax_cm_1 = fig.add_subplot(grid[:, 3:5])\n",
        "    ax_cm_2 = fig.add_subplot(grid[:, 6:])\n",
        "\n",
        "    plot_conf_matrix(\n",
        "        ax_cm_0,\n",
        "        conf_matrix,\n",
        "        [\"Non-turbulent\", \"Turbulent\"],\n",
        "        \"summer\",\n",
        "        title=\"CM Model v0\",\n",
        "    )\n",
        "    plot_conf_matrix(\n",
        "        ax_cm_1,\n",
        "        conf_matrix_v1,\n",
        "        [\"Non-turbulent\", \"Turbulent\"],\n",
        "        \"summer\",\n",
        "        title=\"CM Model v1\",\n",
        "    )\n",
        "    plot_conf_matrix(\n",
        "        ax_cm_2,\n",
        "        conf_matrix_v2,\n",
        "        [\"Non-turbulent\", \"Turbulent\"],\n",
        "        \"summer\",\n",
        "        title=\"CM Model v2\",\n",
        "    )\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh-rusTnBkZW"
      },
      "source": [
        "What conclusion can we draw from this? Is it more beneficial to increase the complexity (depth) of a neural network model, or to expand the database?\n",
        "\n",
        "Hint: Consider the insights from the well-known article by Microsoft researchers M. Banko and E. Brill, \"[Scaling to Very Very Large Corpora for Natural Language Disambiguation](https://dl.acm.org/doi/pdf/10.3115/1073012.1073017),\" as well as the recent opinion letter supporting the same thesis by Google researchers A. Halevy, P. Norvig, and F. Pereira, \"[The Unreasonable Effectiveness of Data](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf).\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhSDCy-Zf3KR"
      },
      "source": [
        "## Exercise (Optional)\n",
        "\n",
        "* Experiment with training the deeper model using the medium-sized dataset. Is it the best model for this scenario?\n",
        "* Train all the models for 500 epochs. Do one or more models exhibit overfitting? If so, why does this occur?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
