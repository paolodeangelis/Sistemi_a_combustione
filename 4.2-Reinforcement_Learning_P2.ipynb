{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmIEfJaUTyNH"
   },
   "source": [
    "# Reinforcement Learning - an introduction (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5pGX7IdTyNN"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paolodeangelis/Sistemi_a_combustione/blob/main/4.2-Reinforcement_Learning_P2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MbMQP0b7zGg"
   },
   "source": [
    "# **1. Setup**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gg4Vl1FH_yVm"
   },
   "source": [
    "### **Install Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyYkz0Vc7xNE",
    "outputId": "140f9727-c272-4686-ce84-d6f6783ffb7d"
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!apt install swig cmake ffmpeg xvfb python3-opengl\n",
    "!pip install stable-baselines3==2.0.0a5 gymnasium[box2d] huggingface_sb3 pyvirtualdisplay imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v81BfAd7yWc"
   },
   "source": [
    "The Next Cell will force the notebook runtime to restart. This is to ensure all the new libraries installed will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaiyY-Fn_EZg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAlous3h_OrZ"
   },
   "source": [
    "### **Start Virtual Display**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xblUYFLM_ZGr",
    "outputId": "f3448bb5-2009-4e4a-c7ef-4a16f21cd8f2"
   },
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27yFBqm6nA4b"
   },
   "source": [
    "# Model 2: [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLtXEwmXnG6K"
   },
   "source": [
    "### Step1: Environment and Parameter Setup\n",
    "load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqAt57CvnW7X"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import NamedTuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLunf4RwnWII"
   },
   "source": [
    "In this section, we define the hyperparameters and configuration parameters using a `NamedTuple` called `Params`. These parameters include the total number of episodes, learning rate, discounting rate (gamma), exploration probability (epsilon), map size, seed for reproducibility, slippery environment flag, number of runs, action size, state size, probability of tiles being frozen, and the folder where plots will be saved.\n",
    "\n",
    "We also set the random number generator (`rng`) seed and create the FrozenLake environment using Gym. The environment's description is generated with random frozen tiles based on the specified probability (`proba_frozen`). The action and state sizes are determined based on the environment's properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBuLakXvnIyd"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters and configuration parameters using a NamedTuple\n",
    "\n",
    "\n",
    "class Params(NamedTuple):\n",
    "    total_episodes: int  # Total episodes\n",
    "    learning_rate: float  # Learning rate\n",
    "    gamma: float  # Discounting rate\n",
    "    epsilon: float  # Exploration probability\n",
    "    map_size: int  # Number of tiles on one side of the squared environment\n",
    "    seed: int  # Define a seed for reproducible results\n",
    "    is_slippery: bool  # Slippery environment flag\n",
    "    n_runs: int  # Number of runs\n",
    "    action_size: int  # Number of possible actions\n",
    "    state_size: int  # Number of possible states\n",
    "    proba_frozen: float  # Probability that a tile is frozen\n",
    "    savefig_folder: Path  # Root folder to save plots\n",
    "\n",
    "\n",
    "# Initialize hyperparameters and configuration parameters\n",
    "params = Params(\n",
    "    total_episodes=5000,\n",
    "    learning_rate=0.5,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.1,\n",
    "    map_size=6,\n",
    "    seed=123,\n",
    "    is_slippery=False,\n",
    "    n_runs=4,\n",
    "    action_size=None,\n",
    "    state_size=None,\n",
    "    proba_frozen=0.9,\n",
    "    savefig_folder=Path(\"../../_static/img/tutorials/\"),\n",
    ")\n",
    "\n",
    "# Set the seed\n",
    "rng = np.random.default_rng(params.seed)\n",
    "\n",
    "# Create the figure folder if it doesn't exist\n",
    "params.savefig_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create the FrozenLake environment with specified parameters\n",
    "env = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    is_slippery=params.is_slippery,\n",
    "    render_mode=\"rgb_array\",\n",
    "    desc=generate_random_map(\n",
    "        size=params.map_size, p=params.proba_frozen, seed=params.seed\n",
    "    ),\n",
    ")\n",
    "\n",
    "params = params._replace(action_size=env.action_space.n)\n",
    "params = params._replace(state_size=env.observation_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMBs9GPhnt9T"
   },
   "source": [
    "### Step 2: Q-Learning Agent Definition\n",
    "\n",
    "In this section, we define the Q-learning agent class (`Qlearning`). The agent is initialized with learning rate, discounting rate (gamma), state size, and action size as parameters. The Q-table is initialized with zeros.\n",
    "\n",
    "The `update` method implements the Q-learning update equation to update Q-values based on the difference between the current estimate and actual rewards. This equation is expressed as:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot \\left( R(s, a) + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ Q(s, a) $ is the current estimate of the Q-value for state 's' and action 'a.'\n",
    "- $ \\alpha $ is the learning rate.\n",
    "- $ R(s, a) $ is the immediate reward received after taking action 'a' in state 's.'\n",
    "- $ \\gamma $ is the discounting rate.\n",
    "- $ \\max_{a'} Q(s', a') $ is the maximum Q-value among possible actions in the next state 's'.\n",
    "\n",
    "The `reset_qtable` method initializes the Q-table with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5n_1gf4Unu_7"
   },
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    def __init__(self, learning_rate, gamma, state_size, action_size):\n",
    "        \"\"\"\n",
    "        Initialize the Q-learning agent.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): The learning rate.\n",
    "            gamma (float): The discounting rate.\n",
    "            state_size (int): Number of possible states.\n",
    "            action_size (int): Number of possible actions.\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.reset_qtable()\n",
    "\n",
    "    def update(self, state, action, reward, new_state):\n",
    "        \"\"\"\n",
    "        Update the Q-table using the Q-learning algorithm.\n",
    "\n",
    "        Args:\n",
    "            state (int): Current state.\n",
    "            action (int): Chosen action.\n",
    "            reward (float): Reward received.\n",
    "            new_state (int): New state after taking action.\n",
    "\n",
    "        Returns:\n",
    "            float: Updated Q-value.\n",
    "        \"\"\"\n",
    "        delta = (\n",
    "            reward\n",
    "            + self.gamma * np.max(self.qtable[new_state, :])\n",
    "            - self.qtable[state, action]\n",
    "        )\n",
    "        q_update = self.qtable[state, action] + self.learning_rate * delta\n",
    "        return q_update\n",
    "\n",
    "    def reset_qtable(self):\n",
    "        \"\"\"Reset the Q-table.\"\"\"\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrYpz2G-n9He"
   },
   "source": [
    "### Step 3: Epsilon-Greedy Exploration Strategy\n",
    "\n",
    "In this section, we define the Epsilon-Greedy exploration strategy class (`EpsilonGreedy`). The strategy is initialized with an exploration probability `epsilon`. The `choose_action` method implements the Epsilon-Greedy strategy to select an action based on whether exploration or exploitation should be performed.\n",
    "\n",
    "Exploration: With probability `epsilon`, a random action is chosen.\n",
    "Exploitation: With probability `1 - epsilon`, the action with the highest Q-value in the current state is selected. If there are multiple actions with the same maximum Q-value, one is chosen randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RybS7o-jn60o"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    def __init__(self, epsilon):\n",
    "        \"\"\"\n",
    "        Initialize the Epsilon-Greedy exploration strategy.\n",
    "\n",
    "        Args:\n",
    "            epsilon (float): Exploration probability.\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_action(self, action_space, state, qtable):\n",
    "        \"\"\"\n",
    "        Choose an action based on the Epsilon-Greedy strategy.\n",
    "\n",
    "        Args:\n",
    "            action_space: Action space of the environment.\n",
    "            state (int): Current state.\n",
    "            qtable: Q-table of the agent.\n",
    "\n",
    "        Returns:\n",
    "            int: Chosen action.\n",
    "        \"\"\"\n",
    "        explor_exploit_tradeoff = rng.uniform(0, 1)\n",
    "\n",
    "        if explor_exploit_tradeoff < self.epsilon:\n",
    "            action = action_space.sample()\n",
    "        else:\n",
    "            if np.all(qtable[state, :]) == qtable[state, 0]:\n",
    "                action = action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(qtable[state, :])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NA3mGARIoL13"
   },
   "source": [
    "### Step 4: Training Loop\n",
    "\n",
    "In this section, we execute the training loop. The outer loop (`run`) runs multiple times to account for stochasticity. Within each run, we reset the Q-table using `learner.reset_qtable()`.\n",
    "\n",
    "The inner loop (`episode`) represents each training episode. We reset the environment, initialize step count and total rewards, and enter the episode loop. In the episode loop, we select actions using the Epsilon-Greedy strategy, perform actions in the environment, and update the Q-table using the Q-learning update equation.\n",
    "\n",
    "We log the rewards, steps, and Q-tables for analysis. The `tqdm` progress bar is used to visualize the progress of training episodes within each run.\n",
    "\n",
    "### Section 5: Analysis and Visualization\n",
    "\n",
    "In this section, you can perform analysis and visualization of training results using the logged data (e.g., rewards, Q-tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWXSD_-5oPy-",
    "outputId": "b93f6a9e-c42a-4c9a-fb51-658bfe58f2fa"
   },
   "outputs": [],
   "source": [
    "learner = Qlearning(\n",
    "    learning_rate=params.learning_rate,\n",
    "    gamma=params.gamma,\n",
    "    state_size=params.state_size,\n",
    "    action_size=params.action_size,\n",
    ")\n",
    "explorer = EpsilonGreedy(\n",
    "    epsilon=params.epsilon,\n",
    ")\n",
    "\n",
    "# Initialize arrays to store rewards, steps, episodes, and Q-tables\n",
    "rewards = np.zeros((params.total_episodes, params.n_runs))\n",
    "steps = np.zeros((params.total_episodes, params.n_runs))\n",
    "episodes = np.arange(params.total_episodes)\n",
    "qtables = np.zeros((params.n_runs, params.state_size, params.action_size))\n",
    "all_states = []\n",
    "all_actions = []\n",
    "\n",
    "# Training loop\n",
    "for run in range(params.n_runs):  # Run multiple times for stochasticity\n",
    "    learner.reset_qtable()  # Reset Q-table between runs\n",
    "\n",
    "    for episode in tqdm(\n",
    "        episodes, desc=f\"Run {run}/{params.n_runs} - Episodes\", leave=False\n",
    "    ):\n",
    "        state = env.reset(seed=params.seed)[0]  # Reset the environment\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        while not done:\n",
    "            action = explorer.choose_action(\n",
    "                action_space=env.action_space, state=state, qtable=learner.qtable\n",
    "            )\n",
    "\n",
    "            all_states.append(state)\n",
    "            all_actions.append(action)\n",
    "\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "            learner.qtable[state, action] = learner.update(\n",
    "                state, action, reward, new_state\n",
    "            )\n",
    "\n",
    "            total_rewards += reward\n",
    "            step += 1\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        rewards[episode, run] = total_rewards\n",
    "        steps[episode, run] = step\n",
    "    qtables[run, :, :] = learner.qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoTFfZ7tou23"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95AOAJ37gGgq",
    "outputId": "1cc02d91-dd75-4e13-cb44-42f1f167d1d6"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "env = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    is_slippery=params.is_slippery,\n",
    "    render_mode=\"rgb_array_list\",\n",
    "    desc=generate_random_map(\n",
    "        size=params.map_size, p=params.proba_frozen, seed=params.seed\n",
    "    ),\n",
    ")\n",
    "_ = env.reset()\n",
    "step_starting_index = 0\n",
    "episode_index = 0\n",
    "explorer.epsilon = 0.0\n",
    "for step_index in range(199):\n",
    "    action = explorer.choose_action(\n",
    "        action_space=env.action_space, state=state, qtable=learner.qtable\n",
    "    )\n",
    "\n",
    "    # Log all states and actions\n",
    "    all_states.append(state)\n",
    "    all_actions.append(action)\n",
    "\n",
    "    learner.qtable[state, action] = learner.update(state, action, reward, new_state)\n",
    "    # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "    new_state, reward, terminated, truncated, info = env.step(action)\n",
    "    # Our new state is state\n",
    "    state = new_state\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        save_video(\n",
    "            env.render(),\n",
    "            \"videos\",\n",
    "            fps=env.metadata[\"render_fps\"],\n",
    "            step_starting_index=step_starting_index,\n",
    "            episode_index=episode_index,\n",
    "        )\n",
    "        step_starting_index = step_index + 1\n",
    "        episode_index += 1\n",
    "        env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "rDNfYdamXPfz",
    "outputId": "fb023029-1b1b-4ede-f4f2-0fcb3416ba6f"
   },
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "mp4 = open(\"/content/videos/rl-video-episode-0.mp4\", \"rb\").read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video width=600 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\"\n",
    "    % data_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwTIIoV4pmgt"
   },
   "source": [
    "# Model 3: [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ttDm5U6pmgt"
   },
   "source": [
    "### Step1: Environment and Parameter Setup\n",
    "load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdxjF8xopmgu"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import NamedTuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tilatw2Jpmgu"
   },
   "source": [
    "In this section, we define the hyperparameters and configuration parameters using a `NamedTuple` called `Params`. These parameters include the total number of episodes, learning rate, discounting rate (gamma), exploration probability (epsilon), map size, seed for reproducibility, slippery environment flag, number of runs, action size, state size, probability of tiles being frozen, and the folder where plots will be saved.\n",
    "\n",
    "We also set the random number generator (`rng`) seed and create the FrozenLake environment using Gym. The environment's description is generated with random frozen tiles based on the specified probability (`proba_frozen`). The action and state sizes are determined based on the environment's properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OiAtunTpmgu"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters and configuration parameters using a NamedTuple\n",
    "\n",
    "\n",
    "class Params(NamedTuple):\n",
    "    total_episodes: int  # Total episodes\n",
    "    learning_rate: float  # Learning rate\n",
    "    seed: int  # Define a seed for reproducible results\n",
    "    n_runs: int  # Number of runs\n",
    "    action_size: int  # Number of possible actions\n",
    "    state_size: int  # Number of possible states\n",
    "    proba_frozen: float  # Probability that a tile is frozen\n",
    "    savefig_folder: Path  # Root folder to save plots\n",
    "    n_episodes: int\n",
    "    start_epsilon: float\n",
    "    epsilon_decay: float\n",
    "    final_epsilon: float\n",
    "\n",
    "\n",
    "# Initialize hyperparameters and configuration parameters\n",
    "start_epsilon = 1.0\n",
    "n_episodes = 100_000\n",
    "params = Params(\n",
    "    total_episodes=5000,\n",
    "    learning_rate=0.01,\n",
    "    n_episodes=n_episodes,\n",
    "    start_epsilon=start_epsilon,\n",
    "    epsilon_decay=start_epsilon / (n_episodes / 2),  # reduce the exploration over time\n",
    "    final_epsilon=0.1,\n",
    "    seed=123,\n",
    "    n_runs=4,\n",
    "    action_size=None,\n",
    "    state_size=None,\n",
    "    proba_frozen=0.9,\n",
    "    savefig_folder=Path(\"../../_static/img/tutorials/\"),\n",
    ")\n",
    "\n",
    "# Set the seed\n",
    "rng = np.random.default_rng(params.seed)\n",
    "\n",
    "# Create the figure folder if it doesn't exist\n",
    "params.savefig_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create the FrozenLake environment with specified parameters\n",
    "env = gym.make(\"Blackjack-v1\", render_mode=\"rgb_array\", natural=False, sab=False)\n",
    "\n",
    "params = params._replace(action_size=env.action_space.n)\n",
    "params = params._replace(state_size=env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9Fj0dFPpmgu"
   },
   "source": [
    "### Step 2: Q-Learning Agent Definition\n",
    "\n",
    "In this section, we define the Q-learning agent class (`Qlearning`). The agent is initialized with learning rate, discounting rate (gamma), state size, and action size as parameters. The Q-table is initialized with zeros.\n",
    "\n",
    "The `update` method implements the Q-learning update equation to update Q-values based on the difference between the current estimate and actual rewards. This equation is expressed as:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot \\left( R(s, a) + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ Q(s, a) $ is the current estimate of the Q-value for state 's' and action 'a.'\n",
    "- $ \\alpha $ is the learning rate.\n",
    "- $ R(s, a) $ is the immediate reward received after taking action 'a' in state 's.'\n",
    "- $ \\gamma $ is the discounting rate.\n",
    "- $ \\max_{a'} Q(s', a') $ is the maximum Q-value among possible actions in the next state 's'.\n",
    "\n",
    "The `reset_qtable` method initializes the Q-table with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pAD4BuMpmgu"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class BlackjackAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NLD4K9yrhCx"
   },
   "outputs": [],
   "source": [
    "agent = BlackjackAgent(\n",
    "    learning_rate=params.learning_rate,\n",
    "    initial_epsilon=params.start_epsilon,\n",
    "    epsilon_decay=params.epsilon_decay,\n",
    "    final_epsilon=params.final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eHyjsHzpmgv"
   },
   "source": [
    "### Step 4: Training Loop\n",
    "\n",
    "In this section, we execute the training loop. The outer loop (`run`) runs multiple times to account for stochasticity. Within each run, we reset the Q-table using `learner.reset_qtable()`.\n",
    "\n",
    "The inner loop (`episode`) represents each training episode. We reset the environment, initialize step count and total rewards, and enter the episode loop. In the episode loop, we select actions using the Epsilon-Greedy strategy, perform actions in the environment, and update the Q-table using the Q-learning update equation.\n",
    "\n",
    "We log the rewards, steps, and Q-tables for analysis. The `tqdm` progress bar is used to visualize the progress of training episodes within each run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lp6QxbL4pmgv",
    "outputId": "c636aca8-d061-412b-fe96-7dfc2e6ac65b"
   },
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update the agent\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhUBUFnYpmgv"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeHWsf2Wpmgv",
    "outputId": "6952362e-8585-4181-e6bf-75a2bfc10e13"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\", render_mode=\"rgb_array_list\", natural=False, sab=False)\n",
    "\n",
    "state, info = env.reset()\n",
    "step_starting_index = 0\n",
    "episode_index = 0\n",
    "explorer.epsilon = 0.0\n",
    "for step_index in range(199):\n",
    "    action = agent.get_action(state)\n",
    "    # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "    new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # update the agent\n",
    "    agent.update(state, action, reward, terminated, new_state)\n",
    "\n",
    "    # Our new state is state\n",
    "    state = new_state\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        save_video(\n",
    "            env.render(),\n",
    "            \"videos\",\n",
    "            fps=env.metadata[\"render_fps\"],\n",
    "            step_starting_index=step_starting_index,\n",
    "            episode_index=episode_index,\n",
    "        )\n",
    "        step_starting_index = step_index + 1\n",
    "        episode_index += 1\n",
    "        state, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "N6vz8aoEpmgw",
    "outputId": "452dac0a-8393-4976-d83d-3ffee4faa737"
   },
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "mp4 = open(\"/content/videos/rl-video-episode-64.mp4\", \"rb\").read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video width=600 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\"\n",
    "    % data_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSL8_xDktkEN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
