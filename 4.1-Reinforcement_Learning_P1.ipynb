{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg1Y7gNMqWlQ"
      },
      "source": [
        "# Reinforcement Learning - an introduction (Part 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VimqeDHYqWlW"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paolodeangelis/Sistemi_a_combustione/blob/main/4.1-Reinforcement_Learning_P1.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Reinforcement Learning with Tic Tac Toe\n",
        "\n",
        "Reinforcement Learning (RL) is a powerful paradigm that has applications in various fields, including energy and chemical engineering. In this notebook, we will explore the fundamentals of RL by using the classic game of Tic Tac Toe (also known as Noughts and Crosses) as an example.\n",
        "\n",
        "### What is Reinforcement Learning?\n",
        "\n",
        "Reinforcement Learning is a type of machine learning where an agent learns to make sequential decisions by interacting with an environment. The core components of reinforcement learning include:\n",
        "\n",
        "- **Agent (A):** The learner or decision-maker that interacts with the environment.\n",
        "- **Environment (E):** The external system with which the agent interacts. It provides feedback to the agent in the form of rewards and state transitions.\n",
        "- **State (S):** A representation of the current situation or configuration of the environment.\n",
        "- **Action (A):** The set of possible choices or decisions that the agent can make.\n",
        "- **Policy (π):** The strategy or rule that defines the agent's behavior, specifying which actions to take in each state.\n",
        "- **Reward (R):** A numerical value that the agent receives from the environment after taking an action in a particular state. The goal of the agent is to maximize the cumulative reward over time.\n",
        "- **Value Function:** The expected cumulative reward that an agent can achieve starting from a particular state while following a given policy π.\n",
        "\n",
        "The value function for each policy can be computed using the Bellman equation:\n",
        "\n",
        "\\[V^{\\pi}(s) = \\sum_{a}\\pi(a|s) \\sum_{s'}P(s' | s, a)[R(s, a, s') + \\gamma V^{\\pi}(s')]\\]\n",
        "\n",
        "where:\n",
        "- \\(V^{\\pi}(s)\\) is the value function for state \\(s\\) under policy \\(\\pi\\).\n",
        "- \\(\\pi(a|s)\\) is the probability of taking action \\(a\\) in state \\(s\\) under policy \\(\\pi\\).\n",
        "- \\(P(s' | s, a)\\) is the probability of transitioning to state \\(s'\\) from state \\(s\\) when taking action \\(a\\).\n",
        "- \\(R(s, a, s')\\) is the immediate reward obtained when transitioning from state \\(s\\) to \\(s'\\) by taking action \\(a\\).\n",
        "- \\(\\gamma\\) is the discount factor.\n",
        "\n",
        "Basic reinforcement learning is modeled as a *Markov decision process*:\n",
        "\n",
        "* a set of environment and agent states,$\\mathcal{S}$;\n",
        "* a set of actions,$\\mathcal{A}$, of the agent;\n",
        "*$P_a(s,s')=\\Pr(S_{t+1}=s'\\mid S_t=s, A_t=a)$, the probability of transition (at time$t$) from state$s$ to state$s'$ under action$a$.\n",
        "*$R_a(s,s')$, the immediate reward after transition from$s$ to$s'$ with action$a$.\n",
        "\n",
        "### How RL Algorithms Work and Learn\n",
        "\n",
        "In RL, the agent learns by interacting with the environment over multiple time steps. The learning process typically follows these steps:\n",
        "\n",
        "<img src=\"https://github.com/paolodeangelis/Sistemi_a_combustione/blob/main/assets/img/AE_loop.png?raw=true\" width=\"500\" alt=\"Agend-Enviroment-Action\">\n",
        "\n",
        "\n",
        "1. **Initialization**: The agent initializes its policy, value functions, and other parameters.\n",
        "\n",
        "2. **Interaction**: The agent takes actions in the environment based on its current policy. It receives rewards from the environment based on its actions.\n",
        "\n",
        "3. **Learning**: The agent updates its policy and value functions based on the rewards received and its interactions with the environment. This is often done using various RL algorithms.\n",
        "\n",
        "4. **Repeat**: Steps 2 and 3 are repeated for many episodes or time steps to improve the agent's performance.\n",
        "\n",
        "The agent's goal is to find an optimal policy that maximizes the cumulative reward over time. This involves a trade-off between exploration (trying new actions to discover better policies) and exploitation (choosing actions that are known to yield high rewards).\n",
        "\n",
        "### RL Algorithms\n",
        "\n",
        "#### 1. Brute Force\n",
        "\n",
        "Brute force RL involves trying every possible policy and selecting the one that yields the highest expected reward. The value function for each policy can be computed using the Bellman equation:\n",
        "\n",
        "However, this approach is usually not feasible for large state and action spaces due to the exponential number of policies.\n",
        "\n",
        "#### 2. Monte Carlo Methods\n",
        "\n",
        "Monte Carlo methods estimate value functions and policies by simulating episodes and averaging the returns obtained. They are well-suited for episodic tasks and are based on the law of large numbers.\n",
        "\n",
        "#### 3. Q-Learning\n",
        "\n",
        "Q-Learning is a model-free, off-policy algorithm that learns Q-values through iterative updates. The Q-value represents the expected cumulative reward for taking a specific action in a specific state. Q-Learning uses the Bellman equation to update Q-values:\n",
        "\n",
        "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha[R(s, a, s') + \\gamma \\max_{a'}Q(s', a') - Q(s, a)]$$\n",
        "\n",
        "where:\n",
        "- $Q(s, a)$ is the Q-value for state-action pair $(s, a)$.\n",
        "- $\\alpha$ is the learning rate.\n",
        "- $R(s, a, s')$ is the immediate reward obtained when transitioning from state $s$ to $s'$ by taking action $a$.\n",
        "- $\\gamma$ is the discount factor.\n",
        "\n",
        "#### 4. Proximal Policy Optimization (PPO)\n",
        "\n",
        "PPO is a policy optimization algorithm that aims to improve policies in an iterative manner. It balances between exploring new policies and exploiting known policies while ensuring stable learning through a clipped objective function. The objective of PPO is to maximize the expected cumulative reward:\n",
        "\n",
        "$$\\max_\\theta \\mathbb{E}[\\min(r(\\theta)\\hat{A}, \\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A})]$$\n",
        "\n",
        "where:\n",
        "- $\\theta$ represents the policy parameters.\n",
        "- $r(\\theta)$ is the ratio of the new policy to the old policy's probability.\n",
        "- $\\hat{A}$ is the advantage function, which estimates the advantage of taking a specific action.\n",
        "- $\\epsilon$ is a hyperparameter that controls the clipping range.\n",
        "\n",
        "Let's get started by setting up the environment and understanding its components.\n"
      ],
      "metadata": {
        "id": "98ftmABKvllH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: Tic-Tac-Toe\n",
        "\n",
        "<img src=\"https://github.com/paolodeangelis/Sistemi_a_combustione/blob/main/assets/img/tic-tac-toe.jpeg?raw=true\" width=\"400\" alt=\"Tic Tac Toe\">\n",
        "\n",
        "In the context of illustrating reinforcement learning, let's consider the game of Tic-Tac-Toe, a simple yet instructive example.\n",
        "\n",
        "### The Tic-Tac-Toe Game\n",
        "\n",
        "Tic-Tac-Toe is a two-player game played on a 3x3 board, where one player uses Xs, and the other uses Os. The objective is to place three marks in a row, horizontally, vertically, or diagonally, to win the game. If the board fills up without a winner, the game ends in a draw.\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "To illustrate reinforcement learning, let's assume we are playing against an imperfect opponent, one whose play allows us to win occasionally. The objective is to construct a player that learns from its opponent's imperfections and maximizes its chances of winning.\n",
        "\n",
        "### Classical Techniques Not Suitable\n",
        "\n",
        "Classical techniques like \"minimax\" from game theory or dynamic programming are not suitable for this problem because they assume knowledge of the opponent's behavior, which is often unavailable in practice.\n",
        "\n",
        "### Reinforcement Learning Approach\n",
        "\n",
        "We can tackle this problem using reinforcement learning. Here's how it works:\n",
        "\n",
        "1. We create a table of values, one for each possible game state, representing the estimated probability of winning from that state. This table is called the value function.\n",
        "\n",
        "2. We start with certain states having known values:\n",
        "   - States with three Xs in a row have a winning probability of 1.\n",
        "   - States with three Os in a row or filled-up states have a winning probability of 0.\n",
        "   - Initial values of other states are set to 0.5, representing a 50% chance of winning.\n",
        "\n",
        "3. We play many games against the opponent, selecting moves that maximize our estimated chances of winning.\n",
        "\n",
        "4. During the game, we update the values of states we encounter. After a move, we adjust the earlier state's value to be closer to the later state's value using a step-size parameter (↵).\n",
        "\n",
        "5. This approach converges to optimal play against the imperfect opponent. The step-size parameter ↵ influences the learning rate.\n",
        "\n"
      ],
      "metadata": {
        "id": "mbyzA2sb6pKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://github.com/paolodeangelis/Sistemi_a_combustione/blob/main/assets/img/RL-action-chain.png?raw=true\" alt=\"Action chain\">\n",
        "<figcaption><strong>Figure 1</strong>: Tic-tac-toe strategy: Solid lines are taken moves, dashed lines are considered but not chosen. * marks the current best move. Exploratory moves, like our second one, don't affect learning. Red arrows show value updates along the tree. (source <a href=\"http://www.incompleteideas.net/book/RLbook2020.pdf\">Reinforcement Learning: An Introduction second edition Richard S. Sutton and Andrew G. Barto</a>)\n",
        "<figcaption>\n",
        "<figure>"
      ],
      "metadata": {
        "id": "_dffiLXl5jpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a Reinforcement Learning Model from Scratch\n",
        "\n",
        "When constructing a RL model from the ground up, we follow a series of structured steps to ensure the success of our learning process. Here's a detailed breakdown of each step:\n",
        "\n",
        "* **1. Setting up the Environment**\n",
        "\n",
        "  Before diving into the specifics of our model, we need to establish the environment in which the learning will take place. This includes:\n",
        "\n",
        "  - Defining the state space: In our Tic-Tac-Toe example, the state space represents all possible configurations of the game board.\n",
        "  - Identifying the action space: This defines the set of possible moves or actions that the RL agent can take in each state.\n",
        "  - Determining the reward structure: We decide how rewards will be assigned to different game outcomes, such as wins, losses, and draws.\n",
        "\n",
        "* **2. Defining the Tic-Tac-Toe Game**\n",
        "\n",
        "  In this step, we define the rules and mechanics of the Tic-Tac-Toe game. This includes:\n",
        "\n",
        "  - Designing the game board: Specifying the size of the board (e.g., 3x3) and how it's represented in code.\n",
        "  - Implementing the game logic: Writing the rules for valid moves, checking for wins, losses, or draws, and updating the game state after each move.\n",
        "  - Handling player interactions: Creating mechanisms for human or agent players to make moves and interact with the game.\n",
        "\n",
        "* **3. Building the Reinforcement Learning Model**\n",
        "\n",
        "  Now, we start constructing the RL model itself. Key elements include:\n",
        "\n",
        "  - Designing the agent: Defining the RL agent that will learn to play Tic-Tac-Toe. This could involve choosing the learning algorithm (e.g., Q-Learning, Deep Q-Networks) and specifying its parameters.\n",
        "  - Developing the state representation: Creating a suitable representation of the game state so that the agent can make decisions based on it.\n",
        "  - Formulating the reward function: Determining how the agent will be rewarded based on the game outcomes and intermediate states.\n",
        "  - Setting exploration-exploitation strategies: Balancing exploration (trying new moves) and exploitation (choosing known good moves) to improve learning.\n",
        "\n",
        "* **4. Training the Model**\n",
        "\n",
        "  In the training phase, we allow the RL agent to play numerous games against opponents (possibly itself). This involves:\n",
        "\n",
        "  - Iterative learning: The agent repeatedly plays the game, makes decisions, and receives rewards. It uses these experiences to update its strategies and policies.\n",
        "  - Reinforcement learning algorithms: Applying the chosen RL algorithm to update the agent's Q-values (or policy) based on reward feedback and state transitions.\n",
        "  - Fine-tuning parameters: Adjusting parameters like the learning rate and discount factor to optimize the learning process.\n",
        "\n",
        "* **5. Testing the Model**\n",
        "\n",
        "  After training, we evaluate the RL model's performance to ensure it has learned an effective policy. This phase includes:\n",
        "\n",
        "  - Assessing gameplay: Having the trained agent play Tic-Tac-Toe against various opponents, including perfect and imperfect players, to gauge its performance.\n",
        "  - Analyzing results: Examining win rates, strategies employed, and any potential shortcomings to identify areas for improvement.\n",
        "  - Iterative refinement: If necessary, returning to earlier steps to modify the agent or environment and retraining the model for better performance.\n"
      ],
      "metadata": {
        "id": "ZPwduMK89bAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 1: Setting up the Environment"
      ],
      "metadata": {
        "id": "Gnm6ttBh-wEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "b0Poj1Rs-6fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "TRMKBcWCyty-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}