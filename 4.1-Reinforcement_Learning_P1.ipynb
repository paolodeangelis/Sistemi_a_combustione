{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg1Y7gNMqWlQ"
      },
      "source": [
        "# Reinforcement Learning - an introduction (Part 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Reinforcement Learning with Tic Tac Toe\n",
        "\n",
        "![Tic Tac Toe](https://github.com/paolodeangelis/Sistemi_a_combustione/blob/main/insert_image_url_here?raw=1)\n",
        "\n",
        "Reinforcement Learning (RL) is a powerful paradigm that has applications in various fields, including energy and chemical engineering. In this notebook, we will explore the fundamentals of RL by using the classic game of Tic Tac Toe (also known as Noughts and Crosses) as an example.\n",
        "\n",
        "### What is Reinforcement Learning?\n",
        "\n",
        "Reinforcement Learning is a type of machine learning where an agent learns to make sequential decisions by interacting with an environment. The core components of reinforcement learning include:\n",
        "\n",
        "- **Agent (A):** The learner or decision-maker that interacts with the environment.\n",
        "- **Environment (E):** The external system with which the agent interacts. It provides feedback to the agent in the form of rewards and state transitions.\n",
        "- **State (S):** A representation of the current situation or configuration of the environment.\n",
        "- **Action (A):** The set of possible choices or decisions that the agent can make.\n",
        "- **Policy (π):** The strategy or rule that defines the agent's behavior, specifying which actions to take in each state.\n",
        "- **Reward (R):** A numerical value that the agent receives from the environment after taking an action in a particular state. The goal of the agent is to maximize the cumulative reward over time.\n",
        "- **Value Function:** The expected cumulative reward that an agent can achieve starting from a particular state while following a given policy π.\n",
        "- **Q-Value Function:** The expected cumulative reward from taking action a in state s and then following a specific policy π.\n",
        "\n",
        "#### Mathematical Notations\n",
        "\n",
        "We can represent these concepts mathematically:\n",
        "\n",
        "- **State-Action Pair:** (S, A) represents a state-action pair, where S is a state, and A is an action.\n",
        "- **Policy (π):** π(a|s) represents the probability of taking action a in state s under policy π.\n",
        "- **State Transition Probability:** P(s' | s, a) represents the probability of transitioning to state s' from state s when taking action a.\n",
        "- **Reward Function:** R(s, a, s') represents the immediate reward obtained when transitioning from state s to s' by taking action a.\n",
        "- **Value Function:** V(s) represents the expected cumulative reward from state s following a specific policy π.\n",
        "\n",
        "### Relations Between Quantities\n",
        "\n",
        "To understand the relationships between these quantities, we can use the Bellman equation, which is a fundamental equation in RL:\n",
        "\n",
        "$$V^{\\pi}(s) = \\sum_{a}\\pi(a|s) \\sum_{s'}P(s' | s, a)[R(s, a, s') + \\gamma V^{\\pi}(s')]$$\n",
        "\n",
        "The Bellman equation relates the value of a state to the expected sum of rewards when following a policy π. It accounts for the probability of taking actions, the probability of state transitions, and the discount factor γ.\n",
        "\n",
        "In addition, the Q-value function is related to the value function and the policy through:\n",
        "\n",
        "$$Q(s, a) = \\sum_{s'}P(s' | s, a)[R(s, a, s') + \\gamma V^{\\pi}(s')]$$\n",
        "\n",
        "This equation expresses the expected cumulative reward of taking action a in state s and then following policy π.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "The primary objective of this notebook is to introduce the concept of RL through practical examples. We will use the Frozen Lake environment, a simple gridworld, to illustrate key RL concepts such as states, actions, rewards, policies, and the learning process.\n",
        "\n",
        "### How RL Algorithms Work and Learn\n",
        "\n",
        "In RL, the agent learns by interacting with the environment over multiple time steps. The learning process typically follows these steps:\n",
        "\n",
        "1. **Initialization**: The agent initializes its policy, value functions, and other parameters.\n",
        "\n",
        "2. **Interaction**: The agent takes actions in the environment based on its current policy. It receives rewards from the environment based on its actions.\n",
        "\n",
        "3. **Learning**: The agent updates its policy and value functions based on the rewards received and its interactions with the environment. This is often done using various RL algorithms.\n",
        "\n",
        "4. **Repeat**: Steps 2 and 3 are repeated for many episodes or time steps to improve the agent's performance.\n",
        "\n",
        "The agent's goal is to find an optimal policy that maximizes the cumulative reward over time. This involves a trade-off between exploration (trying new actions to discover better policies) and exploitation (choosing actions that are known to yield high rewards).\n",
        "\n",
        "### RL Algorithms\n",
        "\n",
        "#### 1. Brute Force\n",
        "\n",
        "Brute force RL involves trying every possible policy and selecting the one that yields the highest expected reward. The value function for each policy can be computed using the Bellman equation:\n",
        "\n",
        "$$V^{\\pi}(s) = \\sum_{a}\\pi(a|s) \\sum_{s'}P(s' | s, a)[R(s, a, s') + \\gamma V^{\\pi}(s')]$$\n",
        "\n",
        "where:\n",
        "- $V^{\\pi}(s)$ is the value function for state $s$ under policy $\\pi$.\n",
        "- $\\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\\pi$.\n",
        "- $P(s' | s, a)$ is the probability of transitioning to state $s'$ from state $s$ when taking action $a$.\n",
        "- $R(s, a, s')$ is the immediate reward obtained when transitioning from state $s$ to $s'$ by taking action $a$.\n",
        "- $\\gamma$ is the discount factor.\n",
        "\n",
        "However, this approach is usually not feasible for large state and action spaces due to the exponential number of policies.\n",
        "\n",
        "#### 2. Monte Carlo Methods\n",
        "\n",
        "Monte Carlo methods estimate value functions and policies by simulating episodes and averaging the returns obtained. They are well-suited for episodic tasks and are based on the law of large numbers.\n",
        "\n",
        "#### 3. Q-Learning\n",
        "\n",
        "Q-Learning is a model-free, off-policy algorithm that learns Q-values through iterative updates. The Q-value represents the expected cumulative reward for taking a specific action in a specific state. Q-Learning uses the Bellman equation to update Q-values:\n",
        "\n",
        "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha[R(s, a, s') + \\gamma \\max_{a'}Q(s', a') - Q(s, a)]$$\n",
        "\n",
        "where:\n",
        "- $Q(s, a)$ is the Q-value for state-action pair $(s, a)$.\n",
        "- $\\alpha$ is the learning rate.\n",
        "- $R(s, a, s')$ is the immediate reward obtained when transitioning from state $s$ to $s'$ by taking action $a$.\n",
        "- $\\gamma$ is the discount factor.\n",
        "\n",
        "#### 4. Proximal Policy Optimization (PPO)\n",
        "\n",
        "PPO is a policy optimization algorithm that aims to improve policies in an iterative manner. It balances between exploring new policies and exploiting known policies while ensuring stable learning through a clipped objective function. The objective of PPO is to maximize the expected cumulative reward:\n",
        "\n",
        "$$\\max_\\theta \\mathbb{E}[\\min(r(\\theta)\\hat{A}, \\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A})]$$\n",
        "\n",
        "where:\n",
        "- $\\theta$ represents the policy parameters.\n",
        "- $r(\\theta)$ is the ratio of the new policy to the old policy's probability.\n",
        "- $\\hat{A}$ is the advantage function, which estimates the advantage of taking a specific action.\n",
        "- $\\epsilon$ is a hyperparameter that controls the clipping range.\n",
        "\n",
        "Let's get started by setting up the environment and understanding its components.\n"
      ],
      "metadata": {
        "id": "98ftmABKvllH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VimqeDHYqWlW"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paolodeangelis/Sistemi_a_combustione/blob/main/4.1-Reinforcement_Learning_P1.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJlX4Ff8qWlY"
      },
      "source": [
        "![working progress](https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/assets/img/warning-work-in-progress.jpg)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}