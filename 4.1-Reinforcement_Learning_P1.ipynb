{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg1Y7gNMqWlQ"
      },
      "source": [
        "# Reinforcement Learning - an introduction (Part 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VimqeDHYqWlW"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paolodeangelis/Sistemi_a_combustione/blob/main/4.1-Reinforcement_Learning_P1.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Reinforcement Learning with Tic Tac Toe\n",
        "\n",
        "Reinforcement Learning (RL) is a powerful paradigm that has applications in various fields, including energy and chemical engineering. In this notebook, we will explore the fundamentals of RL by using the classic game of Tic Tac Toe (also known as Noughts and Crosses) as an example.\n",
        "\n",
        "### What is Reinforcement Learning?\n",
        "\n",
        "Reinforcement Learning is a type of machine learning where an agent learns to make sequential decisions by interacting with an environment. The core components of reinforcement learning include:\n",
        "\n",
        "- **Agent (A):** The learner or decision-maker that interacts with the environment.\n",
        "- **Environment (E):** The external system with which the agent interacts. It provides feedback to the agent in the form of rewards and state transitions.\n",
        "- **State (S):** A representation of the current situation or configuration of the environment.\n",
        "- **Action (A):** The set of possible choices or decisions that the agent can make.\n",
        "- **Policy (π):** The strategy or rule that defines the agent's behavior, specifying which actions to take in each state.\n",
        "- **Reward (R):** A numerical value that the agent receives from the environment after taking an action in a particular state. The goal of the agent is to maximize the cumulative reward over time.\n",
        "- **Value Function:** The expected cumulative reward that an agent can achieve starting from a particular state while following a given policy π.\n",
        "\n",
        "The value function for each policy can be computed using the Bellman equation:\n",
        "\n",
        "$V^{\\pi}(s) = \\sum_{a}\\pi(a|s) \\sum_{s'}P(s' | s, a)[R(s, a, s') + \\gamma V^{\\pi}(s')]$\n",
        "\n",
        "where:\n",
        "-$V^{\\pi}(s)$ is the value function for state $s$ under policy $\\pi$.\n",
        "- $\\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\\pi$.\n",
        "- $P(s' | s, a)$ is the probability of transitioning to state $s'$ from state $s$ when taking action $a$.\n",
        "- $R(s, a, s')$ is the immediate reward obtained when transitioning from state $s$ to $s'$ by taking action $a$.\n",
        "- $\\gamma$ is the discount factor.\n",
        "\n",
        "Basic reinforcement learning is modeled as a *Markov decision process*:\n",
        "\n",
        "* a set of environment and agent states,$\\mathcal{S}$;\n",
        "* a set of actions,$\\mathcal{A}$, of the agent;\n",
        "*$P_a(s,s')=\\Pr(S_{t+1}=s'\\mid S_t=s, A_t=a)$, the probability of transition (at time$t$) from state$s$ to state$s'$ under action$a$.\n",
        "*$R_a(s,s')$, the immediate reward after transition from$s$ to$s'$ with action$a$.\n",
        "\n",
        "### How RL Algorithms Work and Learn\n",
        "\n",
        "In RL, the agent learns by interacting with the environment over multiple time steps. The learning process typically follows these steps:\n",
        "\n",
        "<img src=\"https://github.com/paolodeangelis/Sistemi_a_combustione/blob/main/assets/img/AE_loop.png?raw=true\" width=\"500\" alt=\"Agend-Enviroment-Action\">\n",
        "\n",
        "\n",
        "1. **Initialization**: The agent initializes its policy, value functions, and other parameters.\n",
        "\n",
        "2. **Interaction**: The agent takes actions in the environment based on its current policy. It receives rewards from the environment based on its actions.\n",
        "\n",
        "3. **Learning**: The agent updates its policy and value functions based on the rewards received and its interactions with the environment. This is often done using various RL algorithms.\n",
        "\n",
        "4. **Repeat**: Steps 2 and 3 are repeated for many episodes or time steps to improve the agent's performance.\n",
        "\n",
        "The agent's goal is to find an optimal policy that maximizes the cumulative reward over time. This involves a trade-off between exploration (trying new actions to discover better policies) and exploitation (choosing actions that are known to yield high rewards).\n",
        "\n",
        "### RL Algorithms\n",
        "\n",
        "#### 1. Brute Force\n",
        "\n",
        "Brute force RL involves trying every possible policy and selecting the one that yields the highest expected reward. The value function for each policy can be computed using the Bellman equation:\n",
        "\n",
        "However, this approach is usually not feasible for large state and action spaces due to the exponential number of policies.\n",
        "\n",
        "#### 2. Monte Carlo Methods\n",
        "\n",
        "Monte Carlo methods estimate value functions and policies by simulating episodes and averaging the returns obtained. They are well-suited for episodic tasks and are based on the law of large numbers.\n",
        "\n",
        "#### 3. Q-Learning\n",
        "\n",
        "Q-Learning is a model-free, off-policy algorithm that learns Q-values through iterative updates. The Q-value represents the expected cumulative reward for taking a specific action in a specific state. Q-Learning uses the Bellman equation to update Q-values:\n",
        "\n",
        "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha[R(s, a, s') + \\gamma \\max_{a'}Q(s', a') - Q(s, a)]$$\n",
        "\n",
        "where:\n",
        "- $Q(s, a)$ is the Q-value for state-action pair $(s, a)$.\n",
        "- $\\alpha$ is the learning rate.\n",
        "- $R(s, a, s')$ is the immediate reward obtained when transitioning from state $s$ to $s'$ by taking action $a$.\n",
        "- $\\gamma$ is the discount factor.\n",
        "\n",
        "#### 4. Proximal Policy Optimization (PPO)\n",
        "\n",
        "PPO is a policy optimization algorithm that aims to improve policies in an iterative manner. It balances between exploring new policies and exploiting known policies while ensuring stable learning through a clipped objective function. The objective of PPO is to maximize the expected cumulative reward:\n",
        "\n",
        "$$\\max_\\theta \\mathbb{E}[\\min(r(\\theta)\\hat{A}, \\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A})]$$\n",
        "\n",
        "where:\n",
        "- $\\theta$ represents the policy parameters.\n",
        "- $r(\\theta)$ is the ratio of the new policy to the old policy's probability.\n",
        "- $\\hat{A}$ is the advantage function, which estimates the advantage of taking a specific action.\n",
        "- $\\epsilon$ is a hyperparameter that controls the clipping range.\n",
        "\n",
        "Let's get started by setting up the environment and understanding its components.\n"
      ],
      "metadata": {
        "id": "98ftmABKvllH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: Tic-Tac-Toe\n",
        "\n",
        "<img src=\"https://github.com/paolodeangelis/Sistemi_a_combustione/blob/main/assets/img/tic-tac-toe.jpeg?raw=true\" width=\"400\" alt=\"Tic Tac Toe\">\n",
        "\n",
        "In the context of illustrating reinforcement learning, let's consider the game of Tic-Tac-Toe, a simple yet instructive example.\n",
        "\n",
        "### The Tic-Tac-Toe Game\n",
        "\n",
        "Tic-Tac-Toe is a two-player game played on a 3x3 board, where one player uses Xs, and the other uses Os. The objective is to place three marks in a row, horizontally, vertically, or diagonally, to win the game. If the board fills up without a winner, the game ends in a draw.\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "To illustrate reinforcement learning, let's assume we are playing against an imperfect opponent, one whose play allows us to win occasionally. The objective is to construct a player that learns from its opponent's imperfections and maximizes its chances of winning.\n",
        "\n",
        "### Classical Techniques Not Suitable\n",
        "\n",
        "Classical techniques like \"minimax\" from game theory or dynamic programming are not suitable for this problem because they assume knowledge of the opponent's behavior, which is often unavailable in practice.\n",
        "\n",
        "### Reinforcement Learning Approach\n",
        "\n",
        "We can tackle this problem using reinforcement learning. Here's how it works:\n",
        "\n",
        "1. We create a table of values, one for each possible game state, representing the estimated probability of winning from that state. This table is called the value function.\n",
        "\n",
        "2. We start with certain states having known values:\n",
        "   - States with three Xs in a row have a winning probability of 1.\n",
        "   - States with three Os in a row or filled-up states have a winning probability of 0.\n",
        "   - Initial values of other states are set to 0.5, representing a 50% chance of winning.\n",
        "\n",
        "3. We play many games against the opponent, selecting moves that maximize our estimated chances of winning.\n",
        "\n",
        "4. During the game, we update the values of states we encounter. After a move, we adjust the earlier state's value to be closer to the later state's value using a step-size parameter (↵).\n",
        "\n",
        "5. This approach converges to optimal play against the imperfect opponent. The step-size parameter ↵ influences the learning rate.\n",
        "\n"
      ],
      "metadata": {
        "id": "mbyzA2sb6pKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://github.com/paolodeangelis/Sistemi_a_combustione/blob/main/assets/img/RL-action-chain.png?raw=true\" alt=\"Action chain\">\n",
        "<figcaption><strong>Figure 1</strong>: Tic-tac-toe strategy: Solid lines are taken moves, dashed lines are considered but not chosen. * marks the current best move. Exploratory moves, like our second one, don't affect learning. Red arrows show value updates along the tree. (source <a href=\"http://www.incompleteideas.net/book/RLbook2020.pdf\">Reinforcement Learning: An Introduction second edition Richard S. Sutton and Andrew G. Barto</a>)\n",
        "<figcaption>\n",
        "<figure>"
      ],
      "metadata": {
        "id": "_dffiLXl5jpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a Reinforcement Learning Model from Scratch\n",
        "\n",
        "When constructing a RL model from the ground up, we follow a series of structured steps to ensure the success of our learning process. Here's a detailed breakdown of each step:\n",
        "\n",
        "* **1. Setting up the Environment, i.e defining the Tic-Tac-Toe Game**\n",
        "\n",
        "  Before diving into the specifics of our model, we need to establish the environment in which the learning will take place. This includes:\n",
        "\n",
        "  - Defining the state space: In our Tic-Tac-Toe example, the state space represents all possible configurations of the game board.\n",
        "  - Identifying the action space: This defines the set of possible moves or actions that the RL agent can take in each state.\n",
        "  - Determining the reward structure: We decide how rewards will be assigned to different game outcomes, such as wins, losses, and draws.\n",
        "\n",
        "    For the **tic-tac-toe**  example this means:\n",
        "  - Designing the game board: Specifying the size of the board (e.g., 3x3) and how it's represented in code.\n",
        "  - Implementing the game logic: Writing the rules for valid moves, checking for wins, losses, or draws, and updating the game state after each move.\n",
        "  - Handling player interactions: Creating mechanisms for human or agent players to make moves and interact with the game.\n",
        "\n",
        "\n",
        "* **2. Building the Reinforcement Learning Model**\n",
        "\n",
        "  Now, we start constructing the RL model itself. Key elements include:\n",
        "\n",
        "  - Designing the agent: Defining the RL agent that will learn to play Tic-Tac-Toe. This could involve choosing the learning algorithm (e.g., Q-Learning, Deep Q-Networks) and specifying its parameters.\n",
        "  - Developing the state representation: Creating a suitable representation of the game state so that the agent can make decisions based on it.\n",
        "  - Formulating the reward function: Determining how the agent will be rewarded based on the game outcomes and intermediate states.\n",
        "  - Setting exploration-exploitation strategies: Balancing exploration (trying new moves) and exploitation (choosing known good moves) to improve learning.\n",
        "\n",
        "* **3. Training the Model**\n",
        "\n",
        "  In the training phase, we allow the RL agent to play numerous games against opponents (possibly itself). This involves:\n",
        "\n",
        "  - Iterative learning: The agent repeatedly plays the game, makes decisions, and receives rewards. It uses these experiences to update its strategies and policies.\n",
        "  - Reinforcement learning algorithms: Applying the chosen RL algorithm to update the agent's Q-values (or policy) based on reward feedback and state transitions.\n",
        "  - Fine-tuning parameters: Adjusting parameters like the learning rate and discount factor to optimize the learning process.\n",
        "\n",
        "* **4. Testing the Model**\n",
        "\n",
        "  After training, we evaluate the RL model's performance to ensure it has learned an effective policy. This phase includes:\n",
        "\n",
        "  - Assessing gameplay: Having the trained agent play Tic-Tac-Toe against various opponents, including perfect and imperfect players, to gauge its performance.\n",
        "  - Analyzing results: Examining win rates, strategies employed, and any potential shortcomings to identify areas for improvement.\n",
        "  - Iterative refinement: If necessary, returning to earlier steps to modify the agent or environment and retraining the model for better performance.\n"
      ],
      "metadata": {
        "id": "ZPwduMK89bAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 1 : Setting up the Environment, i.e defining the Tic-Tac-Toe Game"
      ],
      "metadata": {
        "id": "Gnm6ttBh-wEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to:\n",
        "\n",
        "1. Define the enviroment with the object `TicTacToe`\n",
        "2. Explaining the main *methods*\n",
        "3. Test it"
      ],
      "metadata": {
        "id": "u4EiwgR2AihE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "b0Poj1Rs-6fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "TRMKBcWCyty-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class TicTacToe:\n",
        "    def __init__(self, board_rows=3, board_cols=3):\n",
        "        \"\"\"\n",
        "        Initialize the Tic Tac Toe game.\n",
        "\n",
        "        Args:\n",
        "            board_rows (int): Number of rows in the game board.\n",
        "            board_cols (int): Number of columns in the game board.\n",
        "        \"\"\"\n",
        "        self.BOARD_ROWS = board_rows\n",
        "        self.BOARD_COLS = board_cols\n",
        "        self.data = np.zeros((self.BOARD_ROWS, self.BOARD_COLS))\n",
        "        self.winner = None\n",
        "        self.hashVal = None\n",
        "        self.end = None\n",
        "\n",
        "    def getHash(self):\n",
        "        \"\"\"\n",
        "        Calculate the hash value for the current state.\n",
        "\n",
        "        Returns:\n",
        "            int: The unique hash value for the state.\n",
        "        \"\"\"\n",
        "        if self.hashVal is None:\n",
        "            self.hashVal = 0\n",
        "            for i in self.data.reshape(self.BOARD_ROWS * self.BOARD_COLS):\n",
        "                if i == -1:\n",
        "                    i = 2\n",
        "                self.hashVal = self.hashVal * 3 + i\n",
        "        return int(self.hashVal)\n",
        "\n",
        "    def isEnd(self):\n",
        "        \"\"\"\n",
        "        Determine whether the game has ended and who has won.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the game has ended, False otherwise.\n",
        "        \"\"\"\n",
        "        if self.end is not None:\n",
        "            return self.end\n",
        "        results = []\n",
        "\n",
        "        # Check rows and columns\n",
        "        for i in range(0, self.BOARD_ROWS):\n",
        "            results.append(np.sum(self.data[i, :]))\n",
        "            results.append(np.sum(self.data[:, i]))\n",
        "\n",
        "        # Check diagonals\n",
        "        results.append(0)\n",
        "        for i in range(0, self.BOARD_ROWS):\n",
        "            results[-1] += self.data[i, i]\n",
        "        results.append(0)\n",
        "        for i in range(0, self.BOARD_ROWS):\n",
        "            results[-1] += self.data[i, self.BOARD_ROWS - 1 - i]\n",
        "\n",
        "        for result in results:\n",
        "            if result == 3:\n",
        "                self.winner = 1\n",
        "                self.end = True\n",
        "                return self.end\n",
        "            if result == -3:\n",
        "                self.winner = -1\n",
        "                self.end = True\n",
        "                return self.end\n",
        "\n",
        "        # Check for a tie\n",
        "        sum = np.sum(np.abs(self.data))\n",
        "        if sum == self.BOARD_ROWS * self.BOARD_COLS:\n",
        "            self.winner = 0\n",
        "            self.end = True\n",
        "            return self.end\n",
        "\n",
        "        # The game is still ongoing\n",
        "        self.end = False\n",
        "        return self.end\n",
        "\n",
        "    def nextState(self, i, j, symbol):\n",
        "        \"\"\"\n",
        "        Generate the next state after making a move.\n",
        "\n",
        "        Args:\n",
        "            i (int): Row index for the move.\n",
        "            j (int): Column index for the move.\n",
        "            symbol (int): The player's symbol (1 or -1).\n",
        "\n",
        "        Returns:\n",
        "            TicTacToe: The next game state after the move.\n",
        "        \"\"\"\n",
        "        new_game = TicTacToe(self.BOARD_ROWS, self.BOARD_COLS)\n",
        "        new_game.data = np.copy(self.data)\n",
        "        new_game.data[i, j] = symbol\n",
        "        return new_game\n",
        "\n",
        "    def show(self):\n",
        "        \"\"\"\n",
        "        Print the current game board.\n",
        "        \"\"\"\n",
        "        print(\"    1   2   3\")\n",
        "        print(\"  -------------\")\n",
        "        for i in range(0, self.BOARD_ROWS):\n",
        "            print(f\"{i+1} |\", end=' ')\n",
        "            for j in range(0, self.BOARD_COLS):\n",
        "                if self.data[i, j] == 1:\n",
        "                    print('O', end=' ')\n",
        "                if self.data[i, j] == 0:\n",
        "                    print(' ', end=' ')\n",
        "                if self.data[i, j] == -1:\n",
        "                    print('X', end=' ')\n",
        "                if j < 2:\n",
        "                    print('|', end=' ')\n",
        "            print(f\"|  {i+1}\")\n",
        "            if i <= 2:\n",
        "                print(\"  -------------\")\n",
        "        print(\"    1   2   3\\n\")\n"
      ],
      "metadata": {
        "id": "b80weQ64_JKh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the `TicTacToe` Class\n",
        "\n",
        "The `TicTacToe` class represents the Tic-Tac-Toe game. Let's break down its functionalities and methods:\n",
        "\n",
        "\n",
        "#### Initialization\n",
        "\n",
        "```python\n",
        "def __init__(self, board_rows, board_cols):\n",
        "```\n",
        "\n",
        "- The class constructor initializes the game state.\n",
        "- `board_rows` and `board_cols` specify the number of rows and columns in the game board.\n",
        "- The `data` attribute represents the game board as an n * n array, where 1 represents the first player's chessman (X), -1 represents the second player's chessman (O), and 0 represents an empty position.\n",
        "- `winner` is set to `None` initially and will hold the winner's symbol (1 or -1) if there's a winner.\n",
        "- `hashVal` is a unique hash value for the state, computed dynamically.\n",
        "- `end` determines whether the game has ended (True or False)."
      ],
      "metadata": {
        "id": "csUMP5z8A_qw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1fF9ClORmIiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enviroment = TicTacToe()"
      ],
      "metadata": {
        "id": "KoLWzt0ElmDO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get Hash Value\n",
        "\n",
        "```python\n",
        "def getHash(self):\n",
        "```\n",
        "\n",
        "- The `getHash` method calculates a unique hash value for the current game state.\n",
        "- It is used to efficiently represent and index game states.\n",
        "- *Hash function* is any function that can be used to map data of arbitrary size to fixed-size values"
      ],
      "metadata": {
        "id": "BOtMNErrlkP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enviroment.getHash()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kL7Q78LmsWu",
        "outputId": "eb98e76c-86fd-4c5c-e80d-f7fe3c560b97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check if the Game Has Ended\n",
        "\n",
        "```python\n",
        "def isEnd(self):\n",
        "```\n",
        "\n",
        "- The `isEnd` method determines whether the game has ended and, if so, who has won.\n",
        "- It checks for wins in rows, columns, and diagonals, as well as ties.\n",
        "- The game result is stored in `winner` (1 for Player 1, -1 for Player 2, 0 for a tie) and `end` (True for game end, False for ongoing).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s-DVegMsmQO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enviroment.isEnd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz8YQmaEmxYJ",
        "outputId": "5306041c-0a37-4dcd-cf89-f0c0e21e004a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate the Next State\n",
        "\n",
        "```python\n",
        "def nextState(self, i, j, symbol):\n",
        "```\n",
        "\n",
        "- The `nextState` method generates the next game state after making a move.\n",
        "- It takes `i` (row index), `j` (column index), and `symbol` (1 for Player 1, -1 for Player 2) as inputs.\n",
        "- A new `TicTacToe` instance is created with the updated game board.\n"
      ],
      "metadata": {
        "id": "NcON86z0mVkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enviroment = enviroment.nextState(1,1, -1)"
      ],
      "metadata": {
        "id": "I0Kukq-0m0DY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Display the Game Board\n",
        "\n",
        "```python\n",
        "def show(self):\n",
        "```\n",
        "\n",
        "- The `show` method prints the current game board in a human-readable format.\n",
        "- It displays the game board with 'X' for Player 1, 'O' for Player 2, and '0' for empty positions."
      ],
      "metadata": {
        "id": "2gU2S7tAmpyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enviroment.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-vaM3xcnVxH",
        "outputId": "326ecb8e-22e0-4f26-8315-73f1985378c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    1   2   3\n",
            "  -------------\n",
            "1 |   |   |   |  1\n",
            "  -------------\n",
            "2 |   | X |   |  2\n",
            "  -------------\n",
            "3 |   |   |   |  3\n",
            "  -------------\n",
            "    1   2   3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Game\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def play_vs_dumb():\n",
        "    # Initialize the game\n",
        "    game = TicTacToe(3, 3)  # Specify the number of rows and columns\n",
        "    DUMB_SYMBOL = -1\n",
        "    PLAYER_SYMBOL = 1\n",
        "\n",
        "    # Game loop\n",
        "    while not game.isEnd():\n",
        "        clear_output(wait=True)\n",
        "        game.show()  # Use the 'show' method to print the game board\n",
        "\n",
        "        # Dumb Player's Turn (Random Move)\n",
        "        print(\"Dumb Player's Turn (Random Move)\")\n",
        "        available_moves = [(i,j) for i, j in zip(np.where(game.data == 0)[0], np.where(game.data == 0)[1])]\n",
        "\n",
        "        random_move = random.choice(available_moves)\n",
        "        print(random_move)\n",
        "        game = game.nextState(random_move[0], random_move[1], DUMB_SYMBOL)  # Make the dumb player's move\n",
        "\n",
        "        # Check the winner\n",
        "        if game.isEnd():\n",
        "            if game.winner == DUMB_SYMBOL:\n",
        "                clear_output(wait=True)\n",
        "                game.show()\n",
        "                print(\"Dumb Player wins!\")\n",
        "                break\n",
        "            elif game.winner == PLAYER_SYMBOL:\n",
        "                clear_output(wait=True)\n",
        "                game.show()\n",
        "                print(\"Congratulations! You win!\")\n",
        "                break\n",
        "            else:\n",
        "                clear_output(wait=True)\n",
        "                game.show()\n",
        "                print(\"It's a draw!\")\n",
        "                break\n",
        "\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        game.show()  # Display the updated game board after the dumb player's move\n",
        "\n",
        "        # Player's turn\n",
        "        while True:\n",
        "            try:\n",
        "                print(\"Your Turn:\")\n",
        "                row = int(input(\"\\tEnter row (1, 2, or 3): \")) - 1\n",
        "                col = int(input(\"\\tEnter column (1, 2, or 3): \")) - 1\n",
        "                if game.data[row, col] == 0:\n",
        "                    game = game.nextState(row, col, PLAYER_SYMBOL)  # Make the player's move\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Invalid move. Try again.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please enter row and column as integers.\")\n",
        "\n",
        "        # Check the winner\n",
        "        if game.isEnd():\n",
        "            if game.winner == DUMB_SYMBOL:\n",
        "                clear_output(wait=True)\n",
        "                game.show()\n",
        "                print(\"Dumb Player wins!\")\n",
        "                break\n",
        "            elif game.winner == PLAYER_SYMBOL:\n",
        "                clear_output(wait=True)\n",
        "                game.show()\n",
        "                print(\"Congratulations! You win!\")\n",
        "                break\n",
        "            else:\n",
        "                clear_output(wait=True)\n",
        "                game.show()\n",
        "                print(\"It's a draw!\")\n",
        "                break\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LAL0gPZsCxkU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_vs_dumb()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD6lmQWZIDuE",
        "outputId": "63f7f0ca-cf00-4c3b-a850-a1745d6aa262"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    1   2   3\n",
            "  -------------\n",
            "1 | O |   |   |  1\n",
            "  -------------\n",
            "2 | X | O | X |  2\n",
            "  -------------\n",
            "3 |   | X | O |  3\n",
            "  -------------\n",
            "    1   2   3\n",
            "\n",
            "Congratulations! You win!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Building the Reinforcement Learning Model\n",
        "\n",
        "In this step, we will implement the Temporal-Difference (TD) learning agent that will learn to play Tic-Tac-Toe through trial and error. The agent employs the TD learning algorithm to update its value estimates based on state transitions and rewards. TD learning is a fundamental reinforcement learning method that learns to estimate the expected cumulative rewards for each state.\n",
        "\n",
        "#### Temporal-Difference (TD) Learning\n",
        "\n",
        "TD learning is a form of reinforcement learning that updates its value estimates based on the difference between the current estimate and the expected future rewards. The core idea behind TD learning is to learn a state-value function, denoted as V(s), that estimates the expected cumulative reward when starting in state 's' and following the current policy thereafter.\n",
        "\n",
        "The TD learning update equation is as follows:\n",
        "\n",
        "$\n",
        "V(s) \\leftarrow V(s) + \\alpha \\cdot \\left( R + \\gamma \\cdot V(s') - V(s) \\right)\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ V(s) $ is the current estimate of the value for state 's.'\n",
        "- $ \\alpha $ is the learning rate ($ 0 \\leq \\alpha \\leq 1 $) that controls the step size of updates.\n",
        "- $ R $ is the immediate reward received after transitioning from state 's' to state 's'.'\n",
        "- $ \\gamma $ is the discount factor ($ 0 \\leq \\gamma \\leq 1 $) that balances immediate rewards and future rewards.\n",
        "- $ V(s') $ is the value estimate for the next state 's' following the current policy.\n",
        "\n",
        "This explanation provides an overview of the TD learning algorithm and its key components, which are used in the agent's learning process. If you have any further questions or need additional explanations, please let me know."
      ],
      "metadata": {
        "id": "5R9ZG6IkM1Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ausiliar\n",
        "BOARD_ROWS = 3\n",
        "BOARD_COLS = 3\n",
        "\n",
        "def getAllStatesImpl(currentState, currentSymbol, allStates):\n",
        "    for i in range(0, 3):\n",
        "        for j in range(0, 3):\n",
        "            if currentState.data[i][j] == 0:\n",
        "                newState = currentState.nextState(i, j, currentSymbol)\n",
        "                newHash = newState.getHash()\n",
        "                if newHash not in allStates.keys():\n",
        "                    isEnd = newState.isEnd()\n",
        "                    allStates[newHash] = (newState, isEnd)\n",
        "                    if not isEnd:\n",
        "                        getAllStatesImpl(newState, -currentSymbol, allStates)\n",
        "\n",
        "def getAllStates():\n",
        "    currentSymbol = 1\n",
        "    currentState = TicTacToe()\n",
        "    allStates = dict()\n",
        "    allStates[currentState.getHash()] = (currentState, currentState.isEnd())\n",
        "    getAllStatesImpl(currentState, currentSymbol, allStates)\n",
        "    return allStates\n",
        "\n",
        "# all possible board configurations\n",
        "allStates = getAllStates()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "30vFBXXe9fsl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, stepSize=0.1, exploreRate=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the Reinforcement Learning Agent.\n",
        "\n",
        "        Args:\n",
        "            stepSize (float): The step size or learning rate for updating estimations.\n",
        "            exploreRate (float): The exploration rate, controlling the probability of exploration.\n",
        "        \"\"\"\n",
        "        self.allStates = allStates\n",
        "        self.estimations = dict()\n",
        "        self.stepSize = stepSize\n",
        "        self.exploreRate = exploreRate\n",
        "        self.states = []\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the agent's state history.\n",
        "        \"\"\"\n",
        "        self.states = []\n",
        "\n",
        "    def setSymbol(self, symbol):\n",
        "        \"\"\"\n",
        "        Set the agent's symbol and initialize estimations.\n",
        "\n",
        "        Args:\n",
        "            symbol (int): The agent's symbol (1 or -1).\n",
        "        \"\"\"\n",
        "        self.symbol = symbol\n",
        "        for hash in self.allStates.keys():\n",
        "            (state, isEnd) = self.allStates[hash]\n",
        "            if isEnd:\n",
        "                # Initialize estimations for terminal states\n",
        "                self.estimations[hash] = 1.0 if state.winner == self.symbol else 0\n",
        "            else:\n",
        "                # Initialize estimations for non-terminal states\n",
        "                self.estimations[hash] = 0.5\n",
        "\n",
        "    def feedState(self, state):\n",
        "        \"\"\"\n",
        "        Accept a game state and add it to the agent's state history.\n",
        "\n",
        "        Args:\n",
        "            state (State): The current game state.\n",
        "        \"\"\"\n",
        "        self.states.append(state)\n",
        "\n",
        "    def feedReward(self, reward):\n",
        "        \"\"\"\n",
        "        Update estimations based on the received reward using Temporal-Difference Learning.\n",
        "\n",
        "        Args:\n",
        "            reward (float): The received reward.\n",
        "        \"\"\"\n",
        "        if len(self.states) == 0:\n",
        "            return\n",
        "        self.states = [state.getHash() for state in self.states]\n",
        "        target = reward\n",
        "        for latestState in reversed(self.states):\n",
        "            # Temporal-Difference (TD) learning update equation\n",
        "            value = self.estimations[latestState] + self.stepSize * (target - self.estimations[latestState])\n",
        "            self.estimations[latestState] = value\n",
        "            target = value\n",
        "        self.states = []\n",
        "\n",
        "    def takeAction(self):\n",
        "        \"\"\"\n",
        "        Determine the next action to take using an exploration-exploitation strategy.\n",
        "\n",
        "        Returns:\n",
        "            list: A list containing [row, column, symbol] for the next action.\n",
        "        \"\"\"\n",
        "        state = self.states[-1]\n",
        "        nextStates = []\n",
        "        nextPositions = []\n",
        "        for i in range(BOARD_ROWS):\n",
        "            for j in range(BOARD_COLS):\n",
        "                if state.data[i, j] == 0:\n",
        "                    nextPositions.append([i, j])\n",
        "                    nextStates.append(state.nextState(i, j, self.symbol).getHash())\n",
        "        if np.random.binomial(1, self.exploreRate):\n",
        "            # Exploration: Choose a random move\n",
        "            np.random.shuffle(nextPositions)\n",
        "            self.states = []\n",
        "            action = nextPositions[0]\n",
        "            action.append(self.symbol)\n",
        "            return action\n",
        "\n",
        "        values = []\n",
        "        for hash, pos in zip(nextStates, nextPositions):\n",
        "            values.append((self.estimations[hash], pos))\n",
        "        np.random.shuffle(values)\n",
        "        values.sort(key=lambda x: x[0], reverse=True)\n",
        "        # Exploitation: Choose the move with the highest estimated value\n",
        "        action = values[0][1]\n",
        "        action.append(self.symbol)\n",
        "        return action\n",
        "\n",
        "    def savePolicy(self):\n",
        "        \"\"\"\n",
        "        Save the learned policy to a file using pickle.\n",
        "        \"\"\"\n",
        "        fw = open('optimal_policy_' + str(self.symbol), 'wb')\n",
        "        pickle.dump(self.estimations, fw)\n",
        "        fw.close()\n",
        "\n",
        "    def loadPolicy(self):\n",
        "        \"\"\"\n",
        "        Load a learned policy from a file.\n",
        "        \"\"\"\n",
        "        fr = open('optimal_policy_' + str(self.symbol), 'rb')\n",
        "        self.estimations = pickle.load(fr)\n",
        "        fr.close()\n"
      ],
      "metadata": {
        "id": "z4Y02IgLKzit"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize the Agent\n",
        "\n",
        "```python\n",
        "def __init__(self, stepSize=0.1, exploreRate=0.1):\n",
        "    \"\"\"\n",
        "    Initialize the Reinforcement Learning Agent.\n",
        "\n",
        "    Args:\n",
        "        stepSize (float): The step size or learning rate for updating estimations.\n",
        "        exploreRate (float): The exploration rate, controlling the probability of exploration.\n",
        "    \"\"\"\n",
        "    self.allStates = allStates\n",
        "    self.estimations = dict()\n",
        "    self.stepSize = stepSize\n",
        "    self.exploreRate = exploreRate\n",
        "    self.states = []\n",
        "```\n",
        "\n",
        "- Initialize the agent with the specified learning rate and exploration rate.\n",
        "- Create a dictionary to store state estimations.\n",
        "- Initialize an empty list to keep track of visited states."
      ],
      "metadata": {
        "id": "TFoPih9fTqMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reset the agent's state history.\n",
        "\n",
        "```python\n",
        "def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the agent's state history.\n",
        "    \"\"\"\n",
        "    self.states = []\n",
        "```\n",
        "\n",
        "- Clear the list of visited states."
      ],
      "metadata": {
        "id": "-v-xVM-EU5J9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set the agent's symbol and initialize estimations.\n",
        "\n",
        "```python\n",
        "def setSymbol(self, symbol):\n",
        "    \"\"\"\n",
        "    Set the agent's symbol and initialize estimations.\n",
        "\n",
        "    Args:\n",
        "        symbol (int): The agent's symbol (1 or -1).\n",
        "    \"\"\"\n",
        "    self.symbol = symbol\n",
        "    for hash in self.allStates.keys():\n",
        "        (state, isEnd) = self.allStates[hash]\n",
        "        if isEnd:\n",
        "            self.estimations[hash] = 1.0 if state.winner == self.symbol else 0\n",
        "        else:\n",
        "            self.estimations[hash] = 0.5\n",
        "```\n",
        "\n",
        "- Set the agent's symbol (1 or -1).\n",
        "- Initialize state estimations for terminal and non-terminal states based on the symbol.\n"
      ],
      "metadata": {
        "id": "H0I7HA3IU8Df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### State\n",
        "\n",
        "Accept a game state and add it to the agent's state history.\n",
        "\n",
        "```python\n",
        "def feedState(self, state):\n",
        "    \"\"\"\n",
        "    Accept a game state and add it to the agent's state history.\n",
        "\n",
        "    Args:\n",
        "        state (State): The current game state.\n",
        "    \"\"\"\n",
        "    self.states.append(state)\n",
        "```\n",
        "\n",
        "- Append the current game state to the list of visited states.\n",
        "\n"
      ],
      "metadata": {
        "id": "AUa-R5BuU_Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reward\n",
        "Update estimations based on the received reward using Temporal-Difference Learning (TD).\n",
        "\n",
        "```python\n",
        "def feedReward(self, reward):\n",
        "    \"\"\"\n",
        "    Update estimations based on the received reward using Temporal-Difference Learning (TD).\n",
        "\n",
        "    Args:\n",
        "        reward (float): The received reward.\n",
        "    \"\"\"\n",
        "    if len(self.states) == 0:\n",
        "        return\n",
        "    self.states = [state.getHash() for state in self.states]\n",
        "    target = reward\n",
        "    for latestState in reversed(self.states):\n",
        "        value = self.estimations[latestState] + self.stepSize * (target - self.estimations[latestState])\n",
        "        self.estimations[latestState] = value\n",
        "        target = value\n",
        "    self.states = []\n",
        "```\n",
        "\n",
        "- Update state estimations using Temporal-Difference (TD) learning with the received reward.\n",
        "\n"
      ],
      "metadata": {
        "id": "ESPnfwTw3TqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Determine the next action to take using an exploration-exploitation strategy.\n",
        "\n",
        "```python\n",
        "def takeAction(self):\n",
        "    \"\"\"\n",
        "    Determine the next action to take using an exploration-exploitation strategy.\n",
        "\n",
        "    Returns:\n",
        "        list: A list containing [row, column, symbol] for the next action.\n",
        "    \"\"\"\n",
        "    state = self.states[-1]\n",
        "    nextStates = []\n",
        "    nextPositions = []\n",
        "    for i in range(BOARD_ROWS):\n",
        "        for j in range(BOARD_COLS):\n",
        "            if state.data[i, j] == 0:\n",
        "                nextPositions.append([i, j])\n",
        "                nextStates.append(state.nextState(i, j, self.symbol).getHash())\n",
        "    if np.random.binomial(1, self.exploreRate):\n",
        "        np.random.shuffle(nextPositions)\n",
        "        self.states = []\n",
        "        action = nextPositions[0]\n",
        "        action.append(self.symbol)\n",
        "        return action\n",
        "\n",
        "    values = []\n",
        "    for hash, pos in zip(nextStates, nextPositions):\n",
        "        values.append((self.estimations[hash], pos))\n",
        "    np.random.shuffle(values)\n",
        "    values.sort(key=lambda x: x[0], reverse=True)\n",
        "    action = values[0][1]\n",
        "    action.append(self.symbol)\n",
        "    return action\n",
        "```\n",
        "\n",
        "- Determine the next action to take using an exploration-exploitation strategy.\n",
        "- Exploration: Choose a random move with a probability of `exploreRate`.\n",
        "- Exploitation: Choose the move with the highest estimated value."
      ],
      "metadata": {
        "id": "LG8Wsed53YaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Save and Loadthe learned policy to a file.\n",
        "\n",
        "```python\n",
        "def savePolicy(self):\n",
        "    \"\"\"\n",
        "    Save the learned policy to a file.\n",
        "    \"\"\"\n",
        "    fw = open('optimal_policy_' + str(self.symbol), 'wb')\n",
        "    pickle.dump(self.estimations, fw)\n",
        "    fw.close()\n",
        "```\n",
        "\n",
        "- Save the learned policy to a file using pickle.\n",
        "\n",
        "```python\n",
        "def loadPolicy(self):\n",
        "    \"\"\"\n",
        "    Load a learned policy from a file.\n",
        "    \"\"\"\n",
        "    fr = open('optimal_policy_' + str(self.symbol), 'rb')\n",
        "    self.estimations = pickle.load(fr)\n",
        "    fr.close()\n",
        "```\n",
        "\n",
        "- Load a previously learned policy from a file."
      ],
      "metadata": {
        "id": "Mpm587VM3c0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 : Training the Model"
      ],
      "metadata": {
        "id": "r9tHCDvBVpQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Judger` Object\n",
        "\n",
        "In this step, we will implement the `Judger` object that facilitates the training and evaluation of our reinforcement learning agents in playing Tic-Tac-Toe. The `Judger` object is responsible for managing the game, determining the winner, and providing feedback to the agents.\n",
        "\n",
        "The `Judger` object is designed to facilitate the training and evaluation process of our reinforcement learning agents. It takes two agents as input: `agent1` and `agent2`, representing the two players who will play Tic-Tac-Toe against each other. The `feedback` parameter controls whether both players receive rewards when the game ends. If `feedback` is set to `True`, both players receive rewards based on the game outcome. Otherwise, only the winner receives a reward.\n",
        "\n",
        "The `Judger` object maintains the current state of the game, the current player's turn, and handles the game loop. It also manages the transition between game states and checks for game termination conditions."
      ],
      "metadata": {
        "id": "vksSN3xW5B3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "class Judger:\n",
        "    def __init__(self, agent1, agent2, feedback=True):\n",
        "        \"\"\"\n",
        "        Initialize the Judger object.\n",
        "\n",
        "        Args:\n",
        "            agent1 (Agent): The first player (agent) in the game.\n",
        "            agent2 (Agent): The second player (agent) in the game.\n",
        "            feedback (bool): If True, both players receive rewards when the game ends.\n",
        "        \"\"\"\n",
        "        self.p1 = agent1\n",
        "        self.p2 = agent2\n",
        "        self.feedback = feedback\n",
        "        self.currentPlayer = None\n",
        "        self.p1Symbol = 1\n",
        "        self.p2Symbol = -1\n",
        "        self.p1.setSymbol(self.p1Symbol)\n",
        "        self.p2.setSymbol(self.p2Symbol)\n",
        "        self.currentState = TicTacToe()\n",
        "        self.allStates = allStates\n",
        "\n",
        "    def giveReward(self):\n",
        "        \"\"\"\n",
        "        Assign rewards to both players based on the game outcome.\n",
        "        If one player wins, the winning player receives a reward of 1, and the other player receives a reward of 0.\n",
        "        If the game ends in a draw, both players receive intermediate rewards.\n",
        "        \"\"\"\n",
        "        if self.currentState.winner == self.p1Symbol:\n",
        "            self.p1.feedReward(1)\n",
        "            self.p2.feedReward(0)\n",
        "        elif self.currentState.winner == self.p2Symbol:\n",
        "            self.p1.feedReward(0)\n",
        "            self.p2.feedReward(1)\n",
        "        else:\n",
        "            self.p1.feedReward(0.1)\n",
        "            self.p2.feedReward(0.5)\n",
        "\n",
        "    def feedCurrentState(self):\n",
        "        \"\"\"Feed the current game state to both agents.\"\"\"\n",
        "        self.p1.feedState(self.currentState)\n",
        "        self.p2.feedState(self.currentState)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the game and agents to start a new round of Tic-Tac-Toe.\"\"\"\n",
        "        self.p1.reset()\n",
        "        self.p2.reset()\n",
        "        self.currentState = TicTacToe()\n",
        "        self.currentPlayer = None\n",
        "\n",
        "    def play(self, show=False):\n",
        "        \"\"\"\n",
        "        Orchestrate the game loop where players take turns making moves until the game ends.\n",
        "\n",
        "        Args:\n",
        "            show (bool): If True, display the game board after each move.\n",
        "        \"\"\"\n",
        "        self.reset()\n",
        "        self.feedCurrentState()\n",
        "        while True:\n",
        "            # set current player\n",
        "            if self.currentPlayer == self.p1:\n",
        "                self.currentPlayer = self.p2\n",
        "            else:\n",
        "                self.currentPlayer = self.p1\n",
        "            if show:\n",
        "                self.currentState.show()\n",
        "            [i, j, symbol] = self.currentPlayer.takeAction()\n",
        "            self.currentState = self.currentState.nextState(i, j, symbol)\n",
        "            hashValue = self.currentState.getHash()\n",
        "            self.currentState, isEnd = self.allStates[hashValue]\n",
        "            self.feedCurrentState()\n",
        "            if isEnd:\n",
        "                if self.feedback:\n",
        "                    self.giveReward()\n",
        "                return self.currentState.winner\n"
      ],
      "metadata": {
        "id": "kkS85VFf4bXD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Initialize\n",
        "\n",
        "Initialize the `Judger` object with two agents, `agent1` and `agent2`, representing the two players in the game. The `feedback` parameter, when set to `True`, allows both players to receive rewards when the game ends. It sets the initial `currentPlayer` to `None`.\n",
        "\n",
        "```python\n",
        "def __init__(self, agent1, agent2, feedback=True):\n",
        "    \"\"\"\n",
        "    Initialize the Judger object.\n",
        "\n",
        "    Args:\n",
        "        agent1 (Agent): The first player (agent) in the game.\n",
        "        agent2 (Agent): The second player (agent) in the game.\n",
        "        feedback (bool): If True, both players receive rewards when the game ends.\n",
        "    \"\"\"\n",
        "```"
      ],
      "metadata": {
        "id": "Zqel4TRv4hKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Assign rewards\n",
        "Assign rewards to both players based on the game outcome. If one player wins, the winning player receives a reward of 1, and the other player receives a reward of 0. If the game ends in a draw, both players receive intermediate rewards (e.g., 0.1 and 0.5).\n",
        "\n",
        "```python\n",
        "def giveReward(self):\n",
        "    \"\"\"\n",
        "    Assign rewards to both players based on the game outcome.\n",
        "    If one player wins, the winning player receives a reward of 1, and the other player receives a reward of 0.\n",
        "    If the game ends in a draw, both players receive intermediate rewards.\n",
        "    \"\"\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "ispZP_mN49W7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Feed the current game state\n",
        "Feed the current game state to both agents, allowing them to make informed decisions based on the current game state.\n",
        "\n",
        "```python\n",
        "def feedCurrentState(self):\n",
        "    \"\"\"Feed the current game state to both agents.\"\"\"\n",
        "    self.agent1.feedState(self.agent1.currentState)\n",
        "    self.agent2.feedState(self.agent2.currentState)\n",
        "```"
      ],
      "metadata": {
        "id": "Dof3d-um51t7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Reset agents\n",
        "The `reset()` method resets the game, agents, and game state to start a new round of Tic-Tac-Toe.\n",
        "\n",
        "```python\n",
        "def reset(self):\n",
        "    \"\"\"Reset the game, agents, and game state to start a new round of Tic-Tac-Toe.\"\"\"\n",
        "    self.agent1.reset()\n",
        "    self.agent2.reset()\n",
        "    self.currentPlayer = None\n",
        "```"
      ],
      "metadata": {
        "id": "3ir4E6pF55GI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Let the two agents play\n",
        "The `play()` method orchestrates the game loop, where players take turns making moves until the game ends. If `show` is set to `True`, it will display the game board after each move.\n",
        "\n",
        "```python\n",
        "def play(self, show=False):\n",
        "    \"\"\"\n",
        "    Orchestrate the game loop where players take turns making moves until the game ends.\n",
        "\n",
        "    Args:\n",
        "        show (bool): If True, display the game board after each move.\n",
        "    \"\"\"\n",
        "```"
      ],
      "metadata": {
        "id": "d0qqB8Vh57G8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train function"
      ],
      "metadata": {
        "id": "7uBSyxPo66Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs=20000, stepSize=0.1, exploreRate=0.1):\n",
        "    \"\"\"\n",
        "    Train two agents to play Tic-Tac-Toe using Q-learning.\n",
        "\n",
        "    Args:\n",
        "        epochs (int): The number of training epochs (games). Default is 20,000.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Create two agents\n",
        "    agent1 = Agent(stepSize=stepSize, exploreRate=exploreRate)\n",
        "    agent2 = Agent(stepSize=stepSize, exploreRate=exploreRate)\n",
        "\n",
        "    # Create a Judger to manage the game\n",
        "    judger = Judger(agent1, agent2)\n",
        "\n",
        "    # Initialize win counts for each player\n",
        "    agent1_wins = 0.0\n",
        "    agent2_wins = 0.0\n",
        "\n",
        "    # Training loop with tqdm progress bar\n",
        "    for i in tqdm(range(epochs), desc=\"Training\", ncols=100):\n",
        "        # Play a game with the Judger\n",
        "        winner = judger.play()\n",
        "\n",
        "        # Update win counts based on the game outcome\n",
        "        if winner == 1:\n",
        "            agent1_wins += 1\n",
        "        elif winner == -1:\n",
        "            agent2_wins += 1\n",
        "\n",
        "        # Reset the Judger for the next game\n",
        "        judger.reset()\n",
        "\n",
        "    # Print win rates for both agents\n",
        "    agent1_win_rate = agent1_wins / epochs\n",
        "    agent2_win_rate = agent2_wins / epochs\n",
        "    print()\n",
        "    print(f\"Agent 1 Win Rate: {agent1_win_rate:.4f}\")\n",
        "    print(f\"Agent 2 Win Rate: {agent2_win_rate:.4f}\")\n",
        "\n",
        "    # Save learned policies for both agents\n",
        "    agent1.savePolicy()\n",
        "    agent2.savePolicy()\n"
      ],
      "metadata": {
        "id": "xs3M8gynUsHh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train function trains two agents using Q-learning to play Tic-Tac-Toe over a specified number of epochs.\n",
        "It initializes two agents, `agent1` and `agent2`, and a Judger to manage the game.\n",
        "Win counts for each agent are tracked, and the training loop plays games with the `Judger`.\n",
        "After each game, the win counts are updated based on the game outcome, and the `Judger` is reset for the next game.\n",
        "The function prints the win rates for both agents and saves their learned policies."
      ],
      "metadata": {
        "id": "QIxbd7gfWTMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run Train"
      ],
      "metadata": {
        "id": "KRQmpVxu7zSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 3000\n",
        "alpha = 0.25\n",
        "epsilon = 0.1"
      ],
      "metadata": {
        "id": "DZ_olt8UWS0D"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(epochs=num_episodes, stepSize=alpha, exploreRate=epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KYRWVzBXTxl",
        "outputId": "a580815a-7701-4318-a86c-84e41beb17cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|███████████████████████████████████████████████| 3000/3000 [00:02<00:00, 1323.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Agent 1 Win Rate: 0.3733\n",
            "Agent 2 Win Rate: 0.3647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Agent's Performance\n",
        "\n",
        "After training the  agent, it's essential to evaluate its performance by playing a series of games against a random player. This allows us to measure how well the agent has learned to play Tic-Tac-Toe.\n",
        "\n",
        "We'll create a `test` function to achieve this. The `test` function takes two parameters: the trained agent and the number of games to play. It returns the percentage of games won by the agent.\n",
        "\n",
        "In the `test` function, we play the specified number of games. At each step, we determine the current player and let the agent or the random player make a move accordingly. If the game is won by the agent, we increment the win count and calculate the win percentage. This function allows us to assess the agent's performance in real game scenarios."
      ],
      "metadata": {
        "id": "f1UhVW6ueH4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(num_games):\n",
        "    ai_player = Agent(exploreRate=0)\n",
        "    dumb_player = Agent(exploreRate=1)\n",
        "    judger = Judger(ai_player, dumb_player, False)\n",
        "    ai_player.loadPolicy()\n",
        "    player1Win = 0.0\n",
        "    player2Win = 0.0\n",
        "    for i in tqdm(range(num_games), desc=\"Games\", ncols=100):\n",
        "        winner = judger.play()\n",
        "        if winner == 1:\n",
        "            player1Win += 1\n",
        "        if winner == -1:\n",
        "            player2Win += 1\n",
        "        judger.reset()\n",
        "    print()\n",
        "    print(f\"RL Agend wins rates: {player1Win/num_games*100:.4f} %\")\n",
        "    print(f\"Dumb Agend wins: {player2Win/num_games*100:.4f} %\")"
      ],
      "metadata": {
        "id": "FQrrVstReNkz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VruJV8rZe0b1",
        "outputId": "57baa850-e6e5-42bb-fb4d-cfd0d7f3f3bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Games: 100%|██████████████████████████████████████████████████| 5000/5000 [00:02<00:00, 1904.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RL Agend wins rates: 89.6600 %\n",
            "Dumb Agend wins: 6.8600 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's play against AI"
      ],
      "metadata": {
        "id": "oG8TKuP1BlTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Game\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class HumanPlayer:\n",
        "    def __init__(self, stepSize = 0.1, exploreRate=0.1):\n",
        "        self.symbol = None\n",
        "        self.currentState = None\n",
        "        return\n",
        "    def reset(self):\n",
        "        return\n",
        "    def setSymbol(self, symbol):\n",
        "        self.symbol = symbol\n",
        "        return\n",
        "    def feedState(self, state):\n",
        "        self.currentState = state\n",
        "        return\n",
        "    def feedReward(self, reward):\n",
        "        return\n",
        "    def takeAction(self):\n",
        "        clear_output(wait=True)\n",
        "        self.currentState.show()\n",
        "        print(\"Your Turn:\")\n",
        "        row = int(input(\"\\tEnter row (1, 2, or 3): \")) - 1\n",
        "        col = int(input(\"\\tEnter column (1, 2, or 3): \")) - 1\n",
        "        if self.currentState.data[row, col] != 0:\n",
        "            return (row, col, self.symbol)\n",
        "        return (row, col, self.symbol)\n",
        "\n",
        "\n",
        "def play_vs_ai():\n",
        "    player1 = Agent(exploreRate=0)\n",
        "    player2 = HumanPlayer()\n",
        "    judger = Judger(player2, player1, False)\n",
        "    player1.loadPolicy()\n",
        "    winner = judger.play(False)\n",
        "    if winner == player2.symbol:\n",
        "        print(\"Win!\")\n",
        "    elif winner == player1.symbol:\n",
        "        print(\"Lose!\")\n",
        "    else:\n",
        "        print(\"Tie!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R7ZQ9TAZCDq6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_vs_ai()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "6FMrgb-nF1f3",
        "outputId": "f3dcfe01-0fdc-494e-83f8-32ae739ae29b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    1   2   3\n",
            "  -------------\n",
            "1 |   |   |   |  1\n",
            "  -------------\n",
            "2 |   | X |   |  2\n",
            "  -------------\n",
            "3 |   |   | O |  3\n",
            "  -------------\n",
            "    1   2   3\n",
            "\n",
            "Your Turn:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b22567aa036c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay_vs_ai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-6ab227839347>\u001b[0m in \u001b[0;36mplay_vs_ai\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mjudger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJudger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mplayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mwinner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjudger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwinner\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mplayer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Win!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ae42dab386a6>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, show)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentPlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtakeAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mhashValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetHash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-6ab227839347>\u001b[0m in \u001b[0;36mtakeAction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your Turn:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tEnter row (1, 2, or 3): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tEnter column (1, 2, or 3): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5NXd8F7aHMD4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}