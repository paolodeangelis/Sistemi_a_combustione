{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Python 3 (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paolodeangelis/Sistemi_a_combustione/blob/main/1.2-Hands-on_Python3_P2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the libraries needed to approach the following engineering problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Signal analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "In this code section, we import necessary libraries for numerical signal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # import numpy\n",
    "from matplotlib import pyplot as plt  # import the plotting library\n",
    "from scipy.fft import fft, fftfreq  # import the FFT methods\n",
    "from scipy.signal import butter, freqz, lfilter  # import methods for signal analysis\n",
    "\n",
    "plt.style.use(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "Let's begin by defining the problem:\n",
    "\n",
    "In our hypothetical experiment, we have a Data Acquisition (DAQ) instrument with a sampling frequency of 50 kHz. We are conducting measurements on a rotating device with a speed of 3000 rotations per minute. To ensure accurate measurements, the time interval at which we sample should be a multiple of the device's rotational period.\n",
    "\n",
    "The rotational period ($\\tau$) is given by the reciprocal of the rotational frequency ($f$), and in this case:\n",
    "\n",
    "$$ \\tau = \\dfrac{1}{f} = \\dfrac{60}{3000} = 0.02\\;\\text{s} $$\n",
    "\n",
    "Hence, we decide to sample the signal at a rate that is a multiple of this period, so we choose to sample it 10 times during each period.\n",
    "\n",
    "In the following code, we will:\n",
    "\n",
    "1. Define the frequencies of the events that we intend to measure and specify the high frequencies of the noise.\n",
    "2. Create a time array ($t$) to represent the timing of the measurements.\n",
    "3. Finally, we will compute the resulting measured signal by combining the desired signal and the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Hz] Sampling frequency\n",
    "SAMPLE_FREQ = 50e3  # [Hz]\n",
    "# [Hz] Signal frequencies\n",
    "WAVE_FREQ_1 = 5e2  # [Hz] main event\n",
    "WAVE_FREQ_2 = 8.8e2  # [Hz] secondary event\n",
    "WAVE_FREQ_3 = 1.1e3  # [Hz] secondary event\n",
    "# [Hz] Noise frequencies\n",
    "NOISE_FREQ_1 = 7e3  # [Hz]\n",
    "NOISE_FREQ_2 = 12e3  # [Hz]\n",
    "NOISE_FREQ_3 = 15e3  # [Hz]\n",
    "NOISE_FREQ_4 = 23.5e3  # [Hz]\n",
    "\n",
    "\n",
    "# Create signals (wave and noise)\n",
    "t = np.arange(0, 10.0 / WAVE_FREQ_1, 1 / SAMPLE_FREQ)  # [s] sampling interval\n",
    "wave = np.sum(\n",
    "    [\n",
    "        3.3 * np.sin(2 * np.pi * WAVE_FREQ_1 * t),  # wave defined as A*sin(2*pi*f*t)\n",
    "        1.8 * np.sin(2 * np.pi * WAVE_FREQ_2 * t),\n",
    "        1.2 * np.sin(2 * np.pi * WAVE_FREQ_3 * t),\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "noise = np.sum(\n",
    "    [\n",
    "        0.7 * np.sin(2 * np.pi * NOISE_FREQ_1 * t),\n",
    "        0.5 * np.sin(2 * np.pi * NOISE_FREQ_2 * t),\n",
    "        0.8 * np.sin(2 * np.pi * NOISE_FREQ_3 * t),\n",
    "        0.5 * np.sin(2 * np.pi * NOISE_FREQ_4 * t),\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "# Combine wave and noise to create the \"measured\" signal\n",
    "signal = wave + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "In this code block, we are visualizing the problem using the `matplotlib` plot library to create a graphical representation of the signal and noise components. Here's a step-by-step explanation:\n",
    "\n",
    "1. We define a `figure` object named `fig` with `plt.figure`. This object serves as the canvas on which our plot will be displayed.\n",
    "\n",
    "2. The `figsize=(7, 3.5)` argument sets the dimensions of the plot. In this case, it creates a figure that is 7 inches wide and 3.5 inches tall. This specification controls the aspect ratio of the plot.\n",
    "\n",
    "3. We use the `plt.style.context(\"seaborn-v0_8-paper\")` context manager to set the plotting style to \"seaborn-v0_8-paper.\" This style provides a particular appearance and formatting for the plot, enhancing its readability and aesthetics.\n",
    "\n",
    "4. Within the context of the figure, we add a subplot with `ax = fig.add_subplot(111)`. The `add_subplot` function creates a subplot within the figure. The `111` argument means that this is a single subplot (1 row, 1 column), and it is the first subplot.\n",
    "\n",
    "5. We create multiple line plots on the subplot using the `ax.plot()` function. The `t` array on the x-axis represents time, while the `signal`, `noise`, and `wave` arrays on the y-axis represent the signal with noise, noise alone, and the pure signal, respectively. Various parameters are used to customize the plot:\n",
    "   - `alpha` specifies the opacity (transparency) of the lines, making the signal with noise more prominent (alpha=0.75) and the noise less prominent (alpha=0.5).\n",
    "   - `label` assigns labels to each line for creating a legend.\n",
    "   - `color` sets the color of the pure signal to black ('k').\n",
    "\n",
    "6. We add a legend to the plot using `ax.legend()`. The legend helps identify and differentiate the plotted lines by associating them with their respective labels.\n",
    "\n",
    "7. The x-axis label is set to \"Time [s]\" with `ax.set_xlabel(\"Time [s]\")`, and the y-axis label is set to \"Amplitude [-]\" with `ax.set_ylabel(\"Amplitude [-]\")`. These labels provide context and scale for interpreting the plot.\n",
    "\n",
    "8. Finally, `plt.show()` is used to display the plot on the screen or in the Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 3.5))\n",
    "\n",
    "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(t, signal, alpha=0.75, label=\"signal with noise\")\n",
    "    ax.plot(t, noise, alpha=0.5, label=\"noise\")\n",
    "    ax.plot(t, wave, label=\"signal\", color=\"k\")\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"Amplitude [-]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral analysis\n",
    "\n",
    "Now, we analyze the signal in the frequency domain. When dealing with periodic functions, we utilize the mathematical tool known as the [Fourier Transform (FT)](https://en.wikipedia.org/wiki/Fourier_transform). It allows us to transition from the time domain to the frequency domain (or, in the case of spatial periodicity, the reciprocal space). This transformation is defined by the following equations:\n",
    "\n",
    "- Forward Transform:\n",
    "  $$ F(\\omega) = \\int_{-\\infty}^{\\infty} f(t) \\cdot e^{-j 2\\pi \\omega t} \\, dt $$\n",
    "\n",
    "- Inverse Transform:\n",
    "  $$ f(t) = \\int_{-\\infty}^{\\infty} F(\\omega) \\cdot e^{j 2\\pi \\omega t} \\, d\\omega $$\n",
    "\n",
    "Numerically, this transformation can be performed using the [Fast Fourier Transform (FFT)](https://en.wikipedia.org/wiki/Fast_Fourier_transform). In essence, it approximates the integral with a sum, truncated to sufficiently high frequencies. The formula for the FFT is given by:\n",
    "\n",
    "$$ F_k = \\sum_{m=0}^{n-1} f_m \\cdot e^{-j2\\pi k m/n} \\quad \\text{for } k = 0, \\ldots, n-1 $$\n",
    "\n",
    "This essential tool is readily implemented in the `scipy.fft` library.\n",
    "\n",
    "To remove noise from our signal, we employ a low-pass filter ($LP(\\omega)$) designed using the [Butterworth filter](https://en.wikipedia.org/wiki/Butterworth_filter) design method. In filter design, we specify critical parameters, including the filter order and the cutoff frequency ($f_{c}$). \n",
    "\n",
    "The cutoff frequency is a crucial parameter that defines the point at which the filter attenuates or reduces the amplitude of higher-frequency components in the signal. Specifically, it is the frequency at which the filter's response is -3 dB relative to its maximum amplitude. \n",
    "\n",
    "To ensure proper normalization and accurate filtering, we also calculate the [Nyquist frequency](https://en.wikipedia.org/wiki/Nyquist_frequency) ($f_{\\text{Nyq}}$). The Nyquist frequency is equal to half of the sampling frequency ($f_{\\text{sample}}$). It plays a fundamental role in signal processing, as it represents the maximum frequency that can be accurately represented or measured in the discrete time domain. \n",
    "\n",
    "In our case, $f_{\\text{sample}}$ represents the sampling frequency of our signal, which is 50 kHz. Thus, by setting the cutoff frequency and understanding the Nyquist frequency, we ensure that our filtering process operates effectively and aligns with the capabilities of our sampling system.\n",
    "\n",
    "Subsequently, we apply this filter to the original signal in the time domain to obtain the filtered signal. This process is mathematically represented as follows:\n",
    "\n",
    "- Time Domain:\n",
    "  $ f_{\\text{filtered}}(t) = LP(\\omega) * f(t) = \\int_{-\\infty}^{\\infty} LP(\\tau) \\cdot f(t-\\tau) \\, d\\tau $\n",
    "\n",
    "- Frequency Domain:\n",
    "  $ F_{\\text{filtered}}(\\omega) = LP(\\omega) * F(\\omega) = LP(\\omega) \\cdot F(\\omega) $\n",
    "\n",
    "In other words, in the time domain, the filtered signal is obtained by convolving the input signal with the filter's impulse response.\n",
    "\n",
    "Finally, the FFT of the filtered signal is computed to analyze the frequency components of the filtered signal. This process enables us to understand the spectral content of the signal after applying the low-pass filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_fft = fft(signal)\n",
    "freq_fft = fftfreq(len(signal), 1.0 / SAMPLE_FREQ)\n",
    "\n",
    "# Low-pass filter (numeric)\n",
    "order = 10  # filter order\n",
    "cutoff = (\n",
    "    5e3  # cutoff frequency (i.e. the frequency at which the signal is reduced by 3 dB)\n",
    ")\n",
    "nyq = 0.5 * SAMPLE_FREQ  # Nyquist frequency\n",
    "b, a = butter(order, cutoff / nyq, btype=\"low\", analog=False)\n",
    "# apply the filter\n",
    "signal_filtred = lfilter(b, a, signal)  # time space\n",
    "signal_filtred_fft = fft(signal_filtred)  # frequency space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we plot the result to visualize the protocol and its result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 6))\n",
    "\n",
    "# data for the plot\n",
    "filter_freq, filter_amplitude = freqz(b, a, worN=8000)\n",
    "N = len(t)\n",
    "\n",
    "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    ax1.plot(\n",
    "        0.5 * SAMPLE_FREQ * filter_freq / np.pi,\n",
    "        np.abs(filter_amplitude),\n",
    "        label=\"cutoff\",\n",
    "    )\n",
    "    ax1.axvline(cutoff, color=\"r\", ls=\"--\", linewidth=0.75, label=\"cutoff\")\n",
    "    ax2.axvline(cutoff, color=\"r\", ls=\"--\", linewidth=0.75, label=\"cutoff\")\n",
    "    # plot half of the trransform which is symmetrical\n",
    "    ax2.plot(\n",
    "        freq_fft[: N // 2],\n",
    "        2.0 / N * np.abs(signal_fft)[: N // 2],\n",
    "        label=\"signal with noise\",\n",
    "    )\n",
    "    ax2.plot(\n",
    "        freq_fft[: N // 2],\n",
    "        2.0 / N * np.abs(signal_filtred_fft)[: N // 2],\n",
    "        label=\"signal flitred\",\n",
    "    )\n",
    "    ax2.set_xlabel(\"Frequency [Hz]\")\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.legend()\n",
    "        ax.set_ylabel(\"Intesity [-]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we visualize the result in the time domain to observe our measurements after applying the numerical filter through convolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 3.5))\n",
    "\n",
    "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(t, signal, alpha=0.5, label=\"signal with noise\")\n",
    "    ax.plot(t, wave, label=\"source signal\", color=\"k\")\n",
    "    ax.plot(t, signal_filtred, label=\"filtred signal\")\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"Amplitude [-]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Estimating π using Monte Carlo Simulation\n",
    "\n",
    "In this second example, we will explore a classical problem in computational mathematics: estimating the value of $\\pi$ using a [Monte Carlo method](https://en.wikipedia.org/wiki/Monte_Carlo_method). \n",
    "\n",
    "The Monte Carlo method is a powerful technique for solving problems through random sampling, and it can be applied to various mathematical and scientific challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "We want to estimate the value of $\\pi$, which is the fundamental, and well know, mathematical constant representing the ratio of a circle's circumference to its diameter. Mathematically, it's defined as:\n",
    "\n",
    "$$ \\pi = \\frac{C}{d} $$\n",
    "\n",
    "Where:\n",
    "- $ \\pi $ is the value we seek to estimate.\n",
    "- $ C $ stands for the circumference of a circle.\n",
    "- $ d $ represents the diameter of the same circle.\n",
    "\n",
    "However, an interesting relationship comes into play: the area of a unit circle (a circle with a radius of $r = 1$) is equal to $\\pi$. As a result, if we suppose a random event where we are dropping a coin onto a square tile with a side length of 1, which inscribes a quarter of a circle, the probability that the coin lands inside the circle is the ratio of the areas involved.\n",
    "\n",
    "$$\\rho = \\frac{\\text{Number of coins inside the circle}}{\\text{Total number of coins}} = \\frac{A_{\\text{circle}}}{A_{\\text{square}}} = \\frac{\\frac{\\pi}{4}}{1} = \\frac{\\pi}{4} $$\n",
    "\n",
    "This realization allows us to estimate the value of $\\pi$ by understanding the probability of a coin falling inside the circle. Hence, the estimation of $\\pi$ is calculated as:\n",
    "\n",
    "$$ \\pi = 4 \\cdot \\rho $$\n",
    "\n",
    "The Monte Carlo method, in an exceedingly simplified form, is utilized for estimating the probability density function of an event. It does this by randomly sampling the state space of a system and implementing an acceptance criterion. In our case, the acceptance criterion is that a point (or coin) falls within the circle, which can be determined by checking if $x^2 + y^2 \\le 1 $.\n",
    "\n",
    "Please note how this problem encapsulates the beauty of Monte Carlo simulations in estimating \"complex\" mathematical constants by leveraging the principles of probability and geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "In this code section, we import necessary libraries for numerical signal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # import numpy\n",
    "from matplotlib import pyplot as plt  # import the plotting library\n",
    "\n",
    "plt.style.use(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "iterations = 300000\n",
    "\n",
    "# Initialize variables to store convergence data\n",
    "points_in = np.zeros(iterations, dtype=\"uint\")\n",
    "points_tot = np.zeros(iterations, dtype=\"uint\")\n",
    "i_in = np.zeros(iterations, dtype=\"bool\")\n",
    "x = np.zeros(iterations)\n",
    "y = np.zeros(iterations)\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Generate random points in the square\n",
    "    x[i], y[i] = np.random.uniform(0, 1, size=2)\n",
    "\n",
    "    # Calculate the distance from the origin for each point\n",
    "    r = np.sqrt(x[i] ** 2 + y[i] ** 2)\n",
    "\n",
    "    # Acceptance criteria: Check if points are inside the quarter circle\n",
    "    if r <= 1.0:\n",
    "        points_in[i] = points_in[i - 1] + 1\n",
    "        i_in[i] = True\n",
    "    else:\n",
    "        points_in[i] = points_in[i - 1]\n",
    "        i_in[i] = False\n",
    "    points_tot[i] = points_tot[i - 1] + 1\n",
    "\n",
    "# Calculate the estimated value of π\n",
    "rho = points_in / points_tot  # probability\n",
    "pi_estimate = 4 * rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x[i_in], y[i_in], s=8, alpha=0.75, label=\"accepted\")\n",
    "    ax.scatter(\n",
    "        x[np.logical_not(i_in)],\n",
    "        y[np.logical_not(i_in)],\n",
    "        s=8,\n",
    "        alpha=0.75,\n",
    "        label=\"rejected\",\n",
    "    )\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1.1, 0.5))\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlabel(\"x [-]\")\n",
    "    ax.set_ylabel(\"y [-]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How it converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 3.5))\n",
    "\n",
    "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax_twin = ax.twinx()\n",
    "    p1 = ax.plot(pi_estimate)\n",
    "    p2 = ax_twin.plot(np.abs(pi_estimate - np.pi), color=\"#da3e46\")\n",
    "    p3 = ax.axhline(np.pi, color=\"k\", ls=\"--\")\n",
    "    ax.legend(p1 + p2 + [p3], [r\"$4\\rho$ (MC)\", r\"error\", r\"$\\pi$\"], loc=\"upper right\")\n",
    "    ax.set_xlabel(\"Iteratinos [-]\")\n",
    "    ax.set_ylabel(\"y [-]\")\n",
    "    ax.set_ylim([3.12, 3.16])\n",
    "    ax_twin.set_ylabel(r\"Errors $ | \\pi-4\\rho |$ [-]\")\n",
    "    ax_twin.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Solving 1D Heat Diffusion \n",
    "\n",
    "In this example, we will explore the numerical solution of the one-dimensional heat diffusion equation using the explicit Euler method. \n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Imagine placing two metal rods with different temperatures in contact. After they come into contact, heat diffuses from the hot rod to the cold one, and its evolution over time can be described by the [parabolic](https://en.wikipedia.org/wiki/Parabolic_partial_differential_equation) partial differential equation (PDE) known as the [*heat equation*](https://en.wikipedia.org/wiki/Heat_equation). Due to the symmetry of the problem, we can simplify the PDE from three dimensions to one dimension. Therefore, the 1D heat equation is given by:\n",
    "\n",
    "$$\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2}$$\n",
    "\n",
    "Where:\n",
    "- $u(x, t)$ is the temperature at position $x$ and time $t$.\n",
    "- $\\alpha$ is the thermal diffusivity of the material.\n",
    "\n",
    "### Discretization\n",
    "\n",
    "To numerically solve the heat diffusion equation, we discretize time into time steps ($\\Delta t$) and space into grid points ($\\Delta x$). This results in a grid of temperature values at various positions and times.\n",
    "\n",
    "Let's represent the temperature at the $i$-th spatial grid point and the $n$-th time step as $u_i^n$. The heat diffusion equation can be discretized as follows:\n",
    "\n",
    "$$\\frac{u_i^{n+1} - u_i^n}{\\Delta t} = \\alpha \\frac{u_{i+1}^n - 2u_i^n + u_{i-1}^n}{\\Delta x^2}$$\n",
    "\n",
    "This discretized equation allows us to update the temperature at each grid point in the next time step using the values at the current time step.\n",
    "\n",
    "### Explicit Euler Method\n",
    "\n",
    "The explicit [Euler method](https://en.wikipedia.org/wiki/Euler_method) is a straightforward yet effective way to solve time-dependent partial differential equations, such as the heat diffusion equation. It updates the temperature at each grid point using information from the previous time step and is derived from the first-order finite difference of the time derivative.\n",
    "\n",
    "The update formula for the explicit Euler method is:\n",
    "\n",
    "$$u_i^{n+1} = u_i^n + \\alpha \\frac{\\Delta t}{\\Delta x^2} (u_{i+1}^n - 2u_i^n + u_{i-1}^n)$$\n",
    "\n",
    "By repeatedly applying this formula for all grid points, we can approximate the temperature distribution over time.\n",
    "\n",
    "### Well-define problem\n",
    "\n",
    "To ensure the uniqueness of the solution, we must apply boundary conditions (BC). Because it is a parabolic PDE we need one condition for the time domain and two for the space domain:\n",
    "\n",
    "* Initial condition: $u(t=0) = u_0$\n",
    "* Boundary conditions: $u(x=0) = T_{hot}, \\, u(x=L) = T_{cold}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define parameters\n",
    "alpha = 97e-6  # [m^2/s] Thermal diffusivity (aluminum)\n",
    "L = 1.0  # [m] Length of the rod\n",
    "T = 60.0  # [s] Total simulation time\n",
    "\n",
    "# Discretization\n",
    "Nx = 80  # Number of spatial grid points\n",
    "dx = L / Nx\n",
    "dt = 5e-3  # [s] Time step\n",
    "Nt = int(T / dt)  # Number of time step\n",
    "\n",
    "# Initialize temperature array\n",
    "u = np.zeros(Nx + 1)\n",
    "\n",
    "# Initial condition: temperature distribution at t=0\n",
    "u[: Nx // 2] = 400  # [K] hot rod\n",
    "u[Nx // 2 :] = 200  # [K] cold rod\n",
    "\n",
    "# We initialize a vector for storing the profile and plots\n",
    "u_t = np.zeros((4, Nx + 1))\n",
    "t = np.zeros(4)\n",
    "\n",
    "# Time-stepping with explicit Euler\n",
    "n_save = 0\n",
    "for n in range(Nt):\n",
    "    if n % (Nt // 4) == 0:\n",
    "        u_t[n_save, :] = u\n",
    "        t[n_save] = dt * n\n",
    "        n_save += 1\n",
    "    for i in range(1, Nx):\n",
    "        u[i] = u[i] + alpha * dt / dx**2 * (u[i + 1] - 2 * u[i] + u[i - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's, as usual, plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 3.5))\n",
    "x = np.linspace(0, L, Nx + 1)\n",
    "ls = [\":\", \"-.\", \"--\", \"-\"]\n",
    "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
    "    ax = fig.add_subplot(111)\n",
    "    for i in range(4):\n",
    "        ax.plot(x, u_t[i], ls=ls[i], color=\"k\", label=f\"$t=${t[i]:1.1f} s\")\n",
    "    ax.plot(x, u, label=f\"$t=${dt*Nt:1.1f} s\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Temperature Evolution\")\n",
    "    ax.set_xlabel(\"Position (x)\")\n",
    "    ax.set_ylabel(\"Temperature (u)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Time integration of a dynamic system\n",
    "\n",
    "In this example, we will explore the time evolution of a dynamic system, specifically a spring oscillation under the influence of a gravitational field.\n",
    "We will formulate the equation of motion, split it into a system of first-order ordinary differential equations (ODEs) to solve in the phase space, and integrate it using the implicit Euler method. We will consider an ideal spring, so energy is conserved during the oscillation.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The equation of motion for a spring oscillation under the influence of gravity is a second-order ODE:\n",
    "\n",
    "$$m \\dfrac{d^2x}{dt^2} = -kx - mg$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the mass of the object.\n",
    "- $k$ is the spring constant.\n",
    "- $x$ is the displacement from the equilibrium position.\n",
    "- $g$ is the acceleration due to gravity.\n",
    "\n",
    "To solve this equation, we'll first split it into a system of first-order ODEs in the phase space:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\dfrac{dx}{dt} = v \\\\\n",
    "    \\dfrac{dv}{dt} = -\\frac{k}{m}x - g\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "We will then integrate this system using the implicit Euler method.\n",
    "\n",
    "As before, Let's implement the explicit Euler method in Python to solve the system of ODEs and visualize the spring's oscillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# System parameters\n",
    "m = 1.0  # Mass\n",
    "k = 1.0  # Spring constant\n",
    "g = 9.81  # Acceleration due to gravity\n",
    "\n",
    "# Initial conditions\n",
    "x0 = 1.0  # Initial displacement\n",
    "v0 = 0.0  # Initial velocity\n",
    "\n",
    "# Time parameters\n",
    "T = 2 * np.pi  # Total simulation time (3 periods)\n",
    "dt = 0.02  # Time step\n",
    "\n",
    "# Number of time steps\n",
    "num_steps = int(T / dt)\n",
    "\n",
    "# Initialize arrays to store results\n",
    "time = np.zeros(num_steps)\n",
    "displacement = np.zeros(num_steps)\n",
    "velocity = np.zeros(num_steps)\n",
    "\n",
    "# Set initial conditions for explicit Euler\n",
    "displacement[0] = x0\n",
    "velocity[0] = v0\n",
    "\n",
    "# Integrate the system using explicit Euler\n",
    "for i in range(1, num_steps):\n",
    "    # Compute acceleration at the current position\n",
    "    acceleration = -k / m * displacement[i - 1] - g\n",
    "\n",
    "    # Update the velocity\n",
    "    velocity[i] = velocity[i - 1] + acceleration * dt\n",
    "\n",
    "    # Update the displacement\n",
    "    displacement[i] = displacement[i - 1] + velocity[i - 1] * dt\n",
    "\n",
    "    # Update time\n",
    "    time[i] = time[i - 1] + dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the system evolution on the Phase Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "x = np.linspace(0, L, Nx + 1)\n",
    "ls = [\":\", \"-.\", \"--\", \"-\"]\n",
    "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
    "    ax = fig.add_subplot(111)\n",
    "    sp = ax.scatter(displacement, velocity, c=time)\n",
    "    plt.colorbar(sp, label=\"Time [s]\")\n",
    "    ax.set_xlabel(\"Displacement $x$ [m]\")\n",
    "    ax.set_ylabel(\"Velocity $v$ [m/s]\")\n",
    "    ax.set_title(\"Phase space\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the Energy Conservation with Verlet Integration\n",
    "\n",
    "Now, let's explore how to use the Verlet integration method to simulate the spring oscillation while conserving energy. The Verlet method is a symplectic numerical integration technique that can be particularly useful for dynamic systems with conserved energy, like ideal springs.\n",
    "\n",
    "#### Verlet Integration Method\n",
    "\n",
    "The Verlet method is based on the position and velocity Verlet algorithm, which provides a better conservation of energy compared to implicit methods like the Euler method. It updates the position and velocity of the system based on the following formulas:\n",
    "\n",
    "- Update the position: $$x_{n+1} = x_n + v_n \\Delta t + \\frac{1}{2} a_n \\Delta t^2$$\n",
    "- Calculate the acceleration: $$a_{n+1} = -\\frac{k}{m}x_{n+1} - g$$\n",
    "- Update the velocity: $$v_{n+1} = v_n + \\frac{1}{2}(a_n + a_{n+1})\\Delta t$$\n",
    "\n",
    "The Verlet method combines both position and velocity updates, resulting in more accurate and energy-conserving simulations.\n",
    "\n",
    "#### Implementation in Python (Verlet Integration)\n",
    "\n",
    "Let's implement the Verlet integration method in Python to simulate the spring oscillation. We will use the same parameters and initial conditions as in the previous example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store results\n",
    "displacement_verlet = np.zeros(num_steps)\n",
    "velocity_verlet = np.zeros(num_steps)\n",
    "\n",
    "# Set initial conditions\n",
    "displacement_verlet[0] = x0\n",
    "velocity_verlet[0] = v0\n",
    "\n",
    "# Integrate the system using Verlet integration\n",
    "for i in range(1, num_steps):\n",
    "    # Update the position\n",
    "    displacement_verlet[i] = (\n",
    "        displacement_verlet[i - 1]\n",
    "        + velocity_verlet[i - 1] * dt\n",
    "        + 0.5 * (-k / m * displacement_verlet[i - 1] - g) * dt**2\n",
    "    )\n",
    "\n",
    "    # Calculate the acceleration\n",
    "    acceleration = -k / m * displacement_verlet[i]\n",
    "\n",
    "    # Update the velocity\n",
    "    velocity_verlet[i] = (\n",
    "        velocity_verlet[i - 1]\n",
    "        + 0.5 * (acceleration + (-k / m * displacement_verlet[i] - g)) * dt\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "x = np.linspace(0, L, Nx + 1)\n",
    "ls = [\":\", \"-.\", \"--\", \"-\"]\n",
    "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
    "    ax = fig.add_subplot(111)\n",
    "    sp = ax.scatter(displacement_verlet, velocity_verlet, c=time)\n",
    "    plt.colorbar(sp, label=\"Time [s]\")\n",
    "    ax.set_xlabel(\"Displacement $x$ [m]\")\n",
    "    ax.set_ylabel(\"Velocity $v$ [m/s]\")\n",
    "    ax.set_title(\"Phase space\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Unsupervised Clustering for Iris Species Recognition\n",
    "\n",
    "The exercise involves using an unsupervised clustering method to automatically recognize the species of *Iris* solely based on the lengths and widths of the petals and sepals.\n",
    "\n",
    "To do this, we need to:\n",
    "\n",
    "0. Install `scikit-learn`.\n",
    "1. Download and import the dataset (`iris.csv`).\n",
    "2. Take an initial look at the data and determine if any data cleaning is necessary.\n",
    "3. Normalize the values (see [Normalization](https://en.wikipedia.org/wiki/Normalization_(statistics))).\n",
    "4. Reduce the dimensionality using [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
    "5. Use the unsupervised clustering method, *Gaussian Mixture* (see [Gaussian Mixture Models Explained](https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95)).\n",
    "\n",
    "![The three different Iris species](https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/assets/img/iris.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Install the Machine Learning library `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn==1.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the Database `iris.csv`\n",
    "\n",
    "We will download the `iris.csv` database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/data/lab1/iris.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the Database Using `pandas`\n",
    "\n",
    "Let's import the database using the `read_csv` method from `pandas`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "\n",
    "csv = Comma-Separated Values\n",
    "<br>\n",
    "It is a common file format for saving data, readable by both humans and machines.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # importo numpy\n",
    "import pandas as pd  # importo pandas\n",
    "from matplotlib import pyplot as plt  # importo la libreria per plottare\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "iris_data = pd.read_csv(\"iris.csv\")\n",
    "iris_data[\"species\"] = iris_data[\"species\"].astype(\"category\")\n",
    "iris_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Take an Initial Look at the Data and Check for Cleaning Requirements\n",
    "\n",
    "One of the first things to do is to perform a quick data analysis and visualization. A useful metric to examine is the correlation between the different data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(iris_data.corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_corr = iris_data.corr(numeric_only=True)  # Pearson correlation coefficient\n",
    "# vd. https://it.wikipedia.org/wiki/Indice_di_correlazione_di_Pearson\n",
    "iris_corr.style.background_gradient(cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "\n",
    "data_axis = [\"petal_width\", \"petal_length\"]  # prova a cambiarli\n",
    "\n",
    "with plt.style.context(\"seaborn\"):\n",
    "    ax = fig.add_subplot(111)\n",
    "    for label in iris_data[\"species\"].unique():\n",
    "        sub_set = iris_data.loc[\n",
    "            iris_data[\"species\"] == label\n",
    "        ]  # plotto ogni specie con colori diversi\n",
    "        ax.scatter(sub_set[data_axis[0]], sub_set[data_axis[1]], label=label)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(data_axis[0])\n",
    "    ax.set_ylabel(data_axis[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normalize the Values\n",
    "\n",
    "There are various methodologies for normalization, with the main ones being:\n",
    "\n",
    "| Method | Formula    |\n",
    "| --- | --- |\n",
    "| Standardization | $ \\dfrac{X-\\mu }{\\sigma }$ |\n",
    "| Student's t | $ \\dfrac{\\widehat{\\beta }-\\beta _{0}}{\\text{s.e.} ({\\widehat {\\beta }})}$ |\n",
    "| Min-max | $\\dfrac{X - X_{\\min }}{X_{\\max }- X_{\\min }}$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_raw = iris_data.iloc[:, :-1].values.astype(float)  # separiamo i dati...\n",
    "Y_raw = iris_data.iloc[:, -1].values.astype(str)  # ...dalle etichette\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X = normalizer.fit_transform(X_raw)\n",
    "\n",
    "# se tutto e andato bene la media dovrebbe essere 0 e la deviazione standard 1\n",
    "print(\"mean: \", X.mean(axis=0))\n",
    "print(\"std: \", X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Reduce Dimensionality with Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality reduction technique that allows us to represent the data in a lower-dimensional space while retaining most of the variance. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)  # Prova a cambiare il numero di componenti (il massimo è 4)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "data_axis = [\"petal_width\", \"petal_length\"]  # prova a cambiarli\n",
    "\n",
    "with plt.style.context(\"seaborn\"):\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    for label in iris_data[\"species\"].unique():\n",
    "        indexs = np.where(Y_raw == label)[0]\n",
    "        sub_set = iris_data.loc[\n",
    "            iris_data[\"species\"] == label\n",
    "        ]  # plotto ogni specie con colori diversi\n",
    "        ax1.scatter(sub_set[data_axis[0]], sub_set[data_axis[1]], label=label)\n",
    "        ax2.scatter(X_pca[indexs, 0], X_pca[indexs, 1], label=label)\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel(data_axis[0])\n",
    "    ax1.set_ylabel(data_axis[1])\n",
    "    ax2.set_xlabel(\"1st component\")\n",
    "    ax2.set_ylabel(\"2nd component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Use the Unsupervised Clustering Method `GaussianMixture`\n",
    "\n",
    "In this step, we'll use the unsupervised clustering method `GaussianMixture`. This algorithm assumes that the data in each cluster follows a multidimensional Gaussian probability distribution. The algorithm takes the number of desired clusters, `n_components`, and estimates the parameters ($\\mu$, $\\sigma$) that maximize the [Maximum Likelihood Estimate (MLE)](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n",
    "\n",
    "![EM Clustering](https://upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif)\n",
    "\n",
    "This visualization shows the Expectation-Maximization (EM) Clustering process, which is commonly used in Gaussian Mixture Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "cluster_model = GaussianMixture(\n",
    "    n_components=3,  # Il numero di `Mixture` cioè di cluster da cercare\n",
    "    covariance_type=\"full\",  # Stima/calcolo della matrice di covarianza\n",
    ")\n",
    "# training\n",
    "cluster_model.fit(X_pca)\n",
    "Y_prediction = cluster_model.predict(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Clustering Results\n",
    "\n",
    "Let's evaluate the clustering results, initially qualitatively, by plotting the original clusters versus those predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "data_axis = [\"petal_width\", \"petal_length\"]  # prova a cambiarli\n",
    "\n",
    "with plt.style.context(\"seaborn\"):\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    for label in iris_data[\"species\"].unique():\n",
    "        indexs = np.where(Y_raw == label)[0]\n",
    "        sub_set = iris_data.loc[\n",
    "            iris_data[\"species\"] == label\n",
    "        ]  # plotto ogni specie con colori diversi\n",
    "        ax1.scatter(X_pca[indexs, 0], X_pca[indexs, 1], label=label)\n",
    "    for clust_id in np.unique(Y_prediction):\n",
    "        indexs = np.where(Y_prediction == clust_id)[0]\n",
    "        ax2.scatter(X_pca[indexs, 0], X_pca[indexs, 1], label=f\"cluster {clust_id}\")\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"1st component\")\n",
    "        ax.set_ylabel(\"2nd component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then Evaluate the Model Quantitatively\n",
    "\n",
    "Next, we will evaluate the model quantitatively. To do this, we need to use a metric that measures the quality of our model. For clustering algorithms, we should use metrics that are independent of label permutations, such as `labels` (`Y_raw`, `Y_prediction`).\n",
    "\n",
    "In this exercise, we will use a relatively common metric, the **Mutual Information Score**. It's defined as:\n",
    "\n",
    "$$\n",
    "MI(U,V) = \n",
    "\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N} \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NB:</b> \n",
    "\n",
    "Notice how the information measure $I = |A|\\log(|A|) = p_{A}\\log(p_{A})$ closely resembles the definition of entropy in statistical mechanics used in Gibbs' formulation: $S = k_{B} \\sum_i p_{i}\\log(p_{i})$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "mi_score = mutual_info_score(Y_raw, Y_prediction)\n",
    "print(f\"Mutual Information Score = {mi_score*100:1.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
