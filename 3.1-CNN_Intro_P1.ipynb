{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDFpteRcP_aF"
   },
   "source": [
    "# Convolutional Neural Network model - an introduction (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqKIxFghP_aL"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paolodeangelis/Sistemi_a_combustione/blob/main/3.1-CNN_Intro_P1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ez_1bo8gGCz5"
   },
   "source": [
    "## Enabling and testing the GPU\n",
    "\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to `Edit` > `Notebook Settings`\n",
    "- Select `T4 GPU` from the Hardware Accelerator drop-down\n",
    "- If the GPU nodes are busy, use the CPU one.\n",
    "\n",
    "Next, we'll check that we can connect to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roYngX96GfoT",
    "outputId": "1ba16baf-af1c-4e41-d95e-82f35fadb9d1"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "device_name = tf.test.gpu_device_name()  # GPU detection\n",
    "if device_name == \"/device:GPU:0\":\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    print(f\"Running on GPU at: {device_name}\")\n",
    "else:\n",
    "    warnings.warn(\"GPU device not found\")\n",
    "    try:\n",
    "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "            tpu=\"\"\n",
    "        )  # TPU detection\n",
    "        tf.config.experimental_connect_to_cluster(resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "        strategy = tf.distribute.TPUStrategy(resolver)\n",
    "        print(\n",
    "            \"Running on TPU at:\",\n",
    "            \"\\n\\t\".join([f\"{i}\" for i in tf.config.list_logical_devices(\"TPU\")]),\n",
    "        )\n",
    "    except ValueError:\n",
    "        warnings.warn(\"TPU device not found\")\n",
    "        warnings.warn(\"Default parallization strategy will be used\")\n",
    "        strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HODMa97DvP2"
   },
   "source": [
    "## Installing Libraries\n",
    "\n",
    "We begin by installing the necessary libraries to support our data manipulation, visualization, and deep learning modeling. (Note: `Tensorflow` and `Keras` are already installed on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L9lFAQhVEClk",
    "outputId": "286d7068-f7d2-4e48-8852-146eceb282dd"
   },
   "outputs": [],
   "source": [
    "%pip install numpy pandas scipy matplotlib  scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCLXim7CrLiU"
   },
   "source": [
    "And now we import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvGzVOkNrMqV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Data visualization library\n",
    "import numpy as np  # Numerical computing library\n",
    "import pandas as pd  # Data manipulation and analysis library\n",
    "import tensorflow as tf  # Deep learning framework for neural networks\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras import layers, losses, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM7_x9BdZF0I"
   },
   "source": [
    "## Data download and downsamplig\n",
    "\n",
    "In this section, we will download the dataset MNIST (Modified database National Institute of Standards and Technology) is a collection of 70.000 handwritten 10-digit images, downsampled in size (28 × 28 pixels), in black and white and therefore with only one color channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ERVXxSJZF0K"
   },
   "source": [
    "### Download dataset files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Xb3rJW-Y30Q"
   },
   "source": [
    "Then we load the database. Because it is a standard database (a sort of *Hello World* for CNN), it is already included in TF, so we only need to run the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTVortQRZF0M"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7K4qjJlAZSLV"
   },
   "source": [
    "#### Digital *raster* image\n",
    "\n",
    "Wait? *Pixel*, *downsamplig*, color channels... but what is a digital image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBg4Tun5oNj_"
   },
   "source": [
    "Raster images are a crucial element in digital graphics, such as those captured by digital cameras or smartphones. Essentially, raster images consist of pixels, each being a small square that represents a specific color. These pixels, when organized in a grid, create a complete image.\n",
    "\n",
    "From a more mathematical perspective, a raster image can be viewed as a matrix, or more accurately, a tensor. In this structure, each element stores the color(-channel) value of a corresponding pixel.\n",
    "\n",
    "Indeed we can manipulate the image as a `numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "um4FybugkyhO",
    "outputId": "66b01832-6e32-4298-a155-b71357eeab72"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/paolodeangelis/Sistemi_a_combustione/main/assets/img/lena.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE3ELpuxxVsZ"
   },
   "source": [
    "Let's load an image. This can be done using the `PIL` library, which supports a wide range of image formats. Once loaded, the image can be displayed using Matplotlib.\n",
    "\n",
    "> ℹ️ : This image is not exactly random; it's called *Lena* and has been commonly used in journal articles related to image processing and analysis. If you are curious, you might want to check out the following [video](https://www.youtube.com/watch?v=yCdwm2vo09I)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "kwMtr1vfrjMT",
    "outputId": "3ae0fc31-792f-4f9d-86ff-15d6c2dd5e93"
   },
   "outputs": [],
   "source": [
    "# Load an image\n",
    "image = Image.open(\"lena.png\")\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(\"Loaded Image\")\n",
    "    ax.set_xlabel(\"$j$ pixel index\")\n",
    "    ax.set_ylabel(\"$i$ pixel index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhxHW5MqzDW5"
   },
   "source": [
    "In Python, a raster image is represented as a multi-dimensional NumPy array. Each element of this array corresponds to a pixel in the image. The shape of the array provides information about the image's dimensions and color channels. For instance, an image with the shape (height, width, 3) indicates a typical RGB image with three color channels. In our case we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFvJ9vBykyed",
    "outputId": "7c0ea2c8-1aa4-42f4-96d3-9e4c68ef71a4"
   },
   "outputs": [],
   "source": [
    "image_array = np.array(image)\n",
    "print(f\"Shape of the image array: {image_array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LklqopEezlVq"
   },
   "source": [
    "Each pixel in a raster image is typically represented by a combination of color values. In a **24-bit RGB** color scheme, each color channel (Red, Green, Blue) is represented by 8 bits, allowing for $2^8 = 256$ levels of intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "62NpVXS1kybx",
    "outputId": "5540f603-0463-4c59-b37b-9658450ad19d"
   },
   "outputs": [],
   "source": [
    "# Separate the channels\n",
    "red_channel = image_array[:, :, 0]\n",
    "green_channel = image_array[:, :, 1]\n",
    "blue_channel = image_array[:, :, 2]\n",
    "\n",
    "# Display each channel\n",
    "fig = plt.figure(figsize=(5 * 3, 5))\n",
    "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
    "    ax = fig.add_subplot(131)\n",
    "    ax.imshow(red_channel, cmap=\"Reds\", vmax=255)\n",
    "    ax.set_title(\"Red Channel\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    ax = fig.add_subplot(132)\n",
    "    ax.imshow(green_channel, cmap=\"Greens\", vmax=255)\n",
    "    ax.set_title(\"Green Channel\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    ax = fig.add_subplot(133)\n",
    "    ax.imshow(blue_channel, cmap=\"Blues\", vmax=255)\n",
    "    ax.set_title(\"Blue Channel\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339,
     "referenced_widgets": [
      "d4699e4cb0124b1dbb883e45c33fd00d",
      "72dd7b2b295d4dd2ac32f9878ac2e7d5",
      "2c66241d3565493991f9d4384146a257",
      "48d8c9866abb40aab7bcf30c8418b3e3",
      "4c93d932c724460ea366f058696bf3e6",
      "fc2eec63d71f4876abfd6e20d27b71c1",
      "17513c9ae3474c91bf0f4dc4db57c294",
      "b85ca2d1578e44a5b8545a6a411a2357",
      "7249109fcd5f4c7889341009ea33a06e",
      "d1db5d245ddc43a8af1df2325373ece8",
      "79cdad440b374e6ca42f388100b69a9f"
     ]
    },
    "id": "LpKiQSxJkyZC",
    "outputId": "e7e0082e-ecea-48e5-c9f5-4c04bb1d33da"
   },
   "outputs": [],
   "source": [
    "# @title RGB 24-bit widget\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# Function to convert RGB to HEX\n",
    "\n",
    "\n",
    "def rgb_to_hex(r, g, b):\n",
    "    return f\"#{r:02x}{g:02x}{b:02x}\"\n",
    "\n",
    "\n",
    "# Function to display the color and its values\n",
    "\n",
    "\n",
    "def display_color(change):\n",
    "    clear_output(wait=True)\n",
    "    display(widget_box)  # Redisplay the widgets\n",
    "    r, g, b = slider_r.value, slider_g.value, slider_b.value\n",
    "    hex_color = rgb_to_hex(r, g, b)\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(np.array([[[r / 255, g / 255, b / 255]]]))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print(f\"RGB: {r}, {g}, {b}\")\n",
    "    print(f\"BIN: {r:08b}{g:08b}{b:08b}\")\n",
    "    print(f\"HEX: {hex_color}\")\n",
    "\n",
    "\n",
    "# Creating RGB sliders\n",
    "slider_r = widgets.IntSlider(\n",
    "    value=0, min=0, max=255, description=\"Red:\", continuous_update=False\n",
    ")\n",
    "slider_g = widgets.IntSlider(\n",
    "    value=0, min=0, max=255, description=\"Green:\", continuous_update=False\n",
    ")\n",
    "slider_b = widgets.IntSlider(\n",
    "    value=0, min=0, max=255, description=\"Blue:\", continuous_update=False\n",
    ")\n",
    "\n",
    "# Observing the sliders\n",
    "slider_r.observe(display_color, names=\"value\")\n",
    "slider_g.observe(display_color, names=\"value\")\n",
    "slider_b.observe(display_color, names=\"value\")\n",
    "\n",
    "# Grouping the sliders for display\n",
    "widget_box = widgets.VBox([slider_r, slider_g, slider_b])\n",
    "\n",
    "# Displaying the widgets\n",
    "display(widget_box)\n",
    "\n",
    "# Initial display\n",
    "display_color(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0ngU0nd1RvZ"
   },
   "source": [
    "### Dataset ispection and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNhelE4RjCz2"
   },
   "source": [
    "Let’s look how many images we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_vDgDmAkyWk",
    "outputId": "267a0122-7542-410d-9336-e34823a7c02c"
   },
   "outputs": [],
   "source": [
    "print(f\"Train Images:{train_images.shape}\")\n",
    "print(f\"Train Labels:{len(train_labels)}\")\n",
    "print(f\"Test Images:{test_images.shape}\")\n",
    "print(f\"Test Labels:{len(test_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-uFK4IGjI7i"
   },
   "source": [
    "Thus we have 60k images for the training and 10k for the test (86%-14% split)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7hi35wvjEMb"
   },
   "source": [
    "Now let see what there is inside this database by randomly plotting 36 pictures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 826
    },
    "id": "-DExNUSNjEq1",
    "outputId": "98719765-9334-4085-a495-29eaac9986f3"
   },
   "outputs": [],
   "source": [
    "# set a proper figure dimension\n",
    "plt.figure(figsize=(10, 10))\n",
    "# pick 36 random digits in range 0-59999\n",
    "# inner bound is inclusive, outer bound exclusive\n",
    "random_inds = np.random.choice(60000, 36)\n",
    "for i in range(36):\n",
    "    plt.subplot(6, 6, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    image_ind = random_inds[i]\n",
    "    # show images using a binary color map (i.e. Black and White only)\n",
    "    plt.imshow(train_images[image_ind], cmap=plt.cm.binary)\n",
    "    # set the image label\n",
    "    plt.xlabel(train_labels[image_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKTd5YTKjjHN"
   },
   "source": [
    "### Pre-Processing\n",
    "\n",
    "As discussed in the previous lesson, neural networks perform better when using floating-point numbers that are within the interval $[0,1]$. Since our data is in RGB format and assumes integer values between 0 and 255, we perform the following normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55jHRc32jSei"
   },
   "outputs": [],
   "source": [
    "# from range 0-255 to 0-1\n",
    "train_images = (train_images / 255.0).astype(np.float32)\n",
    "test_images = (test_images / 255.0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nF1laWCLWj8T"
   },
   "source": [
    "*Label encoding*\n",
    "\n",
    "In classification tasks for neural networks, each label is transformed into a vector representing all possible classes, called **one-hot** or **one-vs-all encoding**.\n",
    "In the case of digit recognition (0 through 9), this means converting each digit into a 10-element vector.\n",
    "\n",
    "Here's a brief explanation of how this encoding works:\n",
    "\n",
    "Each digit is represented by a 10-element vector.\n",
    "All elements of the vector are initially set to 0.\n",
    "The position corresponding to the digit is set to 1, and the rest remain 0.\n",
    "For example:\n",
    "\n",
    "* The digit '0' is encoded as [1, 0, 0, 0, 0, 0, 0, 0, 0, 0].\n",
    "* The digit '1' is encoded as [0, 1, 0, 0, 0, 0, 0, 0, 0, 0].\n",
    "* And so on, up to the digit '9', which is encoded as [0, 0, 0, 0, 0, 0, 0, 0, 0, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpJB6-7eWnQw"
   },
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.classes_ = np.arange(10)\n",
    "\n",
    "train_labels_encoded = encoder.transform(train_labels)\n",
    "test_labels_encoded = encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7hjRMqRJaAr"
   },
   "source": [
    "## Model 1: Handwritten digit recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZAk29Ogk-vS"
   },
   "source": [
    "### First Try: Let's apply what we learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY6nSq02J2t7"
   },
   "source": [
    "Now it's time to build the model!\n",
    "We are going to use a keras sequential model (as usual) with three different layers. This model represents a feed-forward Fully Connected Neural Network (one that passes values from left to right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOToASwZjSaq"
   },
   "outputs": [],
   "source": [
    "def build_model_1(shape_img: tuple, num_class: int) -> tf.keras.models.Sequential:\n",
    "    \"\"\"\n",
    "    Build a the first possible architecture for our neural network model.\n",
    "    Args:\n",
    "        shape_img (tuple): The shape of the input images (height, width, channels).\n",
    "        num_class (int): The number of classes for the classification task.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.models.Sequential: A Keras Sequential model.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            # Convert 2D image data to 1D\n",
    "            tf.keras.layers.Flatten(input_shape=shape_img, name=\"flattening_layer\"),\n",
    "            # Dense hidden layer with ReLU activation\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\", name=\"hidden_layer_1\"),\n",
    "            # Output layer for classification\n",
    "            tf.keras.layers.Dense(num_class, activation=\"softmax\", name=\"output_layer\"),\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHGldjy6MTpU"
   },
   "source": [
    "Let's call the function to build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akIRshX4Mi_0"
   },
   "outputs": [],
   "source": [
    "model_1 = build_model_1(\n",
    "    train_images.shape[1:],  # image shape\n",
    "    10,  # number of digits (i.e 0, 1, 2, 3, 4, 5, 6 ,7 , 8, 9)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9wwk7QIMVU0"
   },
   "source": [
    "#### Model Summary and Structure Visualization\n",
    "\n",
    "Let's examine the summary of our compiled model and visualize its architecture.\n",
    "\n",
    "**Layer 1 (input)**: This is our input layer and it consists of 784 neurons. We use the flatten layer with an input shape of $(28, 28, 1)$ to denote that our input should come in that shape. The flattening means that our layer will reshape the $(28, 28, 1)$ array into a vector of 784 neurons, so that each pixel will be associated with one neuron. Note: This is the input layer, so 0 parameters have to be trained.\n",
    "\n",
    "**Layer 2 (hidden)**: This is our first and only hidden layer. The term *dense* denotes that this layer will be fully connected, with each neuron from the previous layer connecting to each neuron of this layer. It has 128 neurons and uses the rectified linear unit (ReLU) activation function.\n",
    "\n",
    "- Number of parameters in the first hidden layer weights ($\\mathbf{w}_{h1}$): $784 \\times 128 = 100352$\n",
    "- Number of parameters in the first hidden layer biases ($\\mathbf{b}_{h1}$): $1 \\times 128 = 128$\n",
    "\n",
    "**Layer 3 (output)**: This is our output layer and is also a dense layer. It has 10 neurons that we will look at to determine our model's output. Each neuron represents the probability of a given image belonging to one of the 10 different classes. The activation function *softmax* is used on this layer to calculate a probability distribution for each class. This means the value of any neuron in this layer will be between 0 and 1, where 1 represents a high probability of the image being in that class.\n",
    "\n",
    "- Number of parameters in the output layer weights ($\\mathbf{w}_{o}$): $128 \\times 10 = 1280$\n",
    "- Number of parameters in the output layer biases ($\\mathbf{b}_{o}$): $10 \\times 1 = 10$\n",
    "\n",
    "This results in a total of $(100352 + 128) + (1280 + 10) = 101770$ degrees of freedom (dofs) within our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNT43MtmMNrw",
    "outputId": "41469433-f781-4d79-d618-5e504ccfae29"
   },
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "miMS7VyJMNn8",
    "outputId": "60f5a718-90b0-488f-ae2f-bfe5e181359e"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model=model_1, rankdir=\"LR\", dpi=72, show_shapes=True, show_layer_activations=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCh7_1sUQMX7"
   },
   "source": [
    "#### Model Compilation\n",
    "\n",
    "Now we will compile our neural network model. Model compilation involves defining key components, such as the loss function, optimizer, learning rate, and metrics.\n",
    "\n",
    "##### Loss Function\n",
    "\n",
    "For our classification task, similar to the first binary model shown in Lab 2, we use the Categorical Crossentropy loss function. This loss function is suitable for multi-class classification problems where the classes are mutually exclusive, meaning each entry belongs to precisely one class.\n",
    "\n",
    "The Categorical Crossentropy loss function calculates the loss by comparing the model's predicted probability distribution $ P $ for each class with the actual distribution $ Y $. The formula for this loss function is:\n",
    "\n",
    "$ \\text{Categorical Crossentropy} = -\\sum_{i=1}^{C} y_i \\log(p_i) $\n",
    "\n",
    "where $ C $ is the number of classes, $ y_i $ is a binary indicator (0 or 1) of whether class $ i $ is the correct classification, and $ p_i $ is the predicted probability of the class $ i $.\n",
    "\n",
    "The loss is the sum of the negative log probabilities of the correct class across all samples in the dataset. This function penalizes incorrect predictions more heavily, thereby guiding the model towards more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wTLrS0qMNjn"
   },
   "outputs": [],
   "source": [
    "loss = losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJTWOjpVSbr5"
   },
   "source": [
    "##### Optimizer\n",
    "\n",
    "We use the *Adam* optimizer, a popular choice for training neural networks. The [Adam optimization algorithm](https://doi.org/10.48550/arXiv.1412.6980) is a neural network-specific adaptation of the [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mP0Z-OcaScNP"
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGOU2LGtTVKH"
   },
   "source": [
    "##### Metrics\n",
    "\n",
    "Metrics are functions needed to measure the behavior of our model. There are many to choose from depending on the task of the model. For our case:\n",
    "\n",
    "- **Accuracy**: This metric measures the overall classification accuracy of the model. It is calculated as the ratio of correct predictions to the total number of samples.\n",
    "\n",
    "\n",
    "$ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtK2mx7uTWE9"
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    tf.keras.metrics.CategoricalAccuracy(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBArdA58HErL"
   },
   "source": [
    "#### Training\n",
    "In training, we define two key parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULdIIY_fHKBx"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lhj7X3CiAIVS"
   },
   "source": [
    "* **Batch Size**: It specifies the number of training examples used in each iteration. A smaller batch size updates the model more frequently, while a larger one may improve training efficiency but requiring more volatile memory (RAM).\n",
    "\n",
    "* **Epochs**: Each epoch represents one pass through the entire training dataset. It controls how many times the model iterates over the data, influencing convergence and potential overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uF9yyCmAIVS"
   },
   "source": [
    "Let's (finally) start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Uz4jwPtHGB7",
    "outputId": "de5b45b8-5b64-45d3-fde0-7484e4fcc9f1"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    # These initial lines of code are repeated because they need to be defined\n",
    "    # inside the parallelization context to efficiently utilize the GPU/TPU.\n",
    "\n",
    "    # Step 1: Building the Model\n",
    "    model_1 = build_model_1(train_images.shape[1:], 10)\n",
    "\n",
    "    # Step 2: Compiling the Model\n",
    "    loss = losses.CategoricalCrossentropy()\n",
    "    optimizer = optimizers.Adam(\n",
    "        learning_rate=1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-08\n",
    "    )\n",
    "    metrics = [\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "    ]\n",
    "    model_1.compile(\n",
    "        optimizer,\n",
    "        loss,\n",
    "        metrics,\n",
    "    )\n",
    "\n",
    "    # Step 3: Training the Model\n",
    "    history = model_1.fit(\n",
    "        train_images,\n",
    "        train_labels_encoded,\n",
    "        batch_size,\n",
    "        epochs,\n",
    "        validation_data=(\n",
    "            test_images,\n",
    "            test_labels_encoded,\n",
    "        ),  # Validation set\n",
    "        verbose=1,  # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7LLoxDki2CJ"
   },
   "source": [
    "### Plots training and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "toD-MbFujEFN"
   },
   "outputs": [],
   "source": [
    "# @title Ausiliar plot function\n",
    "\n",
    "\n",
    "def plot_training(\n",
    "    ax,\n",
    "    ax_twin,\n",
    "    history,\n",
    "    metric=\"root_mean_squared_error\",\n",
    "    metric_label=\"RMSE\",\n",
    "    halflife=25,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot training history with specified metric.\n",
    "\n",
    "    Args:\n",
    "        ax (Matplotlib Axis): The main plot axis.\n",
    "        ax_twin (Matplotlib Axis): The twinned plot axis.\n",
    "        history (Pandas DataFrame): Training history data.\n",
    "        metric (str): The name of the metric to plot.\n",
    "        metric_label (str): Label for the metric on the plot.\n",
    "        halflife (int): Exponential moving average halflife for smoothing.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    ax.plot(history.index, history[metric], color=\"k\", ls=\"-\", alpha=0.25)\n",
    "    a1 = ax.plot(\n",
    "        history.index,\n",
    "        history[metric].ewm(halflife=halflife).mean(),\n",
    "        color=\"k\",\n",
    "        ls=\"-\",\n",
    "        label=metric_label + \" (train)\",\n",
    "    )\n",
    "    ax.plot(history.index, history[\"val_\" + metric], color=\"k\", ls=\"--\", alpha=0.25)\n",
    "    a2 = ax.plot(\n",
    "        history.index,\n",
    "        history[\"val_\" + metric].ewm(halflife=halflife).mean(),\n",
    "        color=\"k\",\n",
    "        ls=\"--\",\n",
    "        label=metric_label + \" (test)\",\n",
    "    )\n",
    "    ax_twin.plot(history.index, history[\"loss\"], color=\"r\", ls=\"-\", alpha=0.25)\n",
    "    l1 = ax_twin.plot(\n",
    "        history.index,\n",
    "        history[\"loss\"].ewm(halflife=halflife).mean(),\n",
    "        color=\"r\",\n",
    "        ls=\"-\",\n",
    "        label=\"Loss (train)\",\n",
    "    )\n",
    "    ax_twin.plot(history.index, history[\"val_loss\"], color=\"r\", ls=\"--\", alpha=0.25)\n",
    "    l2 = ax_twin.plot(\n",
    "        history.index,\n",
    "        history[\"val_loss\"].ewm(halflife=halflife).mean(),\n",
    "        color=\"r\",\n",
    "        ls=\"--\",\n",
    "        label=\"Loss (test)\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Epochs [-]\")\n",
    "    ax.set_ylabel(metric_label + \" [-]\")\n",
    "    ax_twin.set_ylabel(metric_label + \"Loss [-]\")\n",
    "    ax_twin.legend(\n",
    "        a1 + a2 + l1 + l2,\n",
    "        [\n",
    "            metric_label + \" (train)\",\n",
    "            metric_label + \" (test)\",\n",
    "            \"Loss (train)\",\n",
    "            \"Loss (test)\",\n",
    "        ],\n",
    "        loc=\"upper right\",\n",
    "    )\n",
    "    metric_data = np.concatenate(\n",
    "        [\n",
    "            history[metric].ewm(halflife=halflife).mean().values,\n",
    "            history[\"val_\" + metric].ewm(halflife=halflife).mean().values,\n",
    "        ]\n",
    "    )\n",
    "    lb, ub = [np.percentile(metric_data, 0.5), np.percentile(metric_data, 99.5)]\n",
    "    delta = ub - lb\n",
    "    ax.set_ylim([lb - 0.05 * delta, ub + 0.05 * delta])\n",
    "    loss_data = np.concatenate(\n",
    "        [\n",
    "            history[\"loss\"].ewm(halflife=halflife).mean().values,\n",
    "            history[\"val_loss\"].ewm(halflife=halflife).mean().values,\n",
    "        ]\n",
    "    )\n",
    "    lb, ub = [np.percentile(loss_data, 0.5), np.percentile(loss_data, 99.5)]\n",
    "    delta = ub - lb\n",
    "    ax_twin.set_ylim([lb - 0.05 * delta, ub + 0.05 * delta])\n",
    "\n",
    "\n",
    "# Define classnames for improved readability\n",
    "class_names = [\n",
    "    \"Zero\",\n",
    "    \"One\",\n",
    "    \"Two\",\n",
    "    \"Three\",\n",
    "    \"Four\",\n",
    "    \"Five\",\n",
    "    \"Six\",\n",
    "    \"Seven\",\n",
    "    \"Eight\",\n",
    "    \"Nine\",\n",
    "]\n",
    "\n",
    "\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = \"blue\"\n",
    "    else:\n",
    "        color = \"red\"\n",
    "\n",
    "    plt.xlabel(\n",
    "        \"{} {:2.0f}% ({})\".format(\n",
    "            class_names[predicted_label],\n",
    "            100 * np.max(predictions_array),\n",
    "            class_names[true_label],\n",
    "        ),\n",
    "        color=color,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array, true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color(\"red\")\n",
    "    thisplot[true_label].set_color(\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVUl8PLKMdac"
   },
   "outputs": [],
   "source": [
    "# Plot traing\n",
    "training_history_try1 = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "oJ_iSIsjQh38",
    "outputId": "38b506db-c2b0-432e-d13e-3c037bf012cd"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3.3))\n",
    "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax_twin = ax.twinx()\n",
    "    plot_training(\n",
    "        ax,\n",
    "        ax_twin,\n",
    "        training_history_try1,\n",
    "        metric=\"categorical_accuracy\",\n",
    "        metric_label=\"Accuracy\",\n",
    "        halflife=20,\n",
    "    )\n",
    "    # ax.set_yscale(\"log\")\n",
    "    # ax_twin.set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_QGAE6giTLx",
    "outputId": "f67d495d-1f0b-494a-f255-7d4ac7b78898"
   },
   "outputs": [],
   "source": [
    "predictions = model_1.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hc0Uv-kUiCmq",
    "outputId": "c1aaa8ca-bcf5-4da8-cdaf-57cdbd122b80"
   },
   "outputs": [],
   "source": [
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows * num_cols\n",
    "plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)\n",
    "    plot_image(i, predictions[i], test_labels, np.squeeze(test_images))\n",
    "    plt.subplot(num_rows, 2 * num_cols, 2 * i + 2)\n",
    "    plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVlFiCkglbmn"
   },
   "source": [
    "### Second Try: Data Augmetation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMx5zQ82lbm3"
   },
   "source": [
    "Let's try to reduce the *overfitting* with the *data augmatation*.\n",
    "\n",
    "**Data augmentation** is a technique used in machine learning and deep learning to increase the diversity of your training dataset without actually collecting new data. This is particularly useful in tasks like image classification, where obtaining a large dataset can be challenging and expensive.\n",
    "\n",
    "The fundamental idea behind data augmentation is to apply a series of random but realistic transformations to the existing data to create additional, varied training examples. These transformations should change the data in ways that could naturally occur, so the model learns to generalize better and becomes more robust to slight variations it might encounter in real-world data.\n",
    "\n",
    "For images, common data augmentation techniques include:\n",
    "- **Rotation**: Rotating the image by a certain angle.\n",
    "- **Translation**: Shifting the image horizontally or vertically.\n",
    "- **Rescaling**: Zooming in or out on the image.\n",
    "- **Flipping**: Mirroring the image horizontally or vertically.\n",
    "- **Cropping**: Taking a random crop of the image.\n",
    "- **Changing Brightness or Contrast**: Adjusting the image's brightness or contrast.\n",
    "- **Adding Noise**: Introducing random pixel noise to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwPh-ywzlbm3"
   },
   "outputs": [],
   "source": [
    "def build_model_2(shape_img: tuple, num_class: int) -> tf.keras.models.Sequential:\n",
    "    \"\"\"\n",
    "    Build a the second possible architecture for our neural network model.\n",
    "    Args:\n",
    "        shape_img (tuple): The shape of the input images (height, width, channels).\n",
    "        num_class (int): The number of classes for the classification task.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.models.Sequential: A Keras Sequential model.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            # Augmetation\n",
    "            layers.RandomZoom(height_factor=(0.0, 0.3)),\n",
    "            layers.RandomRotation(factor=(-0.25, 0.25)),\n",
    "            # Convert 2D image data to 1D\n",
    "            layers.Flatten(name=\"flattening_layer\"),\n",
    "            # Dense hidden layer with ReLU activation\n",
    "            layers.Dense(128, activation=\"relu\", name=\"hidden_layer_1\"),\n",
    "            # Output layer for classification\n",
    "            tf.keras.layers.Dense(num_class, activation=\"softmax\", name=\"output_layer\"),\n",
    "        ]\n",
    "    )\n",
    "    model.build(input_shape=(None, shape_img[0], shape_img[1]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHsHAkQjlbm4"
   },
   "source": [
    "Let's call the function to build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0y9bV4jlbm4"
   },
   "outputs": [],
   "source": [
    "model_2 = build_model_2(\n",
    "    train_images.shape[1:],  # image shape\n",
    "    10,  # number of digits (i.e 0, 1, 2, 3, 4, 5, 6 ,7 , 8, 9)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhcfDCRXlbm5"
   },
   "source": [
    "#### Model Summary and Structure Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Mmvect_lbm6",
    "outputId": "8a555628-13be-4d11-b577-af0d72210e90"
   },
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "LaBviq7Albm6",
    "outputId": "c09ed169-90ea-4459-b878-e1fb9e0eceab"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model=model_2, rankdir=\"LR\", dpi=72, show_shapes=True, show_layer_activations=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HUZHf80lbm7"
   },
   "source": [
    "#### Model Compilation\n",
    "\n",
    "##### Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwdNmqMWlbm7"
   },
   "outputs": [],
   "source": [
    "loss = losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5dIdeYWlbm7"
   },
   "source": [
    "##### Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zprEetvXlbm8"
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vzEpwr6lbm8"
   },
   "source": [
    "##### Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36nB_oLblbm9"
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    tf.keras.metrics.CategoricalAccuracy(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYerlVA_lbm9"
   },
   "source": [
    "#### Training\n",
    "In training, we define two key parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcFFrvCMlbm-"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdfTw2p4lbm-"
   },
   "source": [
    "* **Batch Size**: It specifies the number of training examples used in each iteration. A smaller batch size updates the model more frequently, while a larger one may improve training efficiency but requiring more volatile memory (RAM).\n",
    "\n",
    "* **Epochs**: Each epoch represents one pass through the entire training dataset. It controls how many times the model iterates over the data, influencing convergence and potential overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Qb9wpUYlbm_"
   },
   "source": [
    "Let's (finally) start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ap941uIplbm_",
    "outputId": "dc3183df-db97-48ee-a3c5-5dd6eecf0066"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    # These initial lines of code are repeated because they need to be defined\n",
    "    # inside the parallelization context to efficiently utilize the GPU/TPU.\n",
    "\n",
    "    # Step 1: Building the Model\n",
    "    model_2 = build_model_2(train_images.shape[1:], 10)\n",
    "\n",
    "    # Step 2: Compiling the Model\n",
    "    loss = losses.CategoricalCrossentropy()\n",
    "    optimizer = optimizers.Adam(\n",
    "        learning_rate=1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-08\n",
    "    )\n",
    "    metrics = [\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "    ]\n",
    "    model_2.compile(\n",
    "        optimizer,\n",
    "        loss,\n",
    "        metrics,\n",
    "    )\n",
    "\n",
    "    # Step 3: Training the Model\n",
    "    history = model_2.fit(\n",
    "        train_images,\n",
    "        train_labels_encoded,\n",
    "        batch_size,\n",
    "        epochs,\n",
    "        validation_data=(\n",
    "            test_images,\n",
    "            test_labels_encoded,\n",
    "        ),  # Validation set\n",
    "        verbose=1,  # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tM_c-HSlbnA"
   },
   "source": [
    "### Plots training and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRXtqAYNlbnB"
   },
   "outputs": [],
   "source": [
    "# Plot traing\n",
    "training_history_try2 = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "ceugKO7JlbnC",
    "outputId": "cccf2ab9-7cb8-4b54-985e-f3ac764acb85"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3.3))\n",
    "with plt.style.context(\"seaborn-v0_8-paper\"):\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax_twin = ax.twinx()\n",
    "    plot_training(\n",
    "        ax,\n",
    "        ax_twin,\n",
    "        training_history_try2,\n",
    "        metric=\"categorical_accuracy\",\n",
    "        metric_label=\"Accuracy\",\n",
    "        halflife=20,\n",
    "    )\n",
    "    # ax.set_yscale(\"log\")\n",
    "    # ax_twin.set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FuyKjmxXlbnC",
    "outputId": "0099bfd8-409b-49db-f35a-6f4c24636eef"
   },
   "outputs": [],
   "source": [
    "predictions = model_2.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ujFmGNfylbnD",
    "outputId": "70e3424d-7569-4c27-e676-388b0c3bd7fb"
   },
   "outputs": [],
   "source": [
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows * num_cols\n",
    "plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)\n",
    "    plot_image(i, predictions[i], test_labels, np.squeeze(test_images))\n",
    "    plt.subplot(num_rows, 2 * num_cols, 2 * i + 2)\n",
    "    plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Glf_lrMkbdu9"
   },
   "source": [
    "### Exercise: Use MNIST Fashion Dataset\n",
    "\n",
    "Train a NN model using the MNIST Fashion Dataset this time.\n",
    "\n",
    "steps:\n",
    "- Download dataset\n",
    "- Ispect data\n",
    "- Design a architecture\n",
    "- Chose a loss function and metrics\n",
    "- Compile the model\n",
    "- Training\n",
    "- Visualize the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_iFN10li6V1"
   },
   "source": [
    "#### Dataset sownload\n",
    "For this tutorial we will use the MNIST Fashion Dataset. This is a dataset that is included in keras.\n",
    "\n",
    "This dataset includes 60,000 images for training and 10,000 images for validation/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQmVmgOxjCOV",
    "outputId": "87587d17-2e5b-4698-acd0-1a19dfea6887"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist  # load dataset\n",
    "\n",
    "(train_images, train_labels), (\n",
    "    test_images,\n",
    "    test_labels,\n",
    ") = fashion_mnist.load_data()  # split into tetsing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZDDE4S1b6ub"
   },
   "outputs": [],
   "source": [
    "# Do the rest"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "17513c9ae3474c91bf0f4dc4db57c294": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "2c66241d3565493991f9d4384146a257": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": false,
      "description": "Green:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_b85ca2d1578e44a5b8545a6a411a2357",
      "max": 255,
      "min": 0,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 1,
      "style": "IPY_MODEL_7249109fcd5f4c7889341009ea33a06e",
      "value": 0
     }
    },
    "48d8c9866abb40aab7bcf30c8418b3e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": false,
      "description": "Blue:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_d1db5d245ddc43a8af1df2325373ece8",
      "max": 255,
      "min": 0,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 1,
      "style": "IPY_MODEL_79cdad440b374e6ca42f388100b69a9f",
      "value": 0
     }
    },
    "4c93d932c724460ea366f058696bf3e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7249109fcd5f4c7889341009ea33a06e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "72dd7b2b295d4dd2ac32f9878ac2e7d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": false,
      "description": "Red:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_fc2eec63d71f4876abfd6e20d27b71c1",
      "max": 255,
      "min": 0,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 1,
      "style": "IPY_MODEL_17513c9ae3474c91bf0f4dc4db57c294",
      "value": 0
     }
    },
    "79cdad440b374e6ca42f388100b69a9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "b85ca2d1578e44a5b8545a6a411a2357": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1db5d245ddc43a8af1df2325373ece8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4699e4cb0124b1dbb883e45c33fd00d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72dd7b2b295d4dd2ac32f9878ac2e7d5",
       "IPY_MODEL_2c66241d3565493991f9d4384146a257",
       "IPY_MODEL_48d8c9866abb40aab7bcf30c8418b3e3"
      ],
      "layout": "IPY_MODEL_4c93d932c724460ea366f058696bf3e6"
     }
    },
    "fc2eec63d71f4876abfd6e20d27b71c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
